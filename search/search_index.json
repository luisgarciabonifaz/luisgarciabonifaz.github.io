{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bienvenido a la web de clases de Luis Garc\u00eda /Benvingut a la web de classes de Luis Garc\u00eda","text":"<p>Esta es la wiki que uso para colgar los contenidos de las asignaturas que imparto en Ciclos Formativos</p>"},{"location":"#grado-medio-smr","title":"Grado Medio SMR","text":"<ul> <li>Sistemes Operatius Monolloc </li> </ul>"},{"location":"#curso-de-especializacion-en-inteligencia-artificialy-big-data","title":"Curso de especializacion en Inteligencia Artificialy Big Data","text":"<ul> <li>Big Data Aplicado </li> <li>Sistemas Big Data </li> </ul>"},{"location":"BDA/IndiceBDA/","title":"Big Data Aplicado","text":""},{"location":"BDA/IndiceBDA/#temas","title":"Temas","text":"<ul> <li>T1 - Introducci\u00f3n Big Data </li> <li>T2 - Repaso SQL </li> <li>T3 - Bases de datos NoSQL </li> <li>T4 - Almacenamiento de Datos </li> <li>T5 - ETL </li> <li>T6 - Visualizaci\u00f3n de Datos </li> <li>T7 - Bussines Intelligencen </li> <li>T8 - Proyecto </li> </ul>"},{"location":"BDA/Tema01/IntroduccionBigData/","title":"1. Introducci\u00f3n Big Data","text":""},{"location":"BDA/Tema01/IntroduccionBigData/#11-big-data","title":"1.1. Big Data","text":"<p>El Big Data es el an\u00e1lisis masivo de datos. Una cantidad de datos, tan sumamente grande, que las aplicaciones de software de procesamiento de datos que tradicionalmente se ven\u00edan usando no son capaces de capturar, tratar y poner en valor en un tiempo razonable.</p> <p>El mismo t\u00e9rmino tambi\u00e9n se refiere a las nuevas tecnolog\u00edas que hacen posible el almacenamiento y procesamiento, adem\u00e1s de al uso que se hace de la informaci\u00f3n obtenida a trav\u00e9s de dichas tecnolog\u00edas.</p> <p>La gran popularidad del Big Data es debida principalmente a la oportunidad que ven en ella las grandes empresas. El hecho de poder analizar millones de datos de distintas procedencias como redes sociales, im\u00e1genes digitales, emails, encuestas, logs, se\u00f1ales de m\u00f3vil, etc., permite que la toma de decisiones sea mucho m\u00e1s r\u00e1pida, precisa y efectiva.</p> Big Data permite abordar problemas empresariales que antes eran insolubles. <ul> <li>Generar informaci\u00f3n valiosa</li> <li>Perfeccionar campa\u00f1as y t\u00e9cnicas de marketing</li> <li>Entrenar m\u00e1quinas</li> <li>Modelado predictivo y otras aplicaciones avanzadas de an\u00e1lisis</li> <li>Reducir costos</li> <li>Ahorrar tiempo</li> <li>Comprender mejor las condiciones del mercado</li> <li>Vencer a grandes competidores y retener a clientes leales.</li> </ul> \u00bf Cuando considerar la utilizaci\u00f3n la arquitectura Big Data? <ul> <li>Los datos son demasiado grandes para que los almacenes de datos tradicionales los procesen.</li> <li>Transformaci\u00f3n y an\u00e1lisis de datos no estructurados.</li> <li>Necesidad de analizar datos en tiempo real con baja latencia.</li> </ul>"},{"location":"BDA/Tema01/IntroduccionBigData/#12-historia","title":"1.2. Historia","text":"<p>El t\u00e9rmino \u2018Big Data\u2019 ha estado en uso desde principios de los a\u00f1os 90. Aunque no se sabe exactamente qui\u00e9n fue el primero en usar el t\u00e9rmino, la mayor\u00eda de las personas atribuyen a John R. Mashey (que en ese momento trabajaba en Silicon Graphics) por hacer popular el t\u00e9rmino.</p> <p></p>"},{"location":"BDA/Tema01/IntroduccionBigData/#13-componentes-de-big-data","title":"1.3. Componentes de Big Data","text":"<p>Los componentes de Big Data constan de una variedad de elementos, pero no es necesario utilizarlos todos juntos para un caso de uso. Los componentes var\u00edan seg\u00fan el caso de uso empresarial.</p> <p></p>"},{"location":"BDA/Tema01/IntroduccionBigData/#131-fuentes-de-datos","title":"1.3.1. Fuentes de Datos","text":"<p>Las fuentes incluyen una gran variedad de datos, como bases de datos relacionales tradicionales, archivos de registro de procesos empresariales y aplicaciones, y mensajes en tiempo real generados por eventos y dispositivos de IoT.</p> <p>Los datos en Big Data pueden ser clasificados en tres tipos seg\u00fan su origen:</p> <ul> <li>Generados por m\u00e1quinas: procedentes de sensores (GPS, contadores\u2026), Web Log Data (redes, aplicaciones\u2026), procedentes de puntos de venta (c\u00f3digos de barras de un producto) y financieros (operaciones bancarias).</li> <li>Generados por personas: formularios, registros de contabilidad\u2026</li> <li>Datos externos: redes sociales, patentes, datos web, datos m\u00f3viles, etiquetas RFID (rastreo electr\u00f3nico) y c\u00f3digos de barras, datos de ubicaci\u00f3n, sensores de datos, entre otros.</li> </ul> <p>Seg\u00fan su estructura se pueden clasificar en tres tipos:</p> <ul> <li>Datos estructurados: son aquellos que se encuentran ordenados y organizados en una estructura predefinida, c\u00f3mo una tabla o una base de datos relacional. Estos datos son f\u00e1ciles de gestionar y analizar, ya que su estructura permite una mayor predictibilidad. Ejemplos de datos estructurados incluyen registros en tablas, ficheros XML asociados a un esquema, y facturas autogeneradas al realizar una compra.</li> <li>Datos semiestructurados: son aquellos que tienen cierta estructura, pero no est\u00e1n organizados en una estructura r\u00edgida c\u00f3mo los datos estructurados. Ejemplos de datos semiestructurados incluyen documentos JSON y XML sin esquema asociado.</li> <li>Datos no estructurados: son aquellos que no tienen una estructura predefinida y pueden ser dif\u00edciles de gestionar y analizar. Ejemplos de datos no estructurados incluyen publicaciones en redes sociales, v\u00eddeos, im\u00e1genes, y texto libre.</li> </ul> <p></p>"},{"location":"BDA/Tema01/IntroduccionBigData/#132-almacenamiento-de-datos","title":"1.3.2. Almacenamiento de Datos","text":"<p>El almacenamiento de este gran volumen de datos generalmente implican sistemas de almacenamiento distribuidos de gran capacidad que facilitan el an\u00e1lisis de esa inmensa cantidad de informaci\u00f3n.</p> <p>Hay muchas opciones disponibles para almacenar grandes vol\u00famenes de datos.</p> <ul> <li>Los sistemas de archivos distribuidos, como Hadoop Distributed File System (HDFS) y Google File System (GFS), son una opci\u00f3n popular para almacenar grandes conjuntos de datos en m\u00faltiples servidores.</li> <li>Los servicios de almacenamiento en la nube, como Amazon S3 y Microsoft Azure Blob Storage, tambi\u00e9n son opciones populares para almacenar datos en la nube.</li> </ul> <p>La gesti\u00f3n efectiva de los datos es esencial para garantizar su integridad y disponibilidad. Esto incluye:</p> <ul> <li>La implementaci\u00f3n de medidas de seguridad adecuadas para proteger los datos contra el acceso no autorizado</li> <li>La realizaci\u00f3n de copias de seguridad regulares para garantizar la recuperaci\u00f3n en caso de p\u00e9rdida de datos</li> <li>La implementaci\u00f3n de pol\u00edticas y procedimientos para garantizar que los datos se manejen de manera responsable y \u00e9tica.</li> </ul> <p>Esto \u00faltimo nos lleva al concepto de gobernanza de datos que abarca las pol\u00edticas y procedimientos que se implementan para garantizar que los datos de una organizaci\u00f3n sean precisos y que se manejen correctamente cuando se ingresan, almacenan, manejan, acceden y eliminan.</p> <p>Las responsabilidades de gobernanza de datos incluyen establecer la infraestructura y tecnolog\u00eda, configurar y mantener procesos y pol\u00edticas, e identificar a las personas (o cargos) de una organizaci\u00f3n que tienen la autoridad y responsabilidad de gestionar y salvaguardar tipos espec\u00edficos de datos.</p>"},{"location":"BDA/Tema01/IntroduccionBigData/#133-ingesta-de-mensajes-en-tiempo-real","title":"1.3.3. Ingesta de Mensajes en Tiempo Real","text":"<p>La ingesti\u00f3n de datos en tiempo real incluye mecanismos para recibir mensajes en streaming, almacenarlos y procesarlos para obtener conocimientos. </p> <p>Los componentes que facilitan la recepci\u00f3n de mensajes act\u00faan c\u00f3mo un b\u00fafer mientras los datos se env\u00edan a un almacenamiento para su procesamiento. </p>"},{"location":"BDA/Tema01/IntroduccionBigData/#134-procesamiento-por-lotes","title":"1.3.4. Procesamiento por Lotes","text":"<p>El procesamiento por lotes generalmente implica conjuntos de datos grandes que se procesan secuencialmente para agregar, filtrar y preparar los datos para un an\u00e1lisis posterior. </p> <p>Este enfoque puede ser \u00fatil cuando se trata con datos hist\u00f3ricos o cuando se realizan an\u00e1lisis complejos que requieren m\u00e1s tiempo.</p> <p>Algunos de los componentes de procesamiento por lotes incluyen Azure Data Factory, Azure Batch, U-SQL (Azure Data Lake Analytics), cl\u00fasteres HDInsight Hadoop y Spark</p>"},{"location":"BDA/Tema01/IntroduccionBigData/#135-procesamiento-en-tiempo-real","title":"1.3.5. Procesamiento en Tiempo Real","text":"<p>El procesamiento en tiempo real del Big Data implica analizar datos a medida que se generan para tomar decisiones o desencadenar acciones en tiempo real.</p> <p>Por ejemplo, el procesamiento en tiempo real puede utilizarse para monitorear feeds sociales en busca de menciones de una marca o producto en particular.</p> <p>Casos de Uso en Tiempo Real</p> <p>Algunos de los casos de uso en tiempo real para las arquitecturas mencionadas anteriormente son:</p> <ul> <li>Twitter y Facebook utilizan la arquitectura Lambda para llevar a cabo an\u00e1lisis en tiempo real y por lotes de sus mensajes y publicaciones.</li> <li>Los software integrados en los coches el\u00e9ctricos env\u00edan datos en tiempo real (a trav\u00e9s de dispositivos IoT/Edge) al fabricante, que luego realiza an\u00e1lisis de datos de sensores en tiempo real y por lotes para mejorar el rendimiento de los coches el\u00e9ctricos. Esto se basa en la arquitectura Lambda.</li> <li>El sector de las telecomunicaciones tiene grandes vol\u00famenes de datos que se pueden analizar en tiempo real para identificar y solucionar anomal\u00edas relacionadas con la red. Los datos pueden alimentar modelos de ML en tiempo real y el modelo de ML puede sugerir recomendaciones que se pueden implementar mediante flujos de IA. Esto suele seguir la arquitectura Kappa.</li> </ul>"},{"location":"BDA/Tema01/IntroduccionBigData/#136-procesamiento-hibrido","title":"1.3.6.  Procesamiento Hibrido","text":"<p>Es una mezcla de  los dos anteriores y tiene dos posibles arquitecturas:</p>"},{"location":"BDA/Tema01/IntroduccionBigData/#1361-lambda","title":"1.3.6.1. Lambda","text":"<p>Su objetivo es tener un sistema robusto tolerante a fallos que sea linealmente escalable y que permita realizar escrituras y lecturas con baja latencia.</p> <p></p> <p>La nueva informaci\u00f3n recogida por el sistema se env\u00eda tanto a la capa de batch como a la capa de streaming (Speed Layer).</p> <p>En la capa batch (Batch Layer) se gestiona la informaci\u00f3n en crudo. Los datos nuevos se a\u00f1aden a los ya existentes. Seguidamente se hace un tratamiento mediante un proceso batch cuyo resultado ser\u00e1n las denominadas Batch Views.</p> <p>La capa que sirve los datos(Serving Layer), indexa las Batch Views para que puedan ser consultadas con baja latencia.</p> <p>La capa de streaming (Speed Layer), compensa la alta latencia de las escrituras que ocurre en la serving layer y solo tiene en cuenta los datos nuevos.</p> <p>Finalmente, la respuesta a las consultas realizadas se construye combinando los resultados de las Batch Views y de las vistas en tiempo real (Real-time Views).</p>"},{"location":"BDA/Tema01/IntroduccionBigData/#1362-kappa","title":"1.3.6.2. Kappa","text":"<p>Su objetivo es eliminar la capa batch dejando solamente la capa de streaming.</p> <p>Esta capa, a diferencia de la de tipo batch, no tiene un comienzo ni un fin desde un punto de vista temporal y est\u00e1 continuamente procesando nuevos datos a medida que van llegando.</p> <p>Sus cuatro pilares principales son:</p> <ul> <li>Todo es un stream: las operaciones batch son un subconjunto de las operaciones de streaming, por lo que todo puede ser tratado como un stream.</li> <li>Los datos de partida no se modifican: los datos son almacenados sin ser transformados y las vistas se derivan de ellos. Un estado concreto puede ser recalculado puesto que la informaci\u00f3n de origen no se modifica.</li> <li>Solo existe un flujo de procesamiento: puesto que mantenemos un solo flujo, el c\u00f3digo, el mantenimiento y la actualizaci\u00f3n del sistema se ven reducidos considerablemente.</li> <li>Posibilidad de volver a lanzar un procesamiento: se puede modificar un procesamiento concreto y su configuraci\u00f3n para variar los resultados obtenidos partiendo de los mismos datos de entrada</li> </ul> <p></p>"},{"location":"BDA/Tema01/IntroduccionBigData/#137-almacen-de-datos-analiticos-data-warehouse","title":"1.3.7. Almac\u00e9n de Datos Anal\u00edticos (Data Warehouse)","text":"<p>Los almacenes de datos anal\u00edticos o  Data Warehouses en ingl\u00e9s, son sistemas de almacenamiento y gesti\u00f3n de datos dise\u00f1ados espec\u00edficamente para la recopilaci\u00f3n, organizaci\u00f3n y an\u00e1lisis de grandes vol\u00famenes de informaci\u00f3n empresarial. </p> <p>Estos almacenes est\u00e1n optimizados para facilitar la consulta y el procesamiento de datos con el objetivo de obtener informaci\u00f3n significativa para la toma de decisiones.</p> <p>Algunas de sus caracter\u00edsticas son:</p> <ul> <li>Consolidaci\u00f3n de datos </li> <li>Estructura optimizada</li> <li>Historizaci\u00f3n</li> <li>Rendimiento</li> <li>Herramientas de consulta y reporting</li> <li>Seguridad y control de acceso</li> </ul>"},{"location":"BDA/Tema01/IntroduccionBigData/#138-analisis-e-informes","title":"1.3.8. An\u00e1lisis e Informes","text":""},{"location":"BDA/Tema01/IntroduccionBigData/#1381-analisis","title":"1.3.8.1. An\u00e1lisis","text":"<p>Tipos de an\u00e1lisis de big data:</p> <ul> <li>Predictivo<ul> <li>El an\u00e1lisis predictivo implica utilizar t\u00e9cnicas estad\u00edsticas para hacer predicciones sobre eventos futuros bas\u00e1ndose en datos hist\u00f3ricos. Este enfoque puede aplicarse para generar conocimientos sobre tendencias o comportamientos futuros.</li> </ul> </li> <li>De diagn\u00f3stico<ul> <li>El an\u00e1lisis de diagn\u00f3stico utiliza algoritmos para descubrir patrones y relaciones dentro de grandes conjuntos de datos. Este enfoque puede aplicarse para extraer informaci\u00f3n \u00fatil de grandes vol\u00famenes de datos no estructurados o semiestructurados.</li> </ul> </li> <li>Descriptivo<ul> <li>El an\u00e1lisis descriptivo es un m\u00e9todo de an\u00e1lisis de datos que se utiliza para resumir y describir sus caracter\u00edsticas principales. Se utiliza com\u00fanmente para proporcionar una visi\u00f3n general de los datos y para identificar patrones y tendencias. </li> </ul> Uso <p>El an\u00e1lisis descriptivo puede utilizarse para obtener informaci\u00f3n a partir de conjuntos de datos grandes y complejos. Por ejemplo, una empresa podr\u00eda utilizar el an\u00e1lisis descriptivo para entender la demograf\u00eda de su base de clientes o para identificar patrones en el comportamiento de los clientes.</p> </li> <li>Prescriptivo con Big Data<ul> <li>El an\u00e1lisis prescriptivo es un tipo de an\u00e1lisis de datos que utiliza t\u00e9cnicas avanzadas de an\u00e1lisis, como el aprendizaje autom\u00e1tico y los algoritmos de optimizaci\u00f3n, para sugerir acciones que se pueden tomar para lograr un resultado deseado.</li> </ul> Uso <p>El an\u00e1lisis prescriptivo puede utilizarse para tomar decisiones basadas en datos a partir de la informaci\u00f3n obtenida a partir de conjuntos de datos grandes y complejos. Por ejemplo, una empresa podr\u00eda utilizar el an\u00e1lisis prescriptivo para optimizar su cadena de suministro o para determinar la mejor estrategia de precios para sus productos.</p> </li> </ul>"},{"location":"BDA/Tema01/IntroduccionBigData/#1382-informes-o-visualizacion-de-datos","title":"1.3.8.2. Informes o Visualizaci\u00f3n de datos","text":"<p>La visualizaci\u00f3n de datos es la presentaci\u00f3n de datos en formato ilustrado o gr\u00e1fico. Permite a los tomadores de decisiones ver la anal\u00edtica presentada de forma visual, de modo que puedan captar conceptos dif\u00edciles o identificar nuevos patrones. Con la visualizaci\u00f3n interactiva, se puede llevar el concepto un paso adelante utilizando tecnolog\u00eda para profundizar en diagramas y gr\u00e1ficas para observar mayor detalle, cambiando de forma interactiva qu\u00e9 datos se ven y c\u00f3mo se procesan.</p> <p>Hay muchas herramientas disponibles para la visualizaci\u00f3n de Big Data. Algunas de las herramientas m\u00e1s populares son Tableau, Power BI, Infogram, ChartBlocks, Datawrapper y Ploty. Estas herramientas permiten crear tablas, gr\u00e1ficos, mapas y otros tipos de visualizaciones para ayudar a comprender y comunicar los datos.</p> <p>La visualizaci\u00f3n de datos es importante porque permite transmitir conceptos de manera universal y r\u00e1pida. Tambi\u00e9n puede ayudar a identificar \u00e1reas que necesitan atenci\u00f3n o mejoras y esclarecer qu\u00e9 factores influencian el comportamiento de los clientes.</p>"},{"location":"BDA/Tema01/IntroduccionBigData/#139-orquestacion","title":"1.3.9. Orquestaci\u00f3n","text":"<p>Para lograr un procesamiento por lotes o en tiempo real exitoso, la orquestaci\u00f3n del flujo de datos es muy importante para construir procesos por lotes o en tiempo real confiables y resistentes a fallos. </p>"},{"location":"BDA/Tema01/IntroduccionBigData/#1310-aprendizaje-automatico","title":"1.3.10. Aprendizaje Autom\u00e1tico","text":"<p>Big data y aprendizaje autom\u00e1tico son dos campos estrechamente relacionados y complementarios.</p> <ul> <li>Big data se refiere a conjuntos de datos extremadamente grandes y complejos que son dif\u00edciles de procesar utilizando m\u00e9todos tradicionales.</li> <li>El aprendizaje autom\u00e1tico, por otro lado, es una rama de la inteligencia artificial que se enfoca en desarrollar algoritmos que permitan a las m\u00e1quinas aprender de los datos y mejorar su rendimiento con el tiempo.</li> </ul> <p>El aprendizaje autom\u00e1tico aprovecha el big data para entrenar modelos m\u00e1s precisos y confiables. Cuanto m\u00e1s datos haya disponibles, m\u00e1s precisos y confiables ser\u00e1n los modelos del aprendizaje autom\u00e1tico. El aprendizaje autom\u00e1tico tiene aplicaciones amplias en el procesamiento y an\u00e1lisis del big data, c\u00f3mo la clasificaci\u00f3n y categorizaci\u00f3n autom\u00e1tica de datos, la detecci\u00f3n de anomal\u00edas y la predicci\u00f3n de resultados futuros.</p> <p>La combinaci\u00f3n de big data y aprendizaje autom\u00e1tico ofrece beneficios significativos. Permite descubrir informaci\u00f3n valiosa oculta en grandes vol\u00famenes de datos, mejorar la toma de decisiones basada en datos, automatizar tareas y optimizar procesos empresariales.</p>"},{"location":"BDA/Tema01/IntroduccionBigData/#14-las-vs-de-la-arquitectura-big-data","title":"1.4. Las V's de la Arquitectura Big Data","text":"<p>El concepto de \"Las V's del Big Data\" se refiere a las caracter\u00edsticas clave que describen los datos masivos utilizados en el an\u00e1lisis de datos. A lo largo del tiempo, diferentes fuentes y expertos han propuesto diferentes cantidades de \"V's\" para describir estas caracter\u00edsticas. A continuaci\u00f3n, se mencionan las tres versiones m\u00e1s comunes:</p>"},{"location":"BDA/Tema01/IntroduccionBigData/#las-3-vs-del-big-data","title":"Las 3 V's del Big Data:","text":"<ul> <li> <p>Volumen: Hace referencia a la cantidad de datos generados y almacenados. En el Big Data, se manejan conjuntos de datos extremadamente grandes.</p> </li> <li> <p>Variedad: Se refiere a la diversidad de tipos de datos. Estos pueden incluir texto, im\u00e1genes, audio, video, datos estructurados y no estructurados.</p> </li> <li> <p>Velocidad: Indica la rapidez con la que se generan y se deben analizar los datos. En el Big Data, se trata de procesar datos en tiempo real o casi en tiempo real.</p> </li> </ul>"},{"location":"BDA/Tema01/IntroduccionBigData/#las-5-vs-del-big-data","title":"Las 5 V's del Big Data:","text":"<p>Adem\u00e1s de las 3 V's mencionadas anteriormente, algunas fuentes tambi\u00e9n incluyen:</p> <ul> <li> <p>Veracidad: La calidad y precisi\u00f3n de los datos son esenciales. Los datos deben ser confiables y veraces para tomar decisiones precisas.</p> </li> <li> <p>Valor: Finalmente, el valor se refiere a la capacidad de extraer informaci\u00f3n valiosa y conocimiento \u00fatil de los datos. Los datos deben ser relevantes y aportar valor a una organizaci\u00f3n o proyecto.</p> </li> </ul>"},{"location":"BDA/Tema01/IntroduccionBigData/#las-7-vs-del-big-data","title":"Las 7 V's del Big Data:","text":"<p>Adem\u00e1s de las 5 V's mencionadas anteriormente, algunas fuentes tambi\u00e9n incluyen:</p> <ul> <li> <p>Viabilidad: Se refiere a la capacidad de llevar a cabo proyectos de Big Data de manera factible desde el punto de vista t\u00e9cnico y econ\u00f3mico.</p> </li> <li> <p>Visualizaci\u00f3n: La capacidad de representar y comunicar los datos de manera efectiva a trav\u00e9s de gr\u00e1ficos y herramientas visuales.</p> </li> </ul> <p>Estas V's son importantes para comprender las caracter\u00edsticas esenciales de los datos masivos y c\u00f3mo gestionarlos para obtener el m\u00e1ximo valor</p>"},{"location":"BDA/Tema01/IntroduccionBigData/#15-infraestructura","title":"1.5. Infraestructura","text":"<p>Una infraestructura para Big Data debe facilitar la recopilaci\u00f3n, el almacenamiento y el an\u00e1lisis de grandes vol\u00famenes de datos, que pueden estar en diferentes formatos y gener\u00e1ndose en tiempo real. Para ello, se utilizan tecnolog\u00edas y servicios especiales que han sido creados espec\u00edficamente para dar soluci\u00f3n al procesamiento de estos enormes conjuntos de datos.</p> <p>Algunos de los principales elementos de hardware y software que integran una soluci\u00f3n arquitect\u00f3nica de Big Data son:</p> <ul> <li>Hardware: servidores, dispositivos de almacenamiento, redes de alta velocidad.</li> <li>Software: sistemas operativos, bases de datos, herramientas de an\u00e1lisis y visualizaci\u00f3n de datos.</li> </ul> <p>Estos son algunos ejemplos de infraestructuras para procesar el Big Data:</p> <ul> <li>Apache Hadoop: una de las soluciones m\u00e1s conocidas para analizar Big Data, que utiliza un marco de trabajo de c\u00f3digo abierto para almacenar y procesar grandes conjuntos de datos.</li> <li>Apache Spark: esta herramienta permite almacenar gran parte de los datos de procesamiento en la memoria y en el disco, lo que se traduce en una mayor rapidez.</li> <li>Bases de datos NoSQL: como Cassandra, son utilizadas para almacenar y gestionar grandes vol\u00famenes de datos no estructurados.</li> </ul> <p>La infraestructura para procesar el Big Data debe ser capaz de manejar grandes vol\u00famenes de datos y proporcionar herramientas para su an\u00e1lisis y visualizaci\u00f3n.</p>"},{"location":"BDA/Tema01/IntroduccionBigData/#16-bibliografia","title":"1.6. Bibliografia","text":"<ul> <li>Arquitectura Big Data</li> <li>Qu\u00e9 es Big Data</li> <li>Qu\u00e9 es Big Data y para que sirve</li> <li>Historia de Big Data</li> <li>Arqruitecturas Lamda y Kappa </li> </ul>"},{"location":"BDA/Tema01/Introducion_OLD/","title":"1. Introducci\u00f3n Big Data","text":""},{"location":"BDA/Tema01/Introducion_OLD/#11-big-data","title":"1.1. Big Data","text":"<p>Big Data es una colecci\u00f3n de conjuntos de datos que son lo suficientemente grandes y complejos como para hacer obsoleta la arquitectura tradicional de almacenamiento y procesamiento de datos.</p> <p>\u00bf Cuando considerar la utilizaci\u00f3n la arquitectura Big Data?</p> Escenarios <ul> <li>Los datos son demasiado grandes para que las tiendas de datos tradicionales los procesen.</li> <li>Transformaci\u00f3n y an\u00e1lisis de datos no estructurados.</li> <li>Necesidad de analizar datos en tiempo real con baja latencia.</li> </ul> <p>Tipos de cargas de trabajo</p> <p>Los tipos de cargas de trabajo que se pueden considerar incluyen:</p> <ul> <li>Procesamiento por lotes de datos.</li> <li>Procesamiento en tiempo real de datos en movimiento.</li> </ul> <p>Las 3 Vs del Big Data son Volumen, Velocidad y Variedad.</p> <ul> <li>El Volumen se refiere a la cantidad de datos</li> <li>La Velocidad se refiere a la velocidad a la que se generan y procesan los datos</li> <li>La Variedad se refiere a los diferentes tipos de datos disponibles.</li> </ul> <p>Big Data es importante en el mundo actual porque puede utilizarse para abordar problemas empresariales que antes eran insolubles, generar informaci\u00f3n valiosa, perfeccionar campa\u00f1as y t\u00e9cnicas de marketing, entrenar m\u00e1quinas, modelado predictivo y otras aplicaciones avanzadas de an\u00e1lisis, reducir costos, ahorrar tiempo, comprender mejor las condiciones del mercado, vencer a grandes competidores y retener a clientes leales.</p>"},{"location":"BDA/Tema01/Introducion_OLD/#12-historia","title":"1.2. Historia","text":"<p>El t\u00e9rmino \u2018Big Data\u2019 ha estado en uso desde principios de los a\u00f1os 90. Aunque no se sabe exactamente qui\u00e9n fue el primero en usar el t\u00e9rmino, la mayor\u00eda de las personas atribuyen a John R. Mashey (que en ese momento trabajaba en Silicon Graphics) por hacer popular el t\u00e9rmino.</p> <p></p>"},{"location":"BDA/Tema01/Introducion_OLD/#13-tecnologias","title":"1.3. Tecnolog\u00edas","text":"<p>Hay muchas tecnolog\u00edas populares de Big Data c\u00f3mo Hadoop, Spark y bases de datos NoSQL. Estas tecnolog\u00edas pueden compararse en funci\u00f3n de sus casos de uso y capacidades.</p> <p>Las tecnolog\u00edas de Big Data se pueden dividir en cuatro categor\u00edas principales:</p> <ul> <li>Almacenamiento de datos: Las tecnolog\u00edas de Big Data que se ocupan del almacenamiento de datos tienen la capacidad de recuperar, almacenar y gestionar grandes conjuntos de datos. Est\u00e1 compuesto por infraestructura que permite a los usuarios almacenar los datos para que sean f\u00e1cilmente accesibles. La mayor\u00eda de las plataformas de almacenamiento de datos son compatibles con otros programas. Dos herramientas com\u00fanmente utilizadas son Apache Hadoop y MongoDB.</li> <li>Miner\u00eda de datos: La miner\u00eda de datos extrae patrones y tendencias \u00fatiles a partir del conjunto de datos sin procesar. Tecnolog\u00edas c\u00f3mo Rapidminer y Presto pueden convertir datos estructurados y no estructurados en informaci\u00f3n utilizable.</li> <li>An\u00e1lisis de datos: El an\u00e1lisis de datos utiliza t\u00e9cnicas avanzadas c\u00f3mo el aprendizaje autom\u00e1tico y el an\u00e1lisis estad\u00edstico para obtener informaci\u00f3n a partir del conjunto de datos.</li> <li>Visualizaci\u00f3n de datos: La visualizaci\u00f3n de datos es importante para comprender el Big Data porque permite a los tomadores de decisiones ver anal\u00edticas presentadas visualmente para que puedan comprender conceptos dif\u00edciles o identificar nuevos patrones.</li> </ul> <p>Cada una de estas categor\u00edas est\u00e1 asociada con ciertas herramientas, y deber\u00e1s elegir la herramienta adecuada para tus necesidades empresariales dependiendo del tipo de tecnolog\u00eda de Big Data que se requiera.</p>"},{"location":"BDA/Tema01/Introducion_OLD/#14-aplicaciones","title":"1.4. Aplicaciones","text":"<p>Big Data se utiliza en diversas industrias c\u00f3mo: la salud, las finanzas, el comercio minorista, etc. Los beneficios del uso del Big Data incluyen:</p> <ul> <li>Una mejor toma de decisiones</li> <li>Un aumento de la eficiencia</li> <li>Capacidad para descubrir nuevos conocimientos</li> </ul> <p>Sin embargo, tambi\u00e9n hay desaf\u00edos asociados con el uso del Big Data c\u00f3mo gestionar grandes vol\u00famenes de datos y garantizar la privacidad de los datos.</p>"},{"location":"BDA/Tema01/Introducion_OLD/#15-big-data-y-aprendizaje-automatico","title":"1.5. Big Data y Aprendizaje Autom\u00e1tico","text":"<p>Big data y aprendizaje autom\u00e1tico son dos campos estrechamente relacionados y complementarios. * Big data se refiere a conjuntos de datos extremadamente grandes y complejos que son dif\u00edciles de procesar utilizando m\u00e9todos tradicionales. * El aprendizaje autom\u00e1tico, por otro lado, es una rama de la inteligencia artificial que se enfoca en desarrollar algoritmos que permitan a las m\u00e1quinas aprender de los datos y mejorar su rendimiento con el tiempo.</p> <p>El aprendizaje autom\u00e1tico aprovecha el big data para entrenar modelos m\u00e1s precisos y confiables. Cuanto m\u00e1s datos haya disponibles, m\u00e1s precisos y confiables ser\u00e1n los modelos del aprendizaje autom\u00e1tico. El aprendizaje autom\u00e1tico tiene aplicaciones amplias en el procesamiento y an\u00e1lisis del big data, c\u00f3mo la clasificaci\u00f3n y categorizaci\u00f3n autom\u00e1tica de datos, la detecci\u00f3n de anomal\u00edas y la predicci\u00f3n de resultados futuros.</p> <p>La combinaci\u00f3n de big data y aprendizaje autom\u00e1tico ofrece beneficios significativos. Permite descubrir informaci\u00f3n valiosa oculta en grandes vol\u00famenes de datos, mejorar la toma de decisiones basada en datos, automatizar tareas y optimizar procesos empresariales.</p>"},{"location":"BDA/Tema01/Introducion_OLD/#16-fuentes-u-origen-de-los-datos","title":"1.6. Fuentes u origen de los datos","text":"<p>Los datos en Big Data pueden ser clasificadas en tres tipos seg\u00fan su origen:</p> <ul> <li>Generados por m\u00e1quinas: procedentes de sensores (GPS, contadores\u2026), Web Log Data (redes, aplicaciones\u2026), procedentes de puntos de venta (c\u00f3digos de barras de un producto) y financieros (operaciones bancarias).</li> <li>Generados por personas: formularios, registros de contabilidad\u2026</li> <li>Datos externos: redes sociales, patentes, datos web, datos m\u00f3viles, etiquetas RFID (rastreo electr\u00f3nico) y c\u00f3digos de barras, datos de ubicaci\u00f3n, sensores de datos, entre otros.</li> </ul> <p>Seg\u00fan su estructura se pueden clasificar en tres tipos:</p> <ul> <li>Datos estructurados: son aquellos que se encuentran ordenados y organizados en una estructura predefinida, c\u00f3mo una tabla o una base de datos relacional. Estos datos son f\u00e1ciles de gestionar y analizar, ya que su estructura permite una mayor predictibilidad. Ejemplos de datos estructurados incluyen registros en tablas, ficheros XML asociados a un esquema, y facturas autogeneradas al realizar una compra.</li> <li>Datos semiestructurados: son aquellos que tienen cierta estructura, pero no est\u00e1n organizados en una estructura r\u00edgida c\u00f3mo los datos estructurados. Ejemplos de datos semiestructurados incluyen documentos JSON y XML sin esquema asociado.</li> <li>Datos no estructurados: son aquellos que no tienen una estructura predefinida y pueden ser dif\u00edciles de gestionar y analizar. Ejemplos de datos no estructurados incluyen publicaciones en redes sociales, v\u00eddeos, im\u00e1genes, y texto libre.</li> </ul> <p></p>"},{"location":"BDA/Tema01/Introducion_OLD/#17-infraestructura","title":"1.7. Infraestructura","text":"<p>Una infraestructura para Big Data debe facilitar la recopilaci\u00f3n, el almacenamiento y el an\u00e1lisis de grandes vol\u00famenes de datos, que pueden estar en diferentes formatos y gener\u00e1ndose en tiempo real. Para ello, se utilizan tecnolog\u00edas y servicios especiales que han sido creados espec\u00edficamente para dar soluci\u00f3n al procesamiento de estos enormes conjuntos de datos.</p> <p>Algunos de los principales elementos de hardware y software que integran una soluci\u00f3n arquitect\u00f3nica de Big Data son:</p> <ul> <li>Hardware: servidores, dispositivos de almacenamiento, redes de alta velocidad.</li> <li>Software: sistemas operativos, bases de datos, herramientas de an\u00e1lisis y visualizaci\u00f3n de datos.</li> </ul> <p>Estos son algunos ejemplos de infraestructuras para procesar el Big Data:</p> <ul> <li>Apache Hadoop: una de las soluciones m\u00e1s conocidas para analizar Big Data, que utiliza un marco de trabajo de c\u00f3digo abierto para almacenar y procesar grandes conjuntos de datos.</li> <li>Apache Spark: esta herramienta permite almacenar gran parte de los datos de procesamiento en la memoria y en el disco, lo que se traduce en una mayor rapidez.</li> <li>Bases de datos NoSQL: como Cassandra, son utilizadas para almacenar y gestionar grandes vol\u00famenes de datos no estructurados.</li> </ul> <p>La infraestructura para procesar el Big Data debe ser capaz de manejar grandes vol\u00famenes de datos y proporcionar herramientas para su an\u00e1lisis y visualizaci\u00f3n.</p>"},{"location":"BDA/Tema01/Introducion_OLD/#18-almacenamiento","title":"1.8. Almacenamiento","text":"<p>Hay muchas opciones disponibles para almacenar grandes vol\u00famenes de datos. * Los sistemas de archivos distribuidos, como Hadoop Distributed File System (HDFS) y Google File System (GFS), son una opci\u00f3n popular para almacenar grandes conjuntos de datos en m\u00faltiples servidores. * Los servicios de almacenamiento en la nube, como Amazon S3 y Microsoft Azure Blob Storage, tambi\u00e9n son opciones populares para almacenar datos en la nube.</p> <p>La gesti\u00f3n efectiva de los datos es esencial para garantizar su integridad y disponibilidad. Esto incluye: * La implementaci\u00f3n de medidas de seguridad adecuadas para proteger los datos contra el acceso no autorizado * La realizaci\u00f3n de copias de seguridad regulares para garantizar la recuperaci\u00f3n en caso de p\u00e9rdida de datos * La implementaci\u00f3n de pol\u00edticas y procedimientos para garantizar que los datos se manejen de manera responsable y \u00e9tica.</p>"},{"location":"BDA/Tema01/Introducion_OLD/#19-procesamiento","title":"1.9. Procesamiento","text":"<p>Dos opciones de procesamiento:</p> <ul> <li> <p>En tiempo real</p> <ul> <li>El procesamiento en tiempo real del Big Data implica analizar datos a medida que se generan para tomar decisiones o desencadenar acciones en tiempo real.</li> <li>Por ejemplo, el procesamiento en tiempo real puede utilizarse para monitorear feeds sociales en busca de menciones de una marca o producto en particular.</li> </ul> </li> <li> <p>Por lotes o en Batch</p> <ul> <li>El procesamiento por lotes implica analizar grandes vol\u00famenes de datos a la vez en lugar de en tiempo real.</li> <li>Este enfoque puede ser \u00fatil cuando se trata con datos hist\u00f3ricos o cuando se realizan an\u00e1lisis complejos que requieren m\u00e1s tiempo.</li> </ul> </li> </ul>"},{"location":"BDA/Tema01/Introducion_OLD/#110-analisis-con-big-data","title":"1.10. An\u00e1lisis con Big Data","text":"<p>Tipos de an\u00e1lisis de big data:</p> <ul> <li>Predictivo<ul> <li>El an\u00e1lisis predictivo implica utilizar t\u00e9cnicas estad\u00edsticas para hacer predicciones sobre eventos futuros bas\u00e1ndose en datos hist\u00f3ricos. Este enfoque puede aplicarse para generar conocimientos sobre tendencias o comportamientos futuros.</li> </ul> </li> <li>De diagn\u00f3stico<ul> <li>El an\u00e1lisis de diagn\u00f3stico utiliza algoritmos para descubrir patrones y relaciones dentro de grandes conjuntos de datos. Este enfoque puede aplicarse para extraer informaci\u00f3n \u00fatil de grandes vol\u00famenes de datos no estructurados o semiestructurados.</li> </ul> </li> <li>Descriptivo<ul> <li>El an\u00e1lisis descriptivo es un m\u00e9todo de an\u00e1lisis de datos que se utiliza para resumir y describir sus caracter\u00edsticas principales. Se utiliza com\u00fanmente para proporcionar una visi\u00f3n general de los datos y para identificar patrones y tendencias. </li> <li>En el contexto de Big Data, el an\u00e1lisis descriptivo puede utilizarse para obtener informaci\u00f3n a partir de conjuntos de datos grandes y complejos. Por ejemplo, una empresa podr\u00eda utilizar el an\u00e1lisis descriptivo para entender la demograf\u00eda de su base de clientes o para identificar patrones en el comportamiento de los clientes.</li> </ul> </li> <li>Prescriptivo con Big Data<ul> <li>El an\u00e1lisis prescriptivo es un tipo de an\u00e1lisis de datos que utiliza t\u00e9cnicas avanzadas de an\u00e1lisis, como el aprendizaje autom\u00e1tico y los algoritmos de optimizaci\u00f3n, para sugerir acciones que se pueden tomar para lograr un resultado deseado.</li> <li>En el contexto de Big Data, el an\u00e1lisis prescriptivo puede utilizarse para tomar decisiones basadas en datos a partir de la informaci\u00f3n obtenida a partir de conjuntos de datos grandes y complejos. Por ejemplo, una empresa podr\u00eda utilizar el an\u00e1lisis prescriptivo para optimizar su cadena de suministro o para determinar la mejor estrategia de precios para sus productos.</li> </ul> </li> </ul>"},{"location":"BDA/Tema01/Introducion_OLD/#111-visualizacion-de-datos","title":"1.11. Visualizaci\u00f3n de datos","text":"<p>La visualizaci\u00f3n de datos es la presentaci\u00f3n de datos en formato ilustrado o gr\u00e1fico. Permite a los tomadores de decisiones ver la anal\u00edtica presentada de forma visual, de modo que puedan captar conceptos dif\u00edciles o identificar nuevos patrones. Con la visualizaci\u00f3n interactiva, se puede llevar el concepto un paso adelante utilizando tecnolog\u00eda para profundizar en diagramas y gr\u00e1ficas para observar mayor detalle, cambiando de forma interactiva qu\u00e9 datos se ven y c\u00f3mo se procesan.</p> <p>Hay muchas herramientas disponibles para la visualizaci\u00f3n de Big Data. Algunas de las herramientas m\u00e1s populares son Tableau, Infogram, ChartBlocks, Datawrapper y Ploty. Estas herramientas permiten crear tablas, gr\u00e1ficos, mapas y otros tipos de visualizaciones para ayudar a comprender y comunicar los datos.</p> <p>La visualizaci\u00f3n de datos es importante porque permite transmitir conceptos de manera universal y r\u00e1pida. Tambi\u00e9n puede ayudar a identificar \u00e1reas que necesitan atenci\u00f3n o mejoras y esclarecer qu\u00e9 factores influencian el comportamiento de los clientes.</p>"},{"location":"BDA/Tema02/Pruebas/","title":"Pruebas","text":"Pulsa para ver Esta es la soluci\u00f3n:  <pre><code>CREATE DATABASE menagerie;\nCREATE TABLE pet (name VARCHAR(20), owner VARCHAR(20), species VARCHAR(20), sex CHAR(1), birth DATE, death DATE);\n</code></pre> <p>Mi Documento</p> Escenarios <ul> <li>Los datos son demasiado grandes para que las tiendas de datos tradicionales los procesen.</li> <li>Transformaci\u00f3n y an\u00e1lisis de datos no estructurados.</li> <li>Necesidad de analizar datos en tiempo real con baja latencia.</li> </ul> <pre><code>select * from luis\nwhere nombre=\"luis\"from juan\n</code></pre>"},{"location":"BDA/Tema02/Pruebas/#seccion-1","title":"Secci\u00f3n 1","text":"<p>\u00a1Bienvenido a mi documento de ejemplo!</p> <p>Nota Importante</p> <p>Esta es una nota importante que los lectores pueden ocultar o mostrar.</p> <p>Consejo</p> <p>Aqu\u00ed tienes un consejo \u00fatil para los lectores.</p>"},{"location":"BDA/Tema02/Pruebas/#seccion-2","title":"Secci\u00f3n 2","text":"<p>En esta secci\u00f3n, vamos a mostrar c\u00f3mo usar bloques colapsados anidados.</p> Advertencia <p>Esta es una advertencia que se puede colapsar</p> <p>Peligro</p> <p>\u00a1Cuidado! Esto es peligroso.</p> <p>M\u00e1s Informaci\u00f3n</p> <p>Aqu\u00ed hay informaci\u00f3n adicional.</p>"},{"location":"BDA/Tema02/Pruebas/#seccion-3","title":"Secci\u00f3n 3","text":"<p>Continuemos con m\u00e1s contenido.</p> <p>Consejo</p> <p>Otra sugerencia \u00fatil para los lectores.</p>"},{"location":"BDA/Tema02/Pruebas/#seccion-4","title":"Secci\u00f3n 4","text":"<p>Finalizamos el documento.</p> <p>Nota de Cierre</p> <p>Esta es una nota de cierre.</p>"},{"location":"BDA/Tema02/RepasoSQL/","title":"2. SQL","text":""},{"location":"BDA/Tema02/RepasoSQL/#21-comandos","title":"2.1. Comandos","text":"<p>Existen tres tipos de comandos SQL:</p> <ul> <li>Los DDL (Data Definition Language) que permiten crear y definir nuevas bases de datos, campos e \u00edndices.</li> <li>Los DML (Data Manipulation Language) que permiten generar consultas para ordenar, filtrar y extraer datos de la base de datos.</li> <li>Los DCL (Data Control Language) que se encargan de definir los permisos sobre los datos.</li> </ul> <p>Revisaremos los dos primeros.</p>"},{"location":"BDA/Tema02/RepasoSQL/#22-ddl","title":"2.2. DDL","text":""},{"location":"BDA/Tema02/RepasoSQL/#221-create","title":"2.2.1. CREATE","text":"<p>Este comando crea un objeto dentro del gestor de base de datos. Puede ser una base de datos, tabla, \u00edndice, procedimiento almacenado o vista. <pre><code>CREATE DATABASE menagerie;\nCREATE TABLE pet (name VARCHAR(20), owner VARCHAR(20), species VARCHAR(20), sex CHAR(1), birth DATE, death DATE);&lt;details&gt;\n</code></pre></p> Esta es la soluci\u00f3n: <pre><code>CREATE DATABASE menagerie;\nCREATE TABLE pet (name VARCHAR(20), owner VARCHAR(20), species VARCHAR(20), sex CHAR(1), birth DATE, death DATE);\n</code></pre>"},{"location":"BDA/Tema02/RepasoSQL/#222-alter","title":"2.2.2. ALTER","text":"<p>Este comando permite modificar la estructura de un objeto. Se pueden agregar/quitar campos a una tabla, modificar el tipo de un campo, agregar/quitar \u00edndices a una tabla, modificar un trigger, etc. <pre><code>ALTER TABLE 'NOMBRE_TABLA' ADD NUEVO_CAMPO INT;\n</code></pre></p>"},{"location":"BDA/Tema02/RepasoSQL/#223-drop","title":"2.2.3 DROP","text":"<p>Este comando elimina un objeto de la base de datos. Se puede combinar con la sentencia ALTER. <pre><code>DROP TABLE 'NOMBRE_TABLA';\nDROP DATABASE 'BASEDATOS';\nALTER TABLE 'NOMBRE_TABLA' DROP COLUMN NOMBRE_COLUMNA;\n</code></pre></p>"},{"location":"BDA/Tema02/RepasoSQL/#224-truncate","title":"2.2.4. TRUNCATE","text":"<p>Este comando borra todo el contenido de una tabla, pero no borra la tabla. <pre><code>TRUNCATE TABLE 'NOMBRE_TABLA';\n</code></pre></p>"},{"location":"BDA/Tema02/RepasoSQL/#23-dml","title":"2.3. DML","text":""},{"location":"BDA/Tema02/RepasoSQL/#231-insert","title":"2.3.1. INSERT","text":"<p>Una sentencia INSERT de SQL agrega uno o m\u00e1s registros a una (y s\u00f3lo una) tabla en una base de datos relacional. Forma b\u00e1sica: <pre><code>INSERT INTO 'tabla' ('columna1', ['columna2,...']) VALUES ('valor1', ['valor2,...'])\n</code></pre> Ejemplo: <pre><code>INSERT INTO agenda_telefonica VALUES ('Jhonny Aguiar', 080473968);\n</code></pre></p>"},{"location":"BDA/Tema02/RepasoSQL/#232-update","title":"2.3.2 UPDATE","text":"<p>Una sentencia UPDATE de SQL es utilizada para modificar los valores de un conjunto de registros existentes en una tabla. Ejemplo: <pre><code>UPDATE mi_tabla SET campo1 = 'nuevo valor campo1' WHERE campo2 = 'N';\n</code></pre></p>"},{"location":"BDA/Tema02/RepasoSQL/#233-delete","title":"2.3.3. DELETE","text":"<p>Una sentencia DELETE de SQL borra uno o m\u00e1s registros existentes en una tabla.</p> <p>Forma b\u00e1sica: <pre><code>DELETE FROM 'tabla' WHERE 'columna1' = 'valor1'\n</code></pre> Ejemplo: <pre><code>DELETE FROM My_table WHERE field2 = 'N';\n</code></pre></p>"},{"location":"BDA/Tema02/RepasoSQL/#24-clausulas","title":"2.4. Cl\u00e1usulas","text":"<p>Las cl\u00e1usulas son condiciones de modificaci\u00f3n utilizadas para definir los datos que desea seleccionar o manipular.</p> <ul> <li>FROM: Utilizada para especificar la tabla de la cual se van a seleccionar los registros.</li> <li>GROUP BY: Utilizada para separar los registros seleccionados en grupos espec\u00edficos.</li> <li>HAVING: Utilizada para expresar condici\u00f3n que debe satisfacer cada grupo.</li> <li>ORDER BY: Utilizada para ordenar los registros seleccionados de acuerdo con un orden espec\u00edfico.</li> <li>WHERE: Utilizada para determinar los registros seleccionados en la cl\u00e1usula FROM.</li> </ul> <pre><code>SELECT campos FROM tabla\nWHERE campo=X\nORDER BY campo2\nGROUP BY campos3 HAVING suma&gt;3\n</code></pre>"},{"location":"BDA/Tema02/RepasoSQL/#25-operadores","title":"2.5. Operadores","text":"<ul> <li>Operadores L\u00f3gicos: AND, OR y NOT</li> <li>Operadores de comparaci\u00f3n: &lt;, &gt;, &lt;&gt;, &lt;=, &gt;=, BETWEEN, LIKE, In</li> </ul>"},{"location":"BDA/Tema02/RepasoSQL/#26-funciones-de-agregado","title":"2.6. Funciones de agregado","text":"<p>Las funciones de agregado se usan dentro de una cl\u00e1usula SELECT en grupos de registros para devolver un \u00fanico valor que se aplica a un grupo de registros. Funciones: - AVG - COUNT - SUM - MAX - MIN</p>"},{"location":"BDA/Tema02/RepasoSQL/#27-consultas","title":"2.7. Consultas","text":"<p>Consultas de selecci\u00f3n se utilizan para indicar al motor de datos que devuelva informaci\u00f3n de las bases de datos, esta informaci\u00f3n es devuelta en forma de conjunto de registros. Este conjunto de registros es modificable. <pre><code>SELECT Campos FROM Tabla;\nSELECT Nombre, Telefono FROM Clientes;\n</code></pre></p>"},{"location":"BDA/Tema02/RepasoSQL/#271-consultas-de-seleccion-basicas","title":"2.7.1. Consultas de selecci\u00f3n b\u00e1sicas","text":"<p>Ordenar los registros con ORDER BY: <pre><code>SELECT CodigoPostal, Nombre, Telefono FROM Clientes ORDER BY CodigoPostal DESC , Nombre ASC;\n</code></pre></p>"},{"location":"BDA/Tema02/RepasoSQL/#272-consultas-con-predicado","title":"2.7.2. Consultas con predicado","text":"<p>TOP: Devuelve un cierto n\u00famero de registros que entran en un rango especificado por una cl\u00e1usula ORDER BY. <pre><code>SELECT TOP 25 Nombre, Apellido FROM Estudiantes ORDER BY Nota DESC;\n</code></pre></p> <ul> <li>El Motor de base de datos selecciona todos los registros que cumplen las condiciones de la instrucci\u00f3n SQL: <pre><code>SELECT * FROM Empleados;\n</code></pre></li> </ul> <p>DISTINCT: Omite los registros que contienen datos duplicados en los campos seleccionados. Para que los valores de cada campo listado en la instrucci\u00f3n SELECT se incluyan en la consulta deben ser \u00fanicos. <pre><code>SELECT DISTINCT Apellido FROM Empleados;\n</code></pre></p>"},{"location":"BDA/Tema02/RepasoSQL/#28-criterios-de-seleccion","title":"2.8. Criterios de selecci\u00f3n","text":"<p>Operadores l\u00f3gicos: <pre><code>SELECT * FROM Empleados WHERE Edad &gt; 25 AND Edad &lt; 50;\nSELECT * FROM Empleados WHERE (Edad &gt; 25 AND Edad &lt; 50) OR Sueldo = 100;\nSELECT * FROM Empleados WHERE NOT Estado = 'Soltero';\nSELECT * FROM Empleados WHERE (Sueldo &gt; 100 AND Sueldo &lt; 500) OR (Provincia = 'Madrid' AND Estado = 'Casado');\n</code></pre></p> <p>Operador BETWEEN: <pre><code>SELECT * FROM Pedidos WHERE CodPostal Between 28000 And 28999;\n</code></pre></p> <p>Operador LIKE: Se utiliza para comparar una expresi\u00f3n de cadena con una expresi\u00f3n SQL. <pre><code>SELECT * FROM personas WHERE nombre LIKE 'AN%'\n</code></pre> Operador IN: <pre><code>SELECT * FROM Pedidos WHERE Provincia In ('Madrid', 'Barcelona', 'Sevilla');\n</code></pre></p>"},{"location":"BDA/Tema02/RepasoSQL/#29-agregacion","title":"2.9. Agregaci\u00f3n","text":"<p>AVG: <pre><code>SELECT Avg(Gastos) AS Promedio FROM Pedidos WHERE Gastos &gt; 100;\n</code></pre> MAX, MIN: <pre><code>SELECT Min(Gastos) AS ElMin FROM Pedidos WHERE Pais = 'Costa Rica';\nSELECT Max(Gastos) AS ElMax FROM Pedidos WHERE Pais = 'Costa Rica';\n</code></pre> SUM: <pre><code>SELECT Sum(PrecioUnidad * Cantidad) AS Total FROM DetallePedido;\n</code></pre></p> <p>GROUP BY: <pre><code>SELECT Id_Familia, Sum(Stock) FROM Productos GROUP BY Id_Familia;\n</code></pre> HAVING: <pre><code>SELECT Id_Familia Sum(Stock) FROM Productos GROUP BY Id_Familia HAVING Sum(Stock) &gt; 100 AND NombreProducto Like 'BOS%';\n</code></pre></p>"},{"location":"BDA/Tema02/RepasoSQL/#210-claves","title":"2.10. Claves","text":"<p>Claves Primarias y Ajenas: <pre><code>CREATE TABLE Orders ( OrderID int NOT NULL, OrderNumber int NOT NULL, PersonID int, PRIMARY KEY (OrderID), FOREIGN KEY (PersonID) REFERENCES Persons(PersonID)\n</code></pre></p>"},{"location":"BDA/Tema02/RepasoSQL/#211-consultas-multitabla","title":"2.11. Consultas multitabla","text":"<p>Trabajaremos con estas tablas:</p> <p>Tabla Clientes:</p> cid nombre telefono 1 jose 111 2 manuel 222 3 maria 333 4 jesus 4444 <p>Tabla Acciones:</p> aid cid action cantidad 1 2 REDHAT 10 2 2 NOVEL 20 3 4 SUN 30 4 5 FORD 100 <p>JOIN: </p> <p>La sentencia SQL JOIN se utiliza para relacionar varias tablas. Nos permitir\u00e1 obtener un listado de los campos que tienen coincidencias en ambas tablas. <pre><code>SELECT nombre, telefono, accion, cantidad FROM Clientes JOIN Acciones ON Clientes.cid=Acciones.cid;\n</code></pre> | nombre | telefono | action | cantidad |  | -- | -- |-- | -- | | maria | 222 | REDHAT | 10 | | jesus | 4444 | NOVEL | 20 | | jesus | 4444 | SUN | 30 |</p> <p>LEFT JOIN: </p> <p>La sentencia LEFT JOIN nos dar\u00e1 el resultado anterior m\u00e1s los campos de la tabla de la izquierda del JOIN que no tienen coincidencias en la tabla de la derecha. <pre><code>SELECT nombre, telefono, accion, cantidad FROM Clientes LEFT JOIN Acciones ON Clientes.cid=Acciones.cid;\n</code></pre> | nombre | telefono | action | cantidad |  | -- | -- |-- | -- | | jose | 111 | NULL | NULL | | maria | 222 | REDHAT | 10 | | manuel | 333 | NULL | NULL | | jesus | 4444 | NOVEL | 20 | | jesus | 4444 | SUN | 30 |</p> <p>RIGHT JOIN: </p> <p>Id\u00e9ntico funcionamiento que en el caso anterior pero con la tabla que se incluye en la consulta a la derecha del JOIN. <pre><code>SELECT nombre, telefono, accion, cantidad FROM Clientes RIGHT JOIN Acciones ON Clientes.cid=Acciones.cid;\n</code></pre> | nombre | telefono | action | cantidad |  | -- | -- |-- | -- | | maria | 222 | REDHAT | 10 | | jesus | 4444 | NOVEL | 20 | | jesus | 4444 | SUN | 30 | | NULL | NULL | FORD | 100 |</p> <p>UNION y UNION ALL: </p> <p>Podemos combinar el resultado de varias sentencias con UNION o UNION ALL. UNION no nos muestra los resultados duplicados, pero UNION ALL si los muestra. <pre><code>SELECT nombre, telefono, accion, cantidad FROM Clientes LEFT JOIN Acciones ON Clientes.cid=Acciones.cid WHERE accion IS NULL UNION SELECT nombre, telefono, accion, cantidad\nFROM Clientes RIGHT JOIN Acciones ON Clientes.cid=Acciones.cid WHERE nombre IS NULL;1\n</code></pre> | nombre | telefono | action | cantidad |  | -- | -- |-- | -- | | jose | 111 | NULL | NULL | | manuel | 333 | NULL | NULL | | NULL | NULL | FORD | 100 |</p> <p>RESUMEN</p> <p></p>"},{"location":"BDA/Tema02/RepasoSQL/#212-vistas","title":"2.12. Vistas","text":"<p>La cl\u00e1usula CREATE VIEW permite la creaci\u00f3n de vistas. La cl\u00e1usula asigna un nombre a la vista y permite especificar la consulta que la define. <pre><code>CREATE VIEW ClientesConAcciones AS SELECT nombre, telefono, accion, cantidad FROM Clientes JOIN Acciones ON Clientes.cid=Acciones.cid;\n</code></pre></p>"},{"location":"BDA/Tema02/RepasoSQL/#213-funciones-miscelanea","title":"2.13. Funciones Miscel\u00e1nea","text":"<p>CONVERT: </p> <p>Convierte el valor de salida en un tipo de datos espec\u00edfico. <pre><code>CONVERT(valor, tipo)\n</code></pre></p> <p>ISNULL: </p> <p>Por lo general, si no especifica el valor sin valor para su atributo, es probable que termine con algunos valores nulos en la columna. Pero puede lidiar con ellos f\u00e1cilmente usando la funci\u00f3n isnull(). <pre><code>ISNULL(expresi\u00f3n)\n</code></pre></p> <p>IF: </p> <p>Finalmente, la funci\u00f3n m\u00e1s importante que usar\u00e1 en SQL es la funci\u00f3n if (). Le permite definir la condicionalidad if que se encuentra en cualquier lenguaje de programaci\u00f3n. Tiene una sintaxis simple: <pre><code>IF(expresi\u00f3n, valor_si_verdadero, valor_si_falso)\n</code></pre></p>"},{"location":"BDA/Tema03/NoSQL/","title":"NoSQL","text":"<p>= 3. No  == 3.1. Bases de datos No SQL </p>"},{"location":"BDA/Tema03/NoSQL/#bases-de-datos-nosql","title":"Bases de Datos NoSQL","text":""},{"location":"BDA/Tema03/NoSQL/#que-son-las-bases-de-datos-nosql","title":"\u00bfQu\u00e9 son las bases de datos NoSQL?","text":"<p>Las bases de datos NoSQL (NoSQL significa \"Not Only SQL\" o \"No Solo SQL\") son sistemas de gesti\u00f3n de bases de datos dise\u00f1ados para manejar tipos de datos y escenarios de aplicaci\u00f3n que no se ajustan bien a las bases de datos relacionales tradicionales. </p> <p>A diferencia de las bases de datos SQL, que utilizan un esquema fijo y tablas para almacenar datos, las bases de datos NoSQL utilizan diferentes modelos de datos y estructuras de almacenamiento flexibles.</p>"},{"location":"BDA/Tema03/NoSQL/#caracteristicas-de-las-bases-de-datos-nosql","title":"Caracter\u00edsticas de las bases de datos NoSQL:","text":"<ul> <li> <p>Esquema flexible: NoSQL permite almacenar datos sin necesidad de un esquema predefinido.</p> </li> <li> <p>Escalabilidad horizontal: Las bases de datos NoSQL est\u00e1n dise\u00f1adas para escalar horizontalmente.</p> </li> <li> <p>Modelos de datos variados: Existen varios tipos de bases de datos NoSQL, incluyendo bases de datos de documentos, bases de datos de columnas, bases de datos clave-valor y bases de datos de grafos.</p> </li> <li> <p>Alta disponibilidad: Las bases de datos NoSQL suelen garantizar la disponibilidad continua de los datos.</p> </li> </ul>"},{"location":"BDA/Tema03/NoSQL/#ventajas-de-las-bases-de-datos-nosql","title":"Ventajas de las bases de datos NoSQL:","text":"<ul> <li> <p>Escalabilidad: Son ideales para aplicaciones web y m\u00f3viles que requieren escalabilidad r\u00e1pida y eficiente.</p> </li> <li> <p>Flexibilidad: Pueden manejar datos no estructurados o semiestructurados.</p> </li> <li> <p>Rendimiento: Ofrecen un rendimiento m\u00e1s r\u00e1pido para ciertos tipos de consultas.</p> </li> </ul>"},{"location":"BDA/Tema03/NoSQL/#desventajas-de-las-bases-de-datos-nosql","title":"Desventajas de las bases de datos NoSQL:","text":"<ul> <li> <p>Falta de est\u00e1ndares: La diversidad de modelos y sistemas NoSQL dificulta la elecci\u00f3n y la migraci\u00f3n entre sistemas.</p> </li> <li> <p>Menos soporte para consultas complejas: No son ideales para aplicaciones que requieren operaciones complejas de tipo JOIN y agregaci\u00f3n.</p> </li> </ul>"},{"location":"BDA/Tema03/NoSQL/#tipos-de-bases-de-datos-nosql","title":"Tipos de bases de datos NoSQL:","text":"<ul> <li> <p>Bases de datos de documentos:</p> </li> <li> <p>Explicaci\u00f3n: Almacenan datos en formato de documentos semiestructurados, como JSON o XML.</p> </li> <li> <p>Ejemplos:</p> <ul> <li>MongoDB</li> <li>Couchbase</li> </ul> </li> <li> <p>Bases de datos de columnas:</p> </li> <li> <p>Explicaci\u00f3n: Almacenan datos en columnas en lugar de filas.</p> </li> <li> <p>Ejemplos:</p> <ul> <li>Apache Cassandra</li> <li>HBase</li> </ul> </li> <li> <p>Bases de datos clave-valor:</p> </li> <li> <p>Explicaci\u00f3n: Almacenan datos como pares de clave y valor.</p> </li> <li> <p>Ejemplos:</p> <ul> <li>Redis</li> <li>Riak</li> </ul> </li> <li> <p>Bases de datos de grafos:</p> </li> <li> <p>Explicaci\u00f3n: Almacenan datos como nodos y relaciones.</p> </li> <li> <p>Ejemplos:</p> <ul> <li>Neo4j</li> <li>Amazon Neptune</li> </ul> </li> </ul> <p>Recuerda que cada tipo de base de datos NoSQL tiene sus propias caracter\u00edsticas y casos de uso espec\u00edficos. La elecci\u00f3n depender\u00e1 de las necesidades de tu proyecto.</p> <p>== 3.2. MongoDB  === 3.2.1. Conceptos B\u00e1sicos  Los elementos de MongoDB son:   * Bases de Datos:     * Act\u00faa cada una como un contenedor de alto nivel   * Colecciones:     * Una base de datos tendr\u00e1 0 o m\u00e1s colecciones.      * Una colecci\u00f3n es muy similar a lo que entendemos como tabla dentro de un SGDB.   * Documentos:     * Las colecciones contiene 0 o m\u00e1s documentos, por lo que es similar a una fila o registro de un RDMS.</p> <p>==== 3.2.1.1. Documentos</p> <p>El coraz\u00f3n de MongoDB es el documento, un conjunto ordenado de claves con valores asociados. Su representaci\u00f3n como hemos nombrado en el tema anterior es en JSON, un formato muy intuitivo y que no pensamos que requiera mayor explicaci\u00f3n. Este podr\u00eda ser un ejemplo sencillo de un documento que guarda el nombre, apellidos y dedad de una persona. A la izquierda de los dos puntos el nombre del campo y a la derecha el valor. '''json   {   nombre:\u201dJose Antonio\u201d,   apellidos:\u201dGuillem Benedito\u201d,   edad:35   } ''' Las claves de los documentos:   * No pueden ser nulas.   * No pueden contener los caracteres . (punto) y $ (d\u00f3lar).   * Puede contener cualquiera de los dem\u00e1s caracteres UTF-8 existentes.   * Son case-sensitive (sensible a may\u00fasculas y min\u00fasculas), por lo que las claves \u201cnombre\u201d y \u201cNombre\u201d son diferentes, y por tanto consideradas como campos diferentes.   * Las claves dentro de un mismo documento deben ser \u00fanicas, no pueden duplicarse. As\u00ed por ejemplo el siguiente documento no es v\u00e1lido por tener dos veces la clave nota.</p> <p>'''jon   {   nombre:\u201dJose Antonio\u201d,   nota:8.9,   nota:7.2    }   '''</p> <p>Cada documento en Mongo debe tener obligatoriamente un campo _id con valor \u00fanico y que actuar\u00e1 como identificador \u00fanico del documento. Es tan necesario este campo que cuando se guarda un documento sin especificarlo, Mongo autom\u00e1ticamente le asigna uno del tipo ObjectId.</p> <p>==== 3.2.1.2. Colecciones </p> <p>Una colecci\u00f3n es un grupo de documentos, es lo an\u00e1logo a las tablas en el modelo relacional. Las colecciones tienen esquemas din\u00e1micos, lo que significa que los documentos dentro de una colecci\u00f3n pueden tener m\u00faltiples \u201cformas\u201d. Por ejemplo, los siguientes documentos podr\u00edan guardarse en la misma colecci\u00f3n, a pesar de tener diferentes campos, y diferentes tipos de dato.</p> <p>{ nombre:\u201dJose Antonio\u201d, edad:35 }</p> <p>Hay algunas restricciones respecto a nombre que una colecci\u00f3n puede tener:   * La cadena vac\u00eda (\u201c\u201d) no es un nombre v\u00e1lido.   * Lo puede contener el car\u00e1cter null.   * No se pueden crear colecciones cuyo nombre empiece por \u201csystem.\u201d, ya que es un prefijo reservado para colecciones internas.   * No debe contener el car\u00e1cter $ (d\u00f3lar).</p> <p>=== 3.2.2. Operaciones b\u00e1sicas ==== 3.2.2.1. Inserci\u00f3n ===</p> <p>Para insertar un documento en una colecci\u00f3n, utilice el m\u00e9todo:   db.alumno.insert( {\"name\":\"Antonio Cuenca\"})</p> <p>El comando ha a\u00f1adido autom\u00e1ticamente el campo _id de tipo ObjectId, ya que como hemos explicado, todo documento debe tener un identificador \u00fanico. Pero el uso del tipo ObjectId para el campo _id no es obligatorio, podemos utilizar cualquier valor, siempre y cuando garanticemos su unicidad. A continuaci\u00f3n insertamos una nueva alumna, especificando que su _id es el n\u00famero 10 (tipo Long).   db.alumno.insert( { _id:NumberLong(10), name:\"Raquel\",apellidos:\"Gutierrez Garcia\"} )</p> <p>Si queremos insertar m\u00faltiples documentos,podemos hacer la inserci\u00f3n m\u00e1s r\u00e1pida utilizando batch inserts, que permiten insertar en bloque un array de documentos a la colecci\u00f3n. Esto se consigue con solo pasar un array de objetos al comando insert.   db.numerosprimos.insert(   [{_id:2},{_id:3},{_id:5},{_id:7},{_id:11},{_id:13},{_id:17},{_id:19},   {_id:23}] )</p> <p>==== 3.2.2.2. Borrado  Vaya con cuidado, eliminar colecciones completas es muy sencillo en Mongo. Esto borrar\u00e1 todo, tanto la colecci\u00f3n como meta propiedades asociadas a ella o \u00edndices creados sobre campos.   db.alumno.drop() Por otra parte, para eliminar solo documentos de una colecci\u00f3n, tenemos el comando remove, que recibe como par\u00e1metro el criterio de borrado en forma de documento JSON. En ese caso, solo los documentos que cumplen el criterio se eliminar\u00e1n de la colecci\u00f3n.   db.numerosprimos.remove( {_id:23} )</p> <p>==== 3.2.2.3. Modificaci\u00f3n  Para modificar un documento se utiliza el m\u00e9todo update. Este m\u00e9todo recibe dos par\u00e1metros, el primero es el criterio de actualizaci\u00f3n, y el segundo el modificador, que describe los cambios que deben realizarse.</p> <p>Tenemos dos tipos de actualizaci\u00f3n. El cambio completo del documento o reemplazo y la modificaci\u00f3n de algunos elementos del documento</p> <p>Veamos un ejemplo de reemplazo. Creamos un elemento con los siguientes datos:   db.usuarios.insert({name:\"jose\",friends:32,enemies:2})</p> <p>El documento seria:   {     \"_id\" : ObjectId(\"5386c4fdd73aa9d8f663acda\"),     \"name\" : \"jose\",     \"friends\" : 32,     \"enemies\" : 2   }  </p> <p>Ahora lo reemplazamos:   db.usuarios.update( {name:\"jose\"}, {name:\"jose\",relationships: {friends:32,enemies:2}})</p> <p>Y quedar\u00eda as\u00ed:   {     \"_id\" : ObjectId(\"5386c4fdd73aa9d8f663acda\"),     \"name\" : \"jose\",     \"relationships\" : {        \"friends\" : 32,        \"enemies\" : 2     }   }</p> <p>Si lo que queremos es modificar algunos campos utilizaremos el insert, pero con modificadores. Ve\u00e1moslos:</p> <p>$set</p> <p>Asigna el valor a un campo. Si el campo todav\u00eda no existe en el documento lo crear\u00e1. Se utiliza en el segundo par\u00e1metro que se le pasa al comando update.   db.alumno.insert( { name:\"Arturo\", apellidos:\"Leon Zapata\" })   db.alumno.update( {name:\"Arturo\"}, { $set: {edad:17} })</p> <p>Podemos darle valor a varios campos a la vez, simplemente informando los pares clave:valor separados por coma. En este ejemplo damos valor a tres campos diferentes con un solo comando.   db.alumno.update( {name:\"Arturo\"}, { $set:{nota:8.2,orden:12,actitud:\"positiva\"} })</p> <p>$unset</p> <p>Para eliminar cualquier campo de uno o varios documentos lo hacemos tambi\u00e9n con el comando update pero con el modificador $unset. En MongoDB es habitual utilizar los valores 1 y -1 para indicar verdadero y falso respectivamente. En este caso, al especificar valor 1 estamos diciendo que la clave entra dentro del conjunto de campos en los que queremos aplicar el $unset. De esta forma, si queremos eliminar el campo \u201cedad\u201d del documento que guarda la informaci\u00f3n del alumno \u201cArturo\u201d, lo har\u00edamos as\u00ed.   db.alumno.update({name:\"Arturo\"},{$unset:{edad:1}})</p> <p>$inc</p> <p>Este modificador puede utilizarse para incrementar o decrementar el valor num\u00e9rico de una clave existente o para crear una nueva si no existe.   db.alumno.update({name:\"Arturo\"},{$inc:{puntuacion:2}})</p> <p>Otros operadores que podemos utilizar de forma similar son $mul, $min, $max y $currentDate.   db.alumno.update({name:\"Arturo\"},{$min:{puntuacion:3}}) El ejmplo de arriba modifica la puntuacion si esta es menor que3</p> <p>Ahora vamos a ver modificadores de arrays. $push Se utiliza para a\u00f1adir elementos a un array. Si el array no existe lo crea con los elementos indicados en el push, y si ya existe los a\u00f1ade al final del array.   db.alumno.insert({name:\"Sofia\", apellidos:\"Alarcon Sevilla\"})   db.alumno.update({name:\"Sofia\"},{$push:{\"asignaturas\":{name:\"Matematicas\", nota:9.1}}})</p> <p>$pull</p> <p>Hay varias formas de eliminar elementos de un array. Cuando queremos borrar elementos basados en alg\u00fan criterio, el modificador adecuado es $pull. Partimos de un array y borraremos elementos   db.lists.insert({\"todo\": [\"lavar platos\", \"colada\", \"tender\"]}) Para eliminar la colada har\u00edamos.   db.lists.update({},{$pull:{\"todo\":\"colada\"}})</p> <p>$pop</p> <p>Para eliminar el primer o \u00faltimo elemento del array. Para eliminar el \u00faltimo elemento del array:   db.lists.update({},{$pop:{todo:1}}) Para eliminar el prtimer elemento del array:   db.lists.update({},{$pop:{todo:-1}})</p> <p>Upserts</p> <p>Un upsert es un tipo de update espcial. Si no se encuentra ning\u00fan documento que haga matching con el criterio del update, entonces se crear\u00e1 un nuevo documento combinando el criterio y lo que se quiere actualizar. Si se encuentra un documento que haga matching se actualizar\u00e1 normalmente. Para decirle a MongoDB que queremos hacer un upsert, solo hay que pasar al comando update un tercer par\u00e1metro, con valor true. Esto significa que el update se comportar\u00e1 como hemos explicado que lo hace el upsert.    update({...},{...},true)   db.alumno.update({name:\"Sofia\"},{$set:{apellidos:\"Alarcon Revilla\"}},true)</p> <p>Multiples Documentos</p> <p>Para modificar m\u00faltiples documentos, en el tercer par\u00e1metro del update indicaremos {multi: true}.    db.books.update({lang:\"en\"},{$inc: {price: 50.00}}, {multi:true})</p> <p>==== Consultas ==== El m\u00e9todo find es el que se utiliza para hacer queries en Mongo. Es el equivalente al comando select en el modelo relacional. Al consultar una colecci\u00f3n, Mongo nos devolver\u00e1 un subconjunto de documentos, que variar\u00e1 desde el conjunto vaci\u00f3 hasta la colecci\u00f3n completa. Find tiene varios par\u00e1metros de entrada, el primero de ellos especifica los documentos que queremos recuperar, esto es, el criterio de b\u00fasqueda.   db.coleccion1.find({ clave1:valor1 }) La funci\u00f3n anterior nos devolver\u00eda documentos de la colecci\u00f3n coleccion1 cuyo campo clave1 tenga un valor igual a valor1.</p> <p>El valor por defecto para este primer par\u00e1metro es {}, que significa \u201ccualquier documento\u201d. Por tanto una query como db.coleccion1.find({ }) devolver\u00e1 todos los documentos de la colecci\u00f3n coleccion1. Como es el valor por defecto para el criterio de la consulta es equivalente hacer find({ }) y find( ).</p> <p>db.alumno.find({edad:17})   db.alumno.find({name:\"Antonio\"})</p> <p>Igual que en SQL podemos especificar los campos que queremos recuperar (select campo1, campo2, campo3 from tabla ... ), podemos hacerlo tambi\u00e9n en Mongo. Para ello solo tenemos que pasar un segundo par\u00e1metro al m\u00e9todo find, en el que decimos los que queremos.   db.coleccion1.find({ },{ clave3:1, clave4:1 }) Los campos que queremos que sean d====== 2. No SQL ====== ===== Bases de datos No SQL ===== ===== MongoDB ===== ==== Conceptos B\u00e1sicos ==== Los elementos de MongoDB son:   * Bases de Datos:     * Act\u00faa cada una como un contenedor de alto nivel   * Colecciones:     * Una base de datos tendr\u00e1 0 o m\u00e1s colecciones.      * Una colecci\u00f3n es muy similar a lo que entendemos como tabla dentro de un SGDB.   * Documentos:     * Las colecciones contiene 0 o m\u00e1s documentos, por lo que es similar a una fila o registro de un RDMS.</p> <p>=== Documentos ===</p> <p>El coraz\u00f3n de MongoDB es el documento, un conjunto ordenado de claves con valores asociados. Su representaci\u00f3n como hemos nombrado en el tema anterior es en JSON, un formato muy intuitivo y que no pensamos que requiera mayor explicaci\u00f3n. Este podr\u00eda ser un ejemplo sencillo de un documento que guarda el nombre, apellidos y dedad de una persona. A la izquierda de los dos puntos el nombre del campo y a la derecha el valor.</p> <p>{   nombre:\u201dJose Antonio\u201d,   apellidos:\u201dGuillem Benedito\u201d,   edad:35   }</p> <p>Las claves de los documentos:   * No pueden ser nulas.   * No pueden contener los caracteres . (punto) y $ (d\u00f3lar).   * Puede contener cualquiera de los dem\u00e1s caracteres UTF-8 existentes.   * Son case-sensitive (sensible a may\u00fasculas y min\u00fasculas), por lo que las claves \u201cnombre\u201d y \u201cNombre\u201d son diferentes, y por tanto consideradas como campos diferentes.   * Las claves dentro de un mismo documento deben ser \u00fanicas, no pueden duplicarse. As\u00ed por ejemplo el siguiente documento no es v\u00e1lido por tener dos veces la clave nota.</p> <p>'''json   {   nombre:\u201dJose Antonio\u201d,   nota:8.9,   nota:7.2    }   '''</p> <p>Cada documento en Mongo debe tener obligatoriamente un campo _id con valor \u00fanico y que actuar\u00e1 como identificador \u00fanico del documento. Es tan necesario este campo que cuando se guarda un documento sin especificarlo, Mongo autom\u00e1ticamente le asigna uno del tipo ObjectId.</p> <p>=== Colecciones ===</p> <p>Una colecci\u00f3n es un grupo de documentos, es lo an\u00e1logo a las tablas en el modelo relacional. Las colecciones tienen esquemas din\u00e1micos, lo que significa que los documentos dentro de una colecci\u00f3n pueden tener m\u00faltiples \u201cformas\u201d. Por ejemplo, los siguientes documentos podr\u00edan guardarse en la misma colecci\u00f3n, a pesar de tener diferentes campos, y diferentes tipos de dato.</p> <p>{ nombre:\u201dJose Antonio\u201d, edad:35 }</p> <p>Hay algunas restricciones respecto a nombre que una colecci\u00f3n puede tener:   * La cadena vac\u00eda (\u201c\u201d) no es un nombre v\u00e1lido.   * Lo puede contener el car\u00e1cter null.   * No se pueden crear colecciones cuyo nombre empiece por \u201csystem.\u201d, ya que es un prefijo reservado para colecciones internas.   * No debe contener el car\u00e1cter $ (d\u00f3lar).</p> <p>==== Operaciones b\u00e1sicas ==== === Inserci\u00f3n ===</p> <p>Para insertar un documento en una colecci\u00f3n, utilice el m\u00e9todo:   db.alumno.insert( {\"name\":\"Antonio Cuenca\"})</p> <p>El comando ha a\u00f1adido autom\u00e1ticamente el campo _id de tipo ObjectId, ya que como hemos explicado, todo documento debe tener un identificador \u00fanico. Pero el uso del tipo ObjectId para el campo _id no es obligatorio, podemos utilizar cualquier valor, siempre y cuando garanticemos su unicidad. A continuaci\u00f3n insertamos una nueva alumna, especificando que su _id es el n\u00famero 10 (tipo Long).   db.alumno.insert( { _id:NumberLong(10), name:\"Raquel\",apellidos:\"Gutierrez Garcia\"} )</p> <p>Si queremos insertar m\u00faltiples documentos,podemos hacer la inserci\u00f3n m\u00e1s r\u00e1pida utilizando batch inserts, que permiten insertar en bloque un array de documentos a la colecci\u00f3n. Esto se consigue con solo pasar un array de objetos al comando insert.   db.numerosprimos.insert(   [{_id:2},{_id:3},{_id:5},{_id:7},{_id:11},{_id:13},{_id:17},{_id:19},   {_id:23}] )</p> <p>=== Borrado === Vaya con cuidado, eliminar colecciones completas es muy sencillo en Mongo. Esto borrar\u00e1 todo, tanto la colecci\u00f3n como meta propiedades asociadas a ella o \u00edndices creados sobre campos.   db.alumno.drop() Por otra parte, para eliminar solo documentos de una colecci\u00f3n, tenemos el comando remove, que recibe como par\u00e1metro el criterio de borrado en forma de documento JSON. En ese caso, solo los documentos que cumplen el criterio se eliminar\u00e1n de la colecci\u00f3n.   db.numerosprimos.remove( {_id:23} )</p> <p>=== Modificaci\u00f3n === Para modificar un documento se utiliza el m\u00e9todo update. Este m\u00e9todo recibe dos par\u00e1metros, el primero es el criterio de actualizaci\u00f3n, y el segundo el modificador, que describe los cambios que deben realizarse.</p> <p>Tenemos dos tipos de actualizaci\u00f3n. El cambio completo del documento o reemplazo y la modificaci\u00f3n de algunos elementos del documento</p> <p>Veamos un ejemplo de reemplazo. Creamos un elemento con los siguientes datos:   db.usuarios.insert({name:\"jose\",friends:32,enemies:2})</p> <p>El documento seria:   {     \"_id\" : ObjectId(\"5386c4fdd73aa9d8f663acda\"),     \"name\" : \"jose\",     \"friends\" : 32,     \"enemies\" : 2   }  </p> <p>Ahora lo reemplazamos:   db.usuarios.update( {name:\"jose\"}, {name:\"jose\",relationships: {friends:32,enemies:2}})</p> <p>Y quedar\u00eda as\u00ed:   {     \"_id\" : ObjectId(\"5386c4fdd73aa9d8f663acda\"),     \"name\" : \"jose\",     \"relationships\" : {        \"friends\" : 32,        \"enemies\" : 2     }   }</p> <p>Si lo que queremos es modificar algunos campos utilizaremos el insert, pero con modificadores. Ve\u00e1moslos:</p> <p>$set</p> <p>Asigna el valor a un campo. Si el campo todav\u00eda no existe en el documento lo crear\u00e1. Se utiliza en el segundo par\u00e1metro que se le pasa al comando update.   db.alumno.insert( { name:\"Arturo\", apellidos:\"Leon Zapata\" })   db.alumno.update( {name:\"Arturo\"}, { $set: {edad:17} })</p> <p>Podemos darle valor a varios campos a la vez, simplemente informando los pares clave:valor separados por coma. En este ejemplo damos valor a tres campos diferentes con un solo comando.   db.alumno.update( {name:\"Arturo\"}, { $set:{nota:8.2,orden:12,actitud:\"positiva\"} })</p> <p>$unset</p> <p>Para eliminar cualquier campo de uno o varios documentos lo hacemos tambi\u00e9n con el comando update pero con el modificador $unset. En MongoDB es habitual utilizar los valores 1 y -1 para indicar verdadero y falso respectivamente. En este caso, al especificar valor 1 estamos diciendo que la clave entra dentro del conjunto de campos en los que queremos aplicar el $unset. De esta forma, si queremos eliminar el campo \u201cedad\u201d del documento que guarda la informaci\u00f3n del alumno \u201cArturo\u201d, lo har\u00edamos as\u00ed.   db.alumno.update({name:\"Arturo\"},{$unset:{edad:1}})</p> <p>$inc</p> <p>Este modificador puede utilizarse para incrementar o decrementar el valor num\u00e9rico de una clave existente o para crear una nueva si no existe.   db.alumno.update({name:\"Arturo\"},{$inc:{puntuacion:2}})</p> <p>Otros operadores que podemos utilizar de forma similar son $mul, $min, $max y $currentDate.   db.alumno.update({name:\"Arturo\"},{$min:{puntuacion:3}}) El ejmplo de arriba modifica la puntuacion si esta es menor que3</p> <p>Ahora vamos a ver modificadores de arrays. $push Se utiliza para a\u00f1adir elementos a un array. Si el array no existe lo crea con los elementos indicados en el push, y si ya existe los a\u00f1ade al final del array.   db.alumno.insert({name:\"Sofia\", apellidos:\"Alarcon Sevilla\"})   db.alumno.update({name:\"Sofia\"},{$push:{\"asignaturas\":{name:\"Matematicas\", nota:9.1}}})</p> <p>$pull</p> <p>Hay varias formas de eliminar elementos de un array. Cuando queremos borrar elementos basados en alg\u00fan criterio, el modificador adecuado es $pull. Partimos de un array y borraremos elementos   db.lists.insert({\"todo\": [\"lavar platos\", \"colada\", \"tender\"]}) Para eliminar la colada har\u00edamos.   db.lists.update({},{$pull:{\"todo\":\"colada\"}})</p> <p>$pop</p> <p>Para eliminar el primer o \u00faltimo elemento del array. Para eliminar el \u00faltimo elemento del array:   db.lists.update({},{$pop:{todo:1}}) Para eliminar el prtimer elemento del array:   db.lists.update({},{$pop:{todo:-1}})</p> <p>Upserts</p> <p>Un upsert es un tipo de update espcial. Si no se encuentra ning\u00fan documento que haga matching con el criterio del update, entonces se crear\u00e1 un nuevo documento combinando el criterio y lo que se quiere actualizar. Si se encuentra un documento que haga matching se actualizar\u00e1 normalmente. Para decirle a MongoDB que queremos hacer un upsert, solo hay que pasar al comando update un tercer par\u00e1metro, con valor true. Esto significa que el update se comportar\u00e1 como hemos explicado que lo hace el upsert.    update({...},{...},true)   db.alumno.update({name:\"Sofia\"},{$set:{apellidos:\"Alarcon Revilla\"}},true)</p> <p>Multiples Documentos</p> <p>Para modificar m\u00faltiples documentos, en el tercer par\u00e1metro del update indicaremos {multi: true}.    db.books.update({lang:\"en\"},{$inc: {price: 50.00}}, {multi:true})</p> <p>==== Consultas ==== El m\u00e9todo find es el que se utiliza para hacer queries en Mongo. Es el equivalente al comando select en el modelo relacional. Al consultar una colecci\u00f3n, Mongo nos devolver\u00e1 un subconjunto de documentos, que variar\u00e1 desde el conjunto vaci\u00f3 hasta la colecci\u00f3n completa. Find tiene varios par\u00e1metros de entrada, el primero de ellos especifica los documentos que queremos recuperar, esto es, el criterio de b\u00fasqueda.   db.coleccion1.find({ clave1:valor1 }) La funci\u00f3n anterior nos devolver\u00eda documentos de la colecci\u00f3n coleccion1 cuyo campo clave1 tenga un valor igual a valor1.</p> <p>El valor por defecto para este primer par\u00e1metro es {}, que significa \u201ccualquier documento\u201d. Por tanto una query como db.coleccion1.find({ }) devolver\u00e1 todos los documentos de la colecci\u00f3n coleccion1. Como es el valor por defecto para el criterio de la consulta es equivalente hacer find({ }) y find( ).</p> <p>db.alumno.find({edad:17})   db.alumno.find({name:\"Antonio\"})</p> <p>Igual que en SQL podemos especificar los campos que queremos recuperar (select campo1, campo2, campo3 from tabla ... ), podemos hacerlo tambi\u00e9n en Mongo. Para ello solo tenemos que pasar un segundo par\u00e1metro al m\u00e9todo find, en el que decimos los que queremos.   db.coleccion1.find({ },{ clave3:1, clave4:1 }) Los campos que queremos que sean devueltos les pondremos el valor 1. El campo _id siempre se devuelve. Tambi\u00e9n podemos especificar los campos que no queremos que sean devueltos, en este caso como valor para cada una de esos campos, pondremos cero. El resultado ser\u00e1 que se devolver\u00e1n el resto de documentos.   db.coleccion1.find({ },{ clave3:0, clave4:0 })</p> <p>=== Operadores de comparaci\u00f3n === MongoDB permite utilizar los siguientes operadores de comparaci\u00f3n:   * $lt (less than), se corresponde con la condici\u00f3n &lt; (menor estricto).   * $lte (less than or equal), se corresponde con la condici\u00f3n &lt;= (menor o igual).   * $gt (greather than), se corresponde con la condici\u00f3n &gt; (mayor estricto).   * $gte (greather than or equal), se corresponde con la condici\u00f3n &gt;= (mayor o igual).</p> <p>clave: { $operador: valor }</p> <p>As\u00ed si por ejemplo queremos recuperar los libros con un precio mayor que 10 lo har\u00edamos utilizando el operador $gt sobre el campo precio:   db.libro.find({\"precio\":{$gt:10}},{titulo:1,precio:1})</p> <p>=== Operadores l\u00f3gicos ===</p> <p>$in   clave: { $in: [valor1, valor2, valorN] } Recupera documentos cuyo campo clave est\u00e9 entre alguno de los valores especificados en el array, esto es, [ valor1, valor2, valorN ].   db.libro.find({editorial:{$in:[\"Debolsillo\",\"Planeta\",\"Gigamesh\"]}},{titulo:1,editorial:1})</p> <p>$nin   clave: { $nin: [valor1, valor2, valorN] } Recupera documentos cuyo campo clave NO est\u00e9 entre alguno de los valores especificados en el array, esto es, [ valor1, valor2, valorN ].   db.libro.find({editorial:{$nin:[\"Debolsillo\",\"Planeta\",\"Gigamesh\"]}},{titulo:1,editorial:1})</p> <p>$or   $or: [ {clave1:valor1}, {clave2:valor2}, {claveN:valorN} ] Con el siguiente find, vamos a recuperar aquellos documentos que est\u00e9n en stock (enstock:false) o que tengan editorial (editorial:null).   db.libro.find({$or:[{enstock:false},{editorial:null}]},{titulo:1,editorial:1,enstock:1})</p> <p>$nor   $nor: [ {clave1:valor1}, {clave2:valor2}, {claveN:valorN} ] Con el siguiente find, vamos a recuperar aquellos documentos que no est\u00e9n en stock (enstock:false) o que no tengan editorial (editorial:null).   db.libro.find({$nor:[{enstock:false},{editorial:null}]},{titulo:1,editorial:1,enstock:1})</p> <p>$not</p> <p>$not es lo que se conoce como un metacondicional, que son opeadores que pueden aplicarse por encima de cualquier otro criterio. Su sintaxis es sencilla, solo hay que ponerlo \u201cfuera\u201d de aquello que queremos negar.   $not: { criterio } Y as\u00ed directamente podemos recuperar por ejemplo, documentos que no tengan 480 paginas.   db.libro.find( { paginas: {$not: { $eq:480 } } },{titulo:1,paginas:1} )</p> <p>$exist</p> <p>El operador $exists se utiliza para comprobar los documentos que tienen informado o no un campo determinado. Su sintaxis es la siguiente:   clave: { $exists: boolean} Donde boolean puede ser true o false. En el caso de true devuelve documentos que tienen informado un campo, aunque su valor sea null. Este operador es \u00fatil porque debido a la ausencia de definici\u00f3n de esquema de datos de mongo, cada documento de una misma colecci\u00f3n puede tener los campos que requiera, y al hacer consultas puede que no nos devuelva ciertos documentos bien porque no entra dentro de los valores exigidos, bien porque el campo no est\u00e1 definido. Veamos un ejemplo con dos comandos find donde utilizamos el operador $exists con ambos valores.   db.libro.find({paginas:{$exists:true}},{paginas:1})</p> <p>null tiene un comportamiento un poco extra\u00f1o, ya que hace matching en los siguientes casos:   * Con valores que almacenan null, por ejemplo \u201cy\u201d : null   * Con campos que no existen en un documento. Si por ejemplo el documentoA no tiene informado el campo x, una b\u00fasqueda por { x:null } s\u00ed que se encontrar\u00eda. <p></p> <p>Expresiones regulares</p> <p>MongoDB incluye el trabajo con expresiones regulares de forma nativa. Ejemplo: Recuperamos los libros del autor Posteguillo, sin preocuparnos por may\u00fasculas y min\u00fasculas, har\u00edamos esta consulta.   db.libro.find({autor:/posteguillo/i},{titulo:1,autor:1}).pretty()</p> <p>B\u00fasqueda de texto</p> <p>En MongoDB, podemos realizar b\u00fasquedas de texto usando el \u00edndice de texto y el operador $text. Pasos:</p> <p>Primero se crea un \u00edndice de texto en los  campos para realizar la b\u00fasqueda de  texto. </p> <p>db.books.createIndex({\"title\":\"text\"})   //---  Un campo//   db.books.createIndex({\"title\":\"text\",\"autor\":\"text\"}) //---  Varios campos//</p> <p>Luego se realiza la busqueda con $text. La expresi\u00f3n $text tiene la siguiente sintaxis:   {     $text:     {         $search: ,         $lenguage: , Opcional         $caseSensitive: ,Opcional         $diacriticSensitive:  Opcional     }   } <p>Ejemplos:</p> <p>db.books.find({            $text:{$search:\"la\",$caseSensitive:true}            })</p> <p>db.books.find({            $text:{$search:\"la -Biblia\"}    //--- Busca \"la\" pero no \"Biblia\"//           }) </p> <p>==== Consultas con Arrays ====</p> <p>Creamos una colecci\u00f3n para ver los ejemplos   db.comida.insert({fruta:[\"manzana\",\"platano\",\"pera\"]})</p> <p>Para buscar documentos que tengan un elemento concreto dentro del array, es tan simple como poner el nombre del array y el valor que queremos encontrar.   db.comida.findOne({fruta:\"pera\"})</p> <p>Podemos utilizar cualquiera de los operadores aprendidos hasta el momento, como por ejemplo $in, para recuperar documentos que tengan alguno de los valores especificados.   db.comida.findOne({fruta: { $in:[\"pera\",\"platano\",\"melocoton\"] }})</p> <p>Si necesita recuperar arrays que contengan m\u00e1s de un elemento, puede utilizar el operador $all, que permite especificar una lista de elementos. Modificamos la colecci\u00f3n para ver unos ejemplos   db.comida.drop()   db.comida.insert({_id:1, fruta:[\"manzana\",\"platano\",\"melocoton\"]})   db.comida.insert({_id:2, fruta:[\"manzana\",\"kiwi\",\"naranja\"]})   db.comida.insert({_id:3, fruta:[\"cerezas\",\"platano\",\"manzana\"]})</p> <p>Si ahora queremos documentos que tengan los elementos platano y manzana, lo haremos con $all de esta forma.   db.comida.find({fruta: { $all:[\"platano\",\"manzana\"]}})</p> <p>Para recuperar documentos que tengan un valor concreto en cierto \u00edndice delarray, se consigue poniendo dicho \u00edndice (empezando desde cero) tras un punto del array. Es decir: fruta.0 es el primer elemento del array.   db.comida.find({\"fruta.0\":\"cerezas\"})</p> <p>$size</p> <p>Un condicional \u00fatil para las consultas contra arrays es $size, que permite recuperar arrays que tienen un determinado tama\u00f1o. La siguiente consulta muestra los documentos que tenga 2 frutas.   db.comida.find({fruta:{$size:2}})</p> <p>$slice</p> <p>El operador $slice se utiliza para devolver un subconjunto de los elementos delarray. No influye en el criterio de b\u00fasqueda, solo es para especificar a mongo lo que queremos que nos devuelva. Su sintaxis es la siguiente.   clave: { $slice: x} Donde seg\u00fan x:   * x es un entero positivo (p.ej 10), devuelve los primeros x elementos del array clave.   * x es un entero negativo (p.ej -10), devuelve los \u00faltimos x elementos del array clave.   * x es un array (p.ej [2,4], devuelve los elementos desde el 2 al 4, ambos incluidos.</p> <p>Ejemplos:   db.comida.find({},{fruta:{$slice:2}})   db.comida.find({},{fruta:{$slice:-2}})   db.comida.find({},{fruta:{$slice:[1,3]}})</p> <p>Conjuntos de valores</p> <p>Si queremos obtener todos los diferentes valores que existen en un campo, utilizaremos el m\u00e9todo distinct   db.grades.distinct('type')</p> <p>Contar valores</p> <p>Para contar el n\u00famero de documentos, en vez de find usaremos el m\u00e9todo count. Por ejemplo:   db.grades.count({type:\"exam\"})   db.grades.find({type:\"exam\"}).count()    db.grades.count({type:\"essay\", score:{$gt:90}}) ==== Documentos embebidos ====</p> <p>Para hacer queries contra campos de documentos embebidos dentro de otros documentos, solamente hay que poner la \u201cruta\u201d completa de claves separada por puntos. Es decir, si tenemos por ejemplo una estructura como esta para los libros.   db.libro.save({           \"_id\":\"9788408117117\",           \"titulo\":\"Circo M\u00e1ximo\",           \"autor\": {                     nombre:\"Santiago\",                     apellidos:\"Posteguillo G\u00f3mez\",                     nacimiento: {                                  anyo:1967,                                  ciudad:\"Valencia\"                    }           },          \"editorial\":\"Planeta\",          \"enstock\":true,          \"paginas\":1100,          \"precio\":21.75   })</p> <p>Podemos lanzar queries directamente contra los documentos embebidos, autor y nacimiento de esta forma. Para re   db.libro.findOne({\"autor.nombre\":\"Santiago\"})</p> <p>El find anterior recuperar\u00e1 el primer documento donde el campo nombre, del campo autor sea \u201cSantiago\u201d.</p> <p>db.libro.findOne({\"autor.nacimiento.anyo\":{$gt:1965}}) El find anterior recuperar\u00e1 el primer documento donde el campo anyo, del campo nacimiento, del campo autor sea mayor que 1965. ==== Cursores ==== La base de datos devuelve resultados para find utilizando cursores, que realmente es un puntero a los resultados de una query, no los resultados en si mismos. Los clientes integrados con Mongo, iteran sobre los cursores para recuperar los resultados, y ofrecen un conjunto de funcionalidades como limitar el n\u00famero de resultados, saltar para no tener que recuperar obligatoriamente los primeros, ordenarlos, etc.</p> <p>Las opciones de query m\u00e1s comunes son limitar el n\u00famero de resultados, saltar un n\u00famero de resultados, y ordenarlos. Todas estas opciones deben especificarse en la propia query al sistema.</p> <p>Para fijar un l\u00edmite, debe concatenar la funci\u00f3n limit tras la funci\u00f3n find. Por ejemplo, para solo recuperar los primeros tres libros almacenados, use esto:   db.libro.find({},{titulo:1}).limit(3)</p> <p>La funci\u00f3n skip para \u201csaltar\u201d algunos resultados, funciona de forma similar a limit, concaten\u00e1ndola tras find. Con el siguiente skip(3) saltamos los primeros tres t\u00edtulos, y el cursor empezar\u00e1 a devolver a partir del cuarto.  db.libro.find({},{titulo:1}).skip(3)</p> <p>La funci\u00f3n sort sirve para ordenar los resultados que se devuelven. Recibe un objeto json con las claves y un valor 1 (para ordenaci\u00f3n ascendente) o -1 (para ordenaci\u00f3n descendente).   db.libro.find({},{titulo:1,precio:1,paginas:1}).sort({precio:1,paginas:-1})</p> <p>Estas tres funciones, limit, skip y sort, pueden combinarse como se quiera para recuperar exactamente lo que queramos.   db.libro.find({},{titulo:1,precio:1,paginas:1}).limit(3).skip(2).sort({precio:1, paginas:-1}).pretty()</p> <p>==== Agregaci\u00f3n ==== Una vez tenemos datos almacenados en MongoDB, queremos hacer m\u00e1s que solamente recuperarlos, queremos analizarlos de diversas formas interesantes.  Esto lo conseguimos con la funci\u00f3n de agregaci\u00f3n que Mongo ofrece. La agregaci\u00f3n nos permite transformar y combinar documentos en una colecci\u00f3n. B\u00e1sicamente, se construye un pipeline (secuencia de comandos)  que procesan un conjunto de documentos a trav\u00e9s de varios \u201cbloques\u201d: filtrado, proyecciones, agrupaciones, ordenaci\u00f3n, limitaci\u00f3n y skipping.</p> <p>Por ejemplo, si de la colecci\u00f3n de libros de ejemplo quisi\u00e9ramos saber los tres autores con m\u00e1s libros escritos, crear\u00edamos un pipeline con los siguientes pasos:   * Proyectar\u00edamos el autor de cada libro.0   * Agrupar\u00edamos los autores por nombre, contando el n\u00famero de ocurrencias.   * Ordenar\u00edamos los autores por dicho n\u00famero de ocurrencias, descendentemente.   * Limitar\u00edamos el resultado a los primeros tres.</p> <p>Cada uno de estos pasos se corresponde con un operador de agregaci\u00f3n</p> <ol> <li> <p>{\u201c$project\u201d : { \u201cautor\u201d : 1 }} Esto \u201cproyecta/extrae\u201d el autor de cada documento. La sintaxis es similar al selector  de  campos  utilizado  con  find  (el  segundo  par\u00e1metro),  especificando  \u201cnombrecampo\u201d:  1  para  incluir  o  \u201cnombrecampo\u201d:0  para excluir. Despu\u00e9s de esta operaci\u00f3n, cada documento de los  resultados es algo as\u00ed:  {\u201c_id\u201d:id,  \u201cautor\u201d:\u201dnombre  de  autor\u201d}.  Estos  resultados  solo  existen  en memoria, no se escribir\u00e1n nunca a disco</p> </li> <li> <p>{\u201c$group\u201d : { \u201c_id\u201d:\u201d$autor\u201d, \u201ccount\u201d: {\u201c$sum\u201d:1}}}  Eso agrupa por autores e incrementa \u201ccount\u201d para cada documento en los que el autor aparece. Primero especificamos el campo por el que queremos agrupar, que es \u201cautor\u201d. Esto lo indicamos con el \u201c_id\u201d:\u201d$autor\u201d. Podr\u00edamos entenderlo  como: tras el Group habr\u00e1 un documento resultado para cada autor, por lo que \u201cautor\u201d se convertir\u00e1 en el identificador \u00fanico (\u201c_id\u201d). El segundo campo significa a\u00f1adir 1 al campo \u201ccount\u201d para cada documento en el grupo. F\u00edjese  que los documentos de entrada (libro) no tienen el  campo \u201ccount\u201d, es un nuevo campo creado por el \u201c$group\u201d. Al final de este paso, cada documento resultado es algo como: </p> </li> <li> <p>{\u201c$sort\u201d : {\u201ccount\u201d : -1}} Esto reordena los resultados por el valor del campo \u201ccount\u201d  descendentemente (-1). En caso de ascendente ser\u00eda \u201ccount\u201d:1.</p> </li> <li> <p>{\u201c$limit\u201d : 3}  Limita el n\u00famero de resultados a los primeros tres.</p> </li> </ol> <p>Veamos todo esto con m\u00e1s detalle y partiendo de un ejemplo con una colecci\u00f3n de libros</p> <p> db.libro.aggregate({$project:{autor:1}},                    {$group:{_id:\"$autor\",count:{$sum:1}}},                    {$sort:{count:-1}},{$limit:3}                   )    { \"_id\" : \"Santiago Posteguillo\", \"count\" : 3 }   { \"_id\" : \"George R.R. Martin\", \"count\" : 2 }</p> <p>=== Operaciones Pipeline === Cada operador recibe un conjunto de documentos, hace alg\u00fan tipo de transformaci\u00f3n sobre ellos, y despu\u00e9s pasa el resultado de la transformaci\u00f3n.  Esto es lo que se conoce como pipeline.</p> <p>{{ :clase:iabd:bda:mongodb.png?600 |}}</p> <p>== Operador $match ==</p> <p>El operador $match filtra documentos de forma que podamos ejecutar una agregaci\u00f3n sobre un subconjunto de documentos. $match puede utilizar todos los operadores para query estudiados ($gt, $lt, $in, etc). Con el siguiente ejemplo, filtrar\u00edamos solo aquellos libros con un precio mayor que 20, reduciendo el conjunto inicial de documentos y formando la entrada de  los siguientes pasos del pipeline. db.libro.aggregate({$match:{precio:{$gt:20}}})</p> <p>Como hemos comentado, cualquiera de las condiciones estudiadas del find podemos utilizarlas con match. Como por ejemplo, los libros en stock.  db.libro.aggregate({$match:{enstock:true}}) </p> <p>== Operador $project == La \u201cproyecci\u00f3n\u201d es mucho m\u00e1s potente en el pipeline que en el lenguaje \u201cnormal\u201d  de la query. $project nos permite extraer campos de subdocumentos, renombrar  campos, y realizar operaciones interesantes sobre ellos.  La operaci\u00f3n m\u00e1s sencilla de $project realiza una selecci\u00f3n simple de unos documentos de entrada, es decir, de los documentos de entrada con campos a, b, c y d, con $project decimos que nos quedamos \u00fanicamente con a y c. Para incluir o excluir un campo, se utiliza la misma sintaxis que para el segundo argumento de una query, es decir \u201cnombrecampo\u201d: 1 para incluir o \u201cnombrecampo\u201d:0 para excluir. Por defecto, el campo \u201c_id\u201d siempre se devuelve si existe en los documentos de entrada. Lo podemos eliminar por ejemplo utilizando $project de la siguiente forma, en la que devolver\u00edamos todos los campos excepto \u201c_id\u201d. {$project:{ \u201c_id\u201d:0 }}  Con $project tambi\u00e9n podemos renombrar el campo proyectado. </p> <p>Por ejemplo si queremos devolver el campo \u201c_id\u201d de los libros como \u201cisbn\u201d har\u00edamos esto.   db.libro.aggregate({$project:{isbn:\"$_id\"}}) </p> <p>La clave est\u00e1 en $_id, cuando ponemos el s\u00edmbolo del d\u00f3lar ($) m\u00e1s el nombre de un campo, nos estamos refiriendo al valor que tomar\u00e1 dicho campo para cada documento en el entorno de agregaci\u00f3n. Por tanto con $project: {\u201cisbn\u201d:\u201d$_id\u201d} estamos diciendo que proyectamos un nuevo campo isbn como el valor que tendr\u00e1 el campo _id en cada documento de entrada.  Igualmente podr\u00edamos haber renombrado cualquier otro campo, por ejemplo el campo \u201censtock\u201d como \u201cdisponible\u201d.  db.libro.aggregate({$project:{isbn:\"$_id\",disponible:\"$enstock\"}})</p> <p>== Expresiones en el Pipeline ==</p> <p>La operaciones m\u00e1s simples en $project son inclusi\u00f3n, exclusi\u00f3n y nombres de campo (\u201c$nombrecampo\u201d). Aunque hay otras m\u00e1s, mucho m\u00e1s potentes. Podemos utilizar expresiones, que nos permiten combinar m\u00faltiples literales y variables en un valor \u00fanico. </p> <p>Expresiones matem\u00e1ticas. </p> <p>Las expresiones matem\u00e1ticas nos permiten manipular valores num\u00e9ricos contra los que operar, y que normalmente se especifican en un array. </p> <p>Esta es la sintaxis para cada operador matem\u00e1tico.    * \u201c$add\u201d: [ expr1, expr2, ... , exprN ]. Toma uno o mas expresiones y las suma.    * \u201c$substract\u201d: [ expr1, expr2 ]. Resta la expr2 a la expr1.    * \u201c$multiply\u201d: [ expr1, expr2, ..., exp\u00f3n ]. Toma una o m\u00e1s expresiones y las multiplica.    * \u201c$divide\u201d: [ expr1, expr2 ]. Divide la expr1 entre la expr2.    * \u201c$mod\u201d: [ expr1, expr2 ]. Divide la expr1 entre la expr2 y devuelve el resto. </p> <p>Expresiones de fechas. </p> <p>Muchas agregaciones son basadas en el tiempo. \u00bfQu\u00e9 ocurri\u00f3 la semana pasada? \u00bfY el mes pasado?. Es por esto que el entorno de agregaci\u00f3n  tiene  un  conjunto  de  expresiones  que  pueden  utilizarse  para  extraer informaci\u00f3n sobre fechas de una forma muy usable, son: \u201c$year\u201d, \u201c$month\u201d,\u201c$week\u201d, \u201c$dayOfMonth\u201d, \u201c$dayOfWeek\u201d, \u201c$dayOfYear\u201d, \u201c$hour\u201d, \u201c$minute\u201d y \u201c$second\u201d. </p> <p>Con siguiente sentencia de agregaci\u00f3n vamos a recuperar tanto la fecha, como el a\u00f1o de dicha fecha, gracias a la expresi\u00f3n \u201c$year\u201d.</p> <p>db.libro.aggregate({$project:{fecha:1,year:{$year:\"$fecha* }}})</p> <p>Expresiones de Strings.</p> <p>Hay una pocas operaciones b\u00e1sicas sobre cadenas de caracteres que tambi\u00e9n podemos utilizar. Son  estas: </p> <p>\u201c$substr\u201d: [ expr, start, length ]. Devuelve el substring de la expresi\u00f3n expr, empezando en la posici\u00f3n start de la cadena y devolviendo desde ah\u00ed un total de length caracteres. En el siguiente ejemplo proyectamos un campo inicio los primeros quince caracteres del campo t\u00edtulo. db.libro.aggregate({$project: {inicio:{$substr: [\"$titulo\",0,15] }}})</p> <p>\u201c$concat\u201d: [ expr1, expr2, ..., exprN ]. Devuelve un nuevo string resultado de la concatenaci\u00f3n de expr1, expr2 y as\u00ed hasta exprN. En el siguiente ejemplo proyectamos un campo descripci\u00f3n en el que concatenamos el titulo, un gui\u00f3n, y el autor. db.libro.aggregate({$project:{descripcion:{$concat:[\"$titulo\",\" - \",\"$autor\"]},_id:0}})</p> <p>\u201c$toLower\u201d: expr. Devuelve un string en min\u00fasculas.  \u201c$toUpper\u201d: expr. Devuelve un string en may\u00fasculas. </p> <p>Expresiones l\u00f3gicas.</p> <p>Existen algunas operaciones l\u00f3gicas que podemos utilizar. Replanteamos el ejemplo anterior de la proyecci\u00f3n del campo \u201cbarato\u201d utilizando  uno de estos comparadores.  </p> <p>db.libro.aggregate({$project:{barato:{$gt:[15,\"$precio\"]},precio:1} })</p> <p>== Operador $sort == Podemos ordenar por cualquier campo o conjunto de campos utilizando la misma sintaxis que para las queries \u201cnormales\u201d. Si tenemos que ordenar un n\u00famero de documentos muy elevado, es recomendable desde el punto de vista del rendimiento, que hagamos la ordenaci\u00f3n al principio del pipeline y adem\u00e1s tengamos un \u00edndice por el conjunto de campos por los que ordenar. De otra forma, $sort puede ser lento y consumir mucha memoria.  Se pueden utilizar tanto campos existentes como campos proyectados para ordenar. De esta forma podemos por ejemplo completar la agregraci\u00f3n en la que calcul\u00e1bamos la media aritm\u00e9tica del precio de los libros por autor, y ordenar los resultados por dicha media calculada ascendentemente. </p> <p> db.libro.aggregate({$group:{_id:\"$autor\", media:{$avg:\"$precio\"}}},                    {$sort:{media:1}}                   ) </p> <p>Para ordenar cada campo es valor 1, para ordenaci\u00f3n ascendente y -\u00ad\u20101 para descendente. En el siguiente ejemplo ordenamos primero por precio descendente y despu\u00e9s por p\u00e1ginas ascendente.  db.libro.aggregate({$project:{precio:1,paginas:1}},                    {$sort:{precio:-1,paginas:1}}                   ) </p> <p>== Operador $limit ==</p> <p>La funci\u00f3n $limit toma un n\u00famero N y devuelve los primeros N documentos resultantes.   db.libro.aggregate({$project:{precio:1,paginas:1}},                    {$sort:{precio:-1,paginas:1}},                    {$limit:3}                   ) </p> <p>== Operador $skip ==</p> <p>La funci\u00f3n $skip toma un n\u00famero N y desecha los primeros N documentos del conjunto de resultados.   db.libro.aggregate({$project:{precio:1,paginas:1}},                    {$sort:{precio :-1,paginas:1}},                    {$skip:3}                   ) </p> <p>== operador $unwind. ==</p> <p>Dentro de una agregaci\u00f3n podemos utilizar el operador $unwind, se podr\u00eda traducir como \"aplanar\" ya que aplana los datos de un array convirti\u00e9ndo sus elementos en documentos independientes. Puede ser \u00fatil para poder acceder al interior de los elementos de un array cuando estos no son at\u00f3micos.</p> <p> db.usuario.aggregate({$project: {comentarios:1}},                      {$unwind :\"$comentarios\" }                     ) evueltos les pondremos el valor 1. El campo _id siempre se devuelve. Tambi\u00e9n podemos especificar los campos que no queremos que sean devueltos, en este caso como valor para cada una de esos campos, pondremos cero. El resultado ser\u00e1 que se devolver\u00e1n el resto de documentos.   db.coleccion1.find({ },{ clave3:0, clave4:0 })</p> <p>=== Operadores de comparaci\u00f3n === MongoDB permite utilizar los siguientes operadores de comparaci\u00f3n:   * $lt (less than), se corresponde con la condici\u00f3n &lt; (menor estricto).   * $lte (less than or equal), se corresponde con la condici\u00f3n &lt;= (menor o igual).   * $gt (greather than), se corresponde con la condici\u00f3n &gt; (mayor estricto).   * $gte (greather than or equal), se corresponde con la condici\u00f3n &gt;= (mayor o igual).</p> <p>clave: { $operador: valor }</p> <p>As\u00ed si por ejemplo queremos recuperar los libros con un precio mayor que 10 lo har\u00edamos utilizando el operador $gt sobre el campo precio:   db.libro.find({\"precio\":{$gt:10}},{titulo:1,precio:1})</p> <p>=== Operadores l\u00f3gicos ===</p> <p>$in   clave: { $in: [valor1, valor2, valorN] } Recupera documentos cuyo campo clave est\u00e9 entre alguno de los valores especificados en el array, esto es, [ valor1, valor2, valorN ].   db.libro.find({editorial:{$in:[\"Debolsillo\",\"Planeta\",\"Gigamesh\"]}},{titulo:1,editorial:1})</p> <p>$nin   clave: { $nin: [valor1, valor2, valorN] } Recupera documentos cuyo campo clave NO est\u00e9 entre alguno de los valores especificados en el array, esto es, [ valor1, valor2, valorN ].   db.libro.find({editorial:{$nin:[\"Debolsillo\",\"Planeta\",\"Gigamesh\"]}},{titulo:1,editorial:1})</p> <p>$or   $or: [ {clave1:valor1}, {clave2:valor2}, {claveN:valorN} ] Con el siguiente find, vamos a recuperar aquellos documentos que est\u00e9n en stock (enstock:false) o que tengan editorial (editorial:null).   db.libro.find({$or:[{enstock:false},{editorial:null}]},{titulo:1,editorial:1,enstock:1})</p> <p>$nor   $nor: [ {clave1:valor1}, {clave2:valor2}, {claveN:valorN} ] Con el siguiente find, vamos a recuperar aquellos documentos que no est\u00e9n en stock (enstock:false) o que no tengan editorial (editorial:null).   db.libro.find({$nor:[{enstock:false},{editorial:null}]},{titulo:1,editorial:1,enstock:1})</p> <p>$not</p> <p>$not es lo que se conoce como un metacondicional, que son opeadores que pueden aplicarse por encima de cualquier otro criterio. Su sintaxis es sencilla, solo hay que ponerlo \u201cfuera\u201d de aquello que queremos negar.   $not: { criterio } Y as\u00ed directamente podemos recuperar por ejemplo, documentos que no tengan 480 paginas.   db.libro.find( { paginas: {$not: { $eq:480 } } },{titulo:1,paginas:1} )</p> <p>$exist</p> <p>El operador $exists se utiliza para comprobar los documentos que tienen informado o no un campo determinado. Su sintaxis es la siguiente:   clave: { $exists: boolean} Donde boolean puede ser true o false. En el caso de true devuelve documentos que tienen informado un campo, aunque su valor sea null. Este operador es \u00fatil porque debido a la ausencia de definici\u00f3n de esquema de datos de mongo, cada documento de una misma colecci\u00f3n puede tener los campos que requiera, y al hacer consultas puede que no nos devuelva ciertos documentos bien porque no entra dentro de los valores exigidos, bien porque el campo no est\u00e1 definido. Veamos un ejemplo con dos comandos find donde utilizamos el operador $exists con ambos valores.   db.libro.find({paginas:{$exists:true}},{paginas:1})</p> <p>null tiene un comportamiento un poco extra\u00f1o, ya que hace matching en los siguientes casos:   * Con valores que almacenan null, por ejemplo \u201cy\u201d : null   * Con campos que no existen en un documento. Si por ejemplo el documentoA no tiene informado el campo x, una b\u00fasqueda por { x:null } s\u00ed que se encontrar\u00eda. <p></p> <p>Expresiones regulares</p> <p>MongoDB incluye el trabajo con expresiones regulares de forma nativa. Ejemplo: Recuperamos los libros del autor Posteguillo, sin preocuparnos por may\u00fasculas y min\u00fasculas, har\u00edamos esta consulta.   db.libro.find({autor:/posteguillo/i},{titulo:1,autor:1}).pretty()</p> <p>B\u00fasqueda de texto</p> <p>En MongoDB, podemos realizar b\u00fasquedas de texto usando el \u00edndice de texto y el operador $text. Pasos:</p> <p>Primero se crea un \u00edndice de texto en los  campos para realizar la b\u00fasqueda de  texto. </p> <p>db.books.createIndex({\"title\":\"text\"})   //---  Un campo//   db.books.createIndex({\"title\":\"text\",\"autor\":\"text\"}) //---  Varios campos//</p> <p>Luego se realiza la busqueda con $text. La expresi\u00f3n $text tiene la siguiente sintaxis:   {     $text:     {         $search: ,         $lenguage: , Opcional         $caseSensitive: ,Opcional         $diacriticSensitive:  Opcional     }   } <p>Ejemplos:</p> <p>db.books.find({            $text:{$search:\"la\",$caseSensitive:true}            })</p> <p>db.books.find({            $text:{$search:\"la -Biblia\"}    //--- Busca \"la\" pero no \"Biblia\"//           }) </p> <p>==== Consultas con Arrays ====</p> <p>Creamos una colecci\u00f3n para ver los ejemplos   db.comida.insert({fruta:[\"manzana\",\"platano\",\"pera\"]})</p> <p>Para buscar documentos que tengan un elemento concreto dentro del array, es tan simple como poner el nombre del array y el valor que queremos encontrar.   db.comida.findOne({fruta:\"pera\"})</p> <p>Podemos utilizar cualquiera de los operadores aprendidos hasta el momento, como por ejemplo $in, para recuperar documentos que tengan alguno de los valores especificados.   db.comida.findOne({fruta: { $in:[\"pera\",\"platano\",\"melocoton\"] }})</p> <p>Si necesita recuperar arrays que contengan m\u00e1s de un elemento, puede utilizar el operador $all, que permite especificar una lista de elementos. Modificamos la colecci\u00f3n para ver unos ejemplos   db.comida.drop()   db.comida.insert({_id:1, fruta:[\"manzana\",\"platano\",\"melocoton\"]})   db.comida.insert({_id:2, fruta:[\"manzana\",\"kiwi\",\"naranja\"]})   db.comida.insert({_id:3, fruta:[\"cerezas\",\"platano\",\"manzana\"]})</p> <p>Si ahora queremos documentos que tengan los elementos platano y manzana, lo haremos con $all de esta forma.   db.comida.find({fruta: { $all:[\"platano\",\"manzana\"]}})</p> <p>Para recuperar documentos que tengan un valor concreto en cierto \u00edndice delarray, se consigue poniendo dicho \u00edndice (empezando desde cero) tras un punto del array. Es decir: fruta.0 es el primer elemento del array.   db.comida.find({\"fruta.0\":\"cerezas\"})</p> <p>$size</p> <p>Un condicional \u00fatil para las consultas contra arrays es $size, que permite recuperar arrays que tienen un determinado tama\u00f1o. La siguiente consulta muestra los documentos que tenga 2 frutas.   db.comida.find({fruta:{$size:2}})</p> <p>$slice</p> <p>El operador $slice se utiliza para devolver un subconjunto de los elementos delarray. No influye en el criterio de b\u00fasqueda, solo es para especificar a mongo lo que queremos que nos devuelva. Su sintaxis es la siguiente.   clave: { $slice: x} Donde seg\u00fan x:   * x es un entero positivo (p.ej 10), devuelve los primeros x elementos del array clave.   * x es un entero negativo (p.ej -10), devuelve los \u00faltimos x elementos del array clave.   * x es un array (p.ej [2,4], devuelve los elementos desde el 2 al 4, ambos incluidos.</p> <p>Ejemplos:   db.comida.find({},{fruta:{$slice:2}})   db.comida.find({},{fruta:{$slice:-2}})   db.comida.find({},{fruta:{$slice:[1,3]}})</p> <p>Conjuntos de valores</p> <p>Si queremos obtener todos los diferentes valores que existen en un campo, utilizaremos el m\u00e9todo distinct   db.grades.distinct('type')</p> <p>Contar valores</p> <p>Para contar el n\u00famero de documentos, en vez de find usaremos el m\u00e9todo count. Por ejemplo:   db.grades.count({type:\"exam\"})   db.grades.find({type:\"exam\"}).count()    db.grades.count({type:\"essay\", score:{$gt:90}}) ==== Documentos embebidos ====</p> <p>Para hacer queries contra campos de documentos embebidos dentro de otros documentos, solamente hay que poner la \u201cruta\u201d completa de claves separada por puntos. Es decir, si tenemos por ejemplo una estructura como esta para los libros.   db.libro.save({           \"_id\":\"9788408117117\",           \"titulo\":\"Circo M\u00e1ximo\",           \"autor\": {                     nombre:\"Santiago\",                     apellidos:\"Posteguillo G\u00f3mez\",                     nacimiento: {                                  anyo:1967,                                  ciudad:\"Valencia\"                    }           },          \"editorial\":\"Planeta\",          \"enstock\":true,          \"paginas\":1100,          \"precio\":21.75   })</p> <p>Podemos lanzar queries directamente contra los documentos embebidos, autor y nacimiento de esta forma. Para re   db.libro.findOne({\"autor.nombre\":\"Santiago\"})</p> <p>El find anterior recuperar\u00e1 el primer documento donde el campo nombre, del campo autor sea \u201cSantiago\u201d.</p> <p>db.libro.findOne({\"autor.nacimiento.anyo\":{$gt:1965}}) El find anterior recuperar\u00e1 el primer documento donde el campo anyo, del campo nacimiento, del campo autor sea mayor que 1965. ==== Cursores ==== La base de datos devuelve resultados para find utilizando cursores, que realmente es un puntero a los resultados de una query, no los resultados en si mismos. Los clientes integrados con Mongo, iteran sobre los cursores para recuperar los resultados, y ofrecen un conjunto de funcionalidades como limitar el n\u00famero de resultados, saltar para no tener que recuperar obligatoriamente los primeros, ordenarlos, etc.</p> <p>Las opciones de query m\u00e1s comunes son limitar el n\u00famero de resultados, saltar un n\u00famero de resultados, y ordenarlos. Todas estas opciones deben especificarse en la propia query al sistema.</p> <p>Para fijar un l\u00edmite, debe concatenar la funci\u00f3n limit tras la funci\u00f3n find. Por ejemplo, para solo recuperar los primeros tres libros almacenados, use esto:   db.libro.find({},{titulo:1}).limit(3)</p> <p>La funci\u00f3n skip para \u201csaltar\u201d algunos resultados, funciona de forma similar a limit, concaten\u00e1ndola tras find. Con el siguiente skip(3) saltamos los primeros tres t\u00edtulos, y el cursor empezar\u00e1 a devolver a partir del cuarto.  db.libro.find({},{titulo:1}).skip(3)</p> <p>La funci\u00f3n sort sirve para ordenar los resultados que se devuelven. Recibe un objeto json con las claves y un valor 1 (para ordenaci\u00f3n ascendente) o -1 (para ordenaci\u00f3n descendente).   db.libro.find({},{titulo:1,precio:1,paginas:1}).sort({precio:1,paginas:-1})</p> <p>Estas tres funciones, limit, skip y sort, pueden combinarse como se quiera para recuperar exactamente lo que queramos.   db.libro.find({},{titulo:1,precio:1,paginas:1}).limit(3).skip(2).sort({precio:1, paginas:-1}).pretty()</p> <p>==== Agregaci\u00f3n ==== Una vez tenemos datos almacenados en MongoDB, queremos hacer m\u00e1s que solamente recuperarlos, queremos analizarlos de diversas formas interesantes.  Esto lo conseguimos con la funci\u00f3n de agregaci\u00f3n que Mongo ofrece. La agregaci\u00f3n nos permite transformar y combinar documentos en una colecci\u00f3n. B\u00e1sicamente, se construye un pipeline (secuencia de comandos)  que procesan un conjunto de documentos a trav\u00e9s de varios \u201cbloques\u201d: filtrado, proyecciones, agrupaciones, ordenaci\u00f3n, limitaci\u00f3n y skipping.</p> <p>Por ejemplo, si de la colecci\u00f3n de libros de ejemplo quisi\u00e9ramos saber los tres autores con m\u00e1s libros escritos, crear\u00edamos un pipeline con los siguientes pasos:   * Proyectar\u00edamos el autor de cada libro.0   * Agrupar\u00edamos los autores por nombre, contando el n\u00famero de ocurrencias.   * Ordenar\u00edamos los autores por dicho n\u00famero de ocurrencias, descendentemente.   * Limitar\u00edamos el resultado a los primeros tres.</p> <p>Cada uno de estos pasos se corresponde con un operador de agregaci\u00f3n</p> <ol> <li> <p>{\u201c$project\u201d : { \u201cautor\u201d : 1 }} Esto \u201cproyecta/extrae\u201d el autor de cada documento. La sintaxis es similar al selector  de  campos  utilizado  con  find  (el  segundo  par\u00e1metro),  especificando  \u201cnombrecampo\u201d:  1  para  incluir  o  \u201cnombrecampo\u201d:0  para excluir. Despu\u00e9s de esta operaci\u00f3n, cada documento de los  resultados es algo as\u00ed:  {\u201c_id\u201d:id,  \u201cautor\u201d:\u201dnombre  de  autor\u201d}.  Estos  resultados  solo  existen  en memoria, no se escribir\u00e1n nunca a disco</p> </li> <li> <p>{\u201c$group\u201d : { \u201c_id\u201d:\u201d$autor\u201d, \u201ccount\u201d: {\u201c$sum\u201d:1}}}  Eso agrupa por autores e incrementa \u201ccount\u201d para cada documento en los que el autor aparece. Primero especificamos el campo por el que queremos agrupar, que es \u201cautor\u201d. Esto lo indicamos con el \u201c_id\u201d:\u201d$autor\u201d. Podr\u00edamos entenderlo  como: tras el Group habr\u00e1 un documento resultado para cada autor, por lo que \u201cautor\u201d se convertir\u00e1 en el identificador \u00fanico (\u201c_id\u201d). El segundo campo significa a\u00f1adir 1 al campo \u201ccount\u201d para cada documento en el grupo. F\u00edjese  que los documentos de entrada (libro) no tienen el  campo \u201ccount\u201d, es un nuevo campo creado por el \u201c$group\u201d. Al final de este paso, cada documento resultado es algo como: </p> </li> <li> <p>{\u201c$sort\u201d : {\u201ccount\u201d : -1}} Esto reordena los resultados por el valor del campo \u201ccount\u201d  descendentemente (-1). En caso de ascendente ser\u00eda \u201ccount\u201d:1.</p> </li> <li> <p>{\u201c$limit\u201d : 3}  Limita el n\u00famero de resultados a los primeros tres.</p> </li> </ol> <p>Veamos todo esto con m\u00e1s detalle y partiendo de un ejemplo con una colecci\u00f3n de libros</p> <p> db.libro.aggregate({$project:{autor:1}},                    {$group:{_id:\"$autor\",count:{$sum:1}}},                    {$sort:{count:-1}},{$limit:3}                   )    { \"_id\" : \"Santiago Posteguillo\", \"count\" : 3 }   { \"_id\" : \"George R.R. Martin\", \"count\" : 2 }</p> <p>=== Operaciones Pipeline === Cada operador recibe un conjunto de documentos, hace alg\u00fan tipo de transformaci\u00f3n sobre ellos, y despu\u00e9s pasa el resultado de la transformaci\u00f3n.  Esto es lo que se conoce como pipeline.</p> <p>{{ :clase:iabd:bda:mongodb.png?600 |}}</p> <p>== Operador $match ==</p> <p>El operador $match filtra documentos de forma que podamos ejecutar una agregaci\u00f3n sobre un subconjunto de documentos. $match puede utilizar todos los operadores para query estudiados ($gt, $lt, $in, etc). Con el siguiente ejemplo, filtrar\u00edamos solo aquellos libros con un precio mayor que 20, reduciendo el conjunto inicial de documentos y formando la entrada de  los siguientes pasos del pipeline. db.libro.aggregate({$match:{precio:{$gt:20}}})</p> <p>Como hemos comentado, cualquiera de las condiciones estudiadas del find podemos utilizarlas con match. Como por ejemplo, los libros en stock.  db.libro.aggregate({$match:{enstock:true}}) </p> <p>== Operador $project == La \u201cproyecci\u00f3n\u201d es mucho m\u00e1s potente en el pipeline que en el lenguaje \u201cnormal\u201d  de la query. $project nos permite extraer campos de subdocumentos, renombrar  campos, y realizar operaciones interesantes sobre ellos.  La operaci\u00f3n m\u00e1s sencilla de $project realiza una selecci\u00f3n simple de unos documentos de entrada, es decir, de los documentos de entrada con campos a, b, c y d, con $project decimos que nos quedamos \u00fanicamente con a y c. Para incluir o excluir un campo, se utiliza la misma sintaxis que para el segundo argumento de una query, es decir \u201cnombrecampo\u201d: 1 para incluir o \u201cnombrecampo\u201d:0 para excluir. Por defecto, el campo \u201c_id\u201d siempre se devuelve si existe en los documentos de entrada. Lo podemos eliminar por ejemplo utilizando $project de la siguiente forma, en la que devolver\u00edamos todos los campos excepto \u201c_id\u201d. {$project:{ \u201c_id\u201d:0 }}  Con $project tambi\u00e9n podemos renombrar el campo proyectado. </p> <p>Por ejemplo si queremos devolver el campo \u201c_id\u201d de los libros como \u201cisbn\u201d har\u00edamos esto.   db.libro.aggregate({$project:{isbn:\"$_id\"}}) </p> <p>La clave est\u00e1 en $_id, cuando ponemos el s\u00edmbolo del d\u00f3lar ($) m\u00e1s el nombre de un campo, nos estamos refiriendo al valor que tomar\u00e1 dicho campo para cada documento en el entorno de agregaci\u00f3n. Por tanto con $project: {\u201cisbn\u201d:\u201d$_id\u201d} estamos diciendo que proyectamos un nuevo campo isbn como el valor que tendr\u00e1 el campo _id en cada documento de entrada.  Igualmente podr\u00edamos haber renombrado cualquier otro campo, por ejemplo el campo \u201censtock\u201d como \u201cdisponible\u201d.  db.libro.aggregate({$project:{isbn:\"$_id\",disponible:\"$enstock\"}})</p> <p>== Expresiones en el Pipeline ==</p> <p>La operaciones m\u00e1s simples en $project son inclusi\u00f3n, exclusi\u00f3n y nombres de campo (\u201c$nombrecampo\u201d). Aunque hay otras m\u00e1s, mucho m\u00e1s potentes. Podemos utilizar expresiones, que nos permiten combinar m\u00faltiples literales y variables en un valor \u00fanico. </p> <p>Expresiones matem\u00e1ticas. </p> <p>Las expresiones matem\u00e1ticas nos permiten manipular valores num\u00e9ricos contra los que operar, y que normalmente se especifican en un array. </p> <p>Esta es la sintaxis para cada operador matem\u00e1tico.    * \u201c$add\u201d: [ expr1, expr2, ... , exprN ]. Toma uno o mas expresiones y las suma.    * \u201c$substract\u201d: [ expr1, expr2 ]. Resta la expr2 a la expr1.    * \u201c$multiply\u201d: [ expr1, expr2, ..., exp\u00f3n ]. Toma una o m\u00e1s expresiones y las multiplica.    * \u201c$divide\u201d: [ expr1, expr2 ]. Divide la expr1 entre la expr2.    * \u201c$mod\u201d: [ expr1, expr2 ]. Divide la expr1 entre la expr2 y devuelve el resto. </p> <p>Expresiones de fechas. </p> <p>Muchas agregaciones son basadas en el tiempo. \u00bfQu\u00e9 ocurri\u00f3 la semana pasada? \u00bfY el mes pasado?. Es por esto que el entorno de agregaci\u00f3n  tiene  un  conjunto  de  expresiones  que  pueden  utilizarse  para  extraer informaci\u00f3n sobre fechas de una forma muy usable, son: \u201c$year\u201d, \u201c$month\u201d,\u201c$week\u201d, \u201c$dayOfMonth\u201d, \u201c$dayOfWeek\u201d, \u201c$dayOfYear\u201d, \u201c$hour\u201d, \u201c$minute\u201d y \u201c$second\u201d. </p> <p>Con siguiente sentencia de agregaci\u00f3n vamos a recuperar tanto la fecha, como el a\u00f1o de dicha fecha, gracias a la expresi\u00f3n \u201c$year\u201d.</p> <p>db.libro.aggregate({$project:{fecha:1,year:{$year:\"$fecha* }}})</p> <p>Expresiones de Strings.</p> <p>Hay una pocas operaciones b\u00e1sicas sobre cadenas de caracteres que tambi\u00e9n podemos utilizar. Son  estas: </p> <p>\u201c$substr\u201d: [ expr, start, length ]. Devuelve el substring de la expresi\u00f3n expr, empezando en la posici\u00f3n start de la cadena y devolviendo desde ah\u00ed un total de length caracteres. En el siguiente ejemplo proyectamos un campo inicio los primeros quince caracteres del campo t\u00edtulo. db.libro.aggregate({$project: {inicio:{$substr: [\"$titulo\",0,15] }}})</p> <p>\u201c$concat\u201d: [ expr1, expr2, ..., exprN ]. Devuelve un nuevo string resultado de la concatenaci\u00f3n de expr1, expr2 y as\u00ed hasta exprN. En el siguiente ejemplo proyectamos un campo descripci\u00f3n en el que concatenamos el titulo, un gui\u00f3n, y el autor. db.libro.aggregate({$project:{descripcion:{$concat:[\"$titulo\",\" - \",\"$autor\"]},_id:0}})</p> <p>\u201c$toLower\u201d: expr. Devuelve un string en min\u00fasculas.  \u201c$toUpper\u201d: expr. Devuelve un string en may\u00fasculas. </p> <p>Expresiones l\u00f3gicas.</p> <p>Existen algunas operaciones l\u00f3gicas que podemos utilizar. Replanteamos el ejemplo anterior de la proyecci\u00f3n del campo \u201cbarato\u201d utilizando  uno de estos comparadores.  </p> <p>db.libro.aggregate({$project:{barato:{$gt:[15,\"$precio\"]},precio:1} })</p> <p>== Operador $sort == Podemos ordenar por cualquier campo o conjunto de campos utilizando la misma sintaxis que para las queries \u201cnormales\u201d. Si tenemos que ordenar un n\u00famero de documentos muy elevado, es recomendable desde el punto de vista del rendimiento, que hagamos la ordenaci\u00f3n al principio del pipeline y adem\u00e1s tengamos un \u00edndice por el conjunto de campos por los que ordenar. De otra forma, $sort puede ser lento y consumir mucha memoria.  Se pueden utilizar tanto campos existentes como campos proyectados para ordenar. De esta forma podemos por ejemplo completar la agregraci\u00f3n en la que calcul\u00e1bamos la media aritm\u00e9tica del precio de los libros por autor, y ordenar los resultados por dicha media calculada ascendentemente. </p> <p> db.libro.aggregate({$group:{_id:\"$autor\", media:{$avg:\"$precio\"}}},                    {$sort:{media:1}}                   ) </p> <p>Para ordenar cada campo es valor 1, para ordenaci\u00f3n ascendente y -\u00ad\u20101 para descendente. En el siguiente ejemplo ordenamos primero por precio descendente y despu\u00e9s por p\u00e1ginas ascendente.  db.libro.aggregate({$project:{precio:1,paginas:1}},                    {$sort:{precio:-1,paginas:1}}                   ) </p> <p>== Operador $limit ==</p> <p>La funci\u00f3n $limit toma un n\u00famero N y devuelve los primeros N documentos resultantes.   db.libro.aggregate({$project:{precio:1,paginas:1}},                    {$sort:{precio:-1,paginas:1}},                    {$limit:3}                   ) </p> <p>== Operador $skip ==</p> <p>La funci\u00f3n $skip toma un n\u00famero N y desecha los primeros N documentos del conjunto de resultados.   db.libro.aggregate({$project:{precio:1,paginas:1}},                    {$sort:{precio :-1,paginas:1}},                    {$skip:3}                   ) </p> <p>== operador $unwind. ==</p> <p>Dentro de una agregaci\u00f3n podemos utilizar el operador $unwind, se podr\u00eda traducir como \"aplanar\" ya que aplana los datos de un array convirti\u00e9ndo sus elementos en documentos independientes. Puede ser \u00fatil para poder acceder al interior de los elementos de un array cuando estos no son at\u00f3micos.</p> <p> db.usuario.aggregate({$project: {comentarios:1}},                      {$unwind :\"$comentarios\" }                     ) </p>"},{"location":"BDA/Tema04/Almacenamiento/","title":"Almacenamiento","text":""},{"location":"SBD/IndiceSBD/","title":"Sistemas Big Data","text":""},{"location":"SBD/IndiceSBD/#temas","title":"Temas","text":"<ul> <li>T1 Big Data con AWS </li> <li>T12 AWS </li> </ul>"},{"location":"SBD/Tema01/AWSCLI/","title":"Documentaci\u00f3n: AWS Command Line Interface (AWS CLI)","text":""},{"location":"SBD/Tema01/AWSCLI/#introduccion-a-aws-cli","title":"Introducci\u00f3n a AWS CLI","text":"<p>AWS CLI es una interfaz de l\u00ednea de comandos que te permite interactuar con los servicios y recursos de Amazon Web Services (AWS) directamente desde tu terminal. Con AWS CLI, puedes administrar y configurar recursos de AWS, automatizar tareas y realizar operaciones de forma eficiente sin necesidad de utilizar la interfaz web de AWS.</p>"},{"location":"SBD/Tema01/AWSCLI/#instalacion-de-aws-cli","title":"Instalaci\u00f3n de AWS CLI","text":""},{"location":"SBD/Tema01/AWSCLI/#instalacion-en-windows","title":"Instalaci\u00f3n en Windows","text":"<p>Puedes instalar AWS CLI en Windows siguiendo estos pasos:</p> <ol> <li>Descarga el instalador de AWS CLI para Windows desde el sitio web oficial de AWS.</li> <li>Ejecuta el instalador descargado y sigue las instrucciones de instalaci\u00f3n.</li> </ol>"},{"location":"SBD/Tema01/AWSCLI/#instalacion-en-linux-y-macos","title":"Instalaci\u00f3n en Linux y macOS","text":"<p>En sistemas Linux y macOS, puedes instalar AWS CLI utilizando el administrador de paquetes <code>pip</code> de Python o a trav\u00e9s de la instalaci\u00f3n de paquetes de tu sistema.</p>"},{"location":"SBD/Tema01/AWSCLI/#instalacion-con-pip","title":"Instalaci\u00f3n con <code>pip</code>","text":"<p>Ejecuta el siguiente comando en tu terminal:</p> <pre><code>pip install awscli\n</code></pre>"},{"location":"SBD/Tema01/AWSCLI/#instalacion-en-macos-con-homebrew","title":"Instalaci\u00f3n en macOS con Homebrew","text":"<p>Si utilizas Homebrew en macOS, puedes instalar AWS CLI con el siguiente comando:</p> <pre><code>brew install awscli\n</code></pre>"},{"location":"SBD/Tema01/AWSCLI/#configuracion-de-aws-cli","title":"Configuraci\u00f3n de AWS CLI","text":"<p>Despu\u00e9s de instalar AWS CLI, debes configurar las credenciales de acceso para que puedas autenticarte en AWS. Puedes hacerlo utilizando el comando <code>aws configure</code> y proporcionando tus credenciales de AWS, que incluyen la clave de acceso y la clave secreta.</p> <pre><code>aws configure\n</code></pre> <p>Tambi\u00e9n puedes configurar la regi\u00f3n predeterminada y el formato de salida preferido (por ejemplo, JSON) durante este proceso.</p>"},{"location":"SBD/Tema01/AWSCLI/#uso-basico-de-aws-cli","title":"Uso B\u00e1sico de AWS CLI","text":"<p>AWS CLI permite ejecutar comandos para interactuar con servicios de AWS. Aqu\u00ed tienes algunos ejemplos de comandos b\u00e1sicos:</p>"},{"location":"SBD/Tema01/AWSCLI/#listar-instancias-ec2","title":"Listar Instancias EC2","text":"<pre><code>aws ec2 describe-instances\n</code></pre>"},{"location":"SBD/Tema01/AWSCLI/#crear-un-bucket-de-amazon-s3","title":"Crear un Bucket de Amazon S3","text":"<pre><code>aws s3 mb s3://mi-bucket\n</code></pre>"},{"location":"SBD/Tema01/AWSCLI/#subir-un-archivo-a-amazon-s3","title":"Subir un Archivo a Amazon S3","text":"<pre><code>aws s3 cp mi-archivo.txt s3://mi-bucket/\n</code></pre>"},{"location":"SBD/Tema01/AWSCLI/#crear-una-instancia-ec2","title":"Crear una Instancia EC2","text":"<pre><code>aws ec2 run-instances --image-id ami-XXXXXXXXXXXXXXXXX --instance-type t2.micro --key-name mi-keypair\n</code></pre>"},{"location":"SBD/Tema01/AWSCLI/#scripts-y-automatizacion","title":"Scripts y Automatizaci\u00f3n","text":"<p>AWS CLI es una herramienta poderosa para la automatizaci\u00f3n de tareas en AWS. Puedes escribir scripts y secuencias de comandos para realizar operaciones complejas y administrar recursos de manera eficiente.</p>"},{"location":"SBD/Tema01/AWSCLI/#ayuda-y-documentacion","title":"Ayuda y Documentaci\u00f3n","text":"<p>AWS CLI proporciona documentaci\u00f3n detallada y ayuda en l\u00ednea para cada comando y servicio. Puedes utilizar el comando <code>aws help</code> para obtener informaci\u00f3n sobre el uso de comandos espec\u00edficos y acceder a la documentaci\u00f3n oficial de AWS CLI en l\u00ednea.</p>"},{"location":"SBD/Tema01/AWSCLI/#conclusion","title":"Conclusion","text":"<p>AWS CLI es una herramienta esencial para interactuar y administrar recursos en la nube de AWS de manera eficiente. Con la capacidad de automatizar tareas y realizar operaciones avanzadas desde la l\u00ednea de comandos, AWS CLI es una herramienta valiosa para desarrolladores y administradores de sistemas que trabajan con AWS.</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/","title":"Documentaci\u00f3n: Amazon Simple Storage Service (Amazon S3)","text":""},{"location":"SBD/Tema01/AlmacenamientoAWS/#introduccion-a-amazon-s3","title":"Introducci\u00f3n a Amazon S3","text":"<p>Amazon Simple Storage Service (Amazon S3) es un servicio de almacenamiento en la nube escalable y duradero que permite almacenar y recuperar datos en cualquier momento desde cualquier ubicaci\u00f3n en la web. Amazon S3 es ampliamente utilizado para el almacenamiento de objetos, copias de seguridad, distribuci\u00f3n de contenido y como componente central de muchas aplicaciones en la nube.</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#conceptos-clave-de-amazon-s3","title":"Conceptos Clave de Amazon S3","text":""},{"location":"SBD/Tema01/AlmacenamientoAWS/#1-buckets","title":"1. Buckets","text":"<ul> <li>Buckets: Un bucket de Amazon S3 es un contenedor para almacenar objetos. Los objetos se almacenan en buckets y deben tener un nombre \u00fanico en todo Amazon S3.</li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#2-objetos","title":"2. Objetos","text":"<ul> <li>Objetos: Los objetos son unidades de datos que se almacenan en buckets. Un objeto puede ser cualquier tipo de archivo, como documentos, im\u00e1genes, videos y m\u00e1s.</li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#3-regiones","title":"3. Regiones","text":"<ul> <li>Regiones: Amazon S3 est\u00e1 disponible en m\u00faltiples regiones de todo el mundo. Puedes seleccionar la regi\u00f3n que mejor se adapte a tus necesidades de rendimiento y cumplimiento.</li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#4-clases-de-almacenamiento","title":"4. Clases de Almacenamiento","text":"<ul> <li>Clases de Almacenamiento: Amazon S3 ofrece diversas clases de almacenamiento, como S3 Standard, S3 Intelligent-Tiering, S3 Glacier, S3 Glacier Deep Archive, entre otras, cada una dise\u00f1ada para diferentes casos de uso y costos.</li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#5-control-de-acceso","title":"5. Control de Acceso","text":"<ul> <li>Control de Acceso: Puedes definir pol\u00edticas de control de acceso para determinar qui\u00e9n puede acceder y qu\u00e9 acciones pueden realizar en tus buckets y objetos.</li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#configuracion-y-uso-basico-de-amazon-s3","title":"Configuraci\u00f3n y Uso B\u00e1sico de Amazon S3","text":""},{"location":"SBD/Tema01/AlmacenamientoAWS/#1-creacion-de-un-bucket","title":"1. Creaci\u00f3n de un Bucket","text":"<ul> <li>Desde la Consola de AWS, puedes crear un nuevo bucket y seleccionar su regi\u00f3n.</li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#2-carga-de-objetos","title":"2. Carga de Objetos","text":"<ul> <li>Puedes cargar objetos en tu bucket utilizando la Consola de AWS, la AWS CLI o SDKs.</li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#3-acceso-a-objetos","title":"3. Acceso a Objetos","text":"<ul> <li>Puedes configurar permisos de acceso a objetos para definir qui\u00e9n puede ver y descargar los objetos.</li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#4-versiones-de-objetos","title":"4. Versiones de Objetos","text":"<ul> <li>Amazon S3 admite versiones de objetos, lo que permite mantener un historial de cambios en los objetos.</li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#5-almacenamiento-de-datos","title":"5. Almacenamiento de Datos","text":"<ul> <li>Utiliza las clases de almacenamiento adecuadas seg\u00fan tus requisitos de rendimiento y costo.</li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#casos-de-uso-comunes-de-amazon-s3","title":"Casos de Uso Comunes de Amazon S3","text":""},{"location":"SBD/Tema01/AlmacenamientoAWS/#1-almacenamiento-de-copias-de-seguridad","title":"1. Almacenamiento de Copias de Seguridad","text":"<ul> <li>Utiliza Amazon S3 para almacenar copias de seguridad de tus datos y sistemas cr\u00edticos.</li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#2-distribucion-de-contenido-estatico","title":"2. Distribuci\u00f3n de Contenido Est\u00e1tico","text":"<ul> <li>Almacena y distribuye contenido web est\u00e1tico, como im\u00e1genes y archivos CSS, para mejorar la velocidad de carga de tu sitio web.</li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#3-almacenamiento-de-datos-para-aplicaciones","title":"3. Almacenamiento de Datos para Aplicaciones","text":"<ul> <li>Almacena datos de aplicaciones y archivos generados por usuarios en Amazon S3.</li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#4-almacenamiento-de-datos-para-analisis","title":"4. Almacenamiento de Datos para An\u00e1lisis","text":"<ul> <li>Almacena datos para an\u00e1lisis y procesamiento posterior utilizando servicios de AWS como Amazon Redshift y Amazon Athena.</li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#recomendaciones-de-seguridad","title":"Recomendaciones de Seguridad","text":"<ul> <li>Configuraci\u00f3n de Control de Acceso: Limita el acceso a tus buckets y objetos utilizando pol\u00edticas y ACLs.</li> <li>Cifrado de Datos: Habilita el cifrado de datos en reposo y en tr\u00e1nsito para proteger la confidencialidad de tus datos.</li> <li>Versionado de Objetos: Utiliza la funci\u00f3n de versionado de objetos para mantener un historial de cambios.</li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#ayuda-y-documentacion","title":"Ayuda y Documentaci\u00f3n","text":"<p>Amazon S3 ofrece documentaci\u00f3n completa en la p\u00e1gina oficial de AWS. Tambi\u00e9n puedes utilizar la AWS CLI y SDKs para interactuar program\u00e1ticamente con Amazon S3.</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#conclusion","title":"Conclusion","text":"<p>Amazon Simple Storage Service (Amazon S3) es un servicio de almacenamiento vers\u00e1til y altamente escalable que se utiliza en una amplia variedad de aplicaciones y casos de uso. Con la capacidad de almacenar, proteger y distribuir datos de manera confiable, Amazon S3 es un componente esencial de la infraestructura en la nube de AWS. Explora Amazon S3 para almacenar y administrar tus datos en la nube de forma eficiente y segura.</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#documentacion-ampliada-amazon-simple-storage-service-amazon-s3","title":"Documentaci\u00f3n Ampliada: Amazon Simple Storage Service (Amazon S3)","text":""},{"location":"SBD/Tema01/AlmacenamientoAWS/#amazon-s3-clases-de-almacenamiento","title":"Amazon S3: Clases de Almacenamiento","text":"<p>Amazon S3 ofrece una variedad de clases de almacenamiento para satisfacer diferentes requisitos de rendimiento, costo y retenci\u00f3n de datos. Cada clase de almacenamiento est\u00e1 dise\u00f1ada para casos de uso espec\u00edficos. A continuaci\u00f3n, se explican las principales clases de almacenamiento de Amazon S3:</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#1-s3-standard","title":"1. S3 Standard","text":"<ul> <li> <p>Descripci\u00f3n: Esta es la clase de almacenamiento predeterminada de Amazon S3 y ofrece un alto rendimiento y durabilidad. Los datos almacenados en S3 Standard est\u00e1n disponibles de inmediato y se replican autom\u00e1ticamente en m\u00faltiples zonas de disponibilidad.</p> </li> <li> <p>Casos de Uso: Almacenamiento de datos cr\u00edticos para aplicaciones en l\u00ednea, distribuci\u00f3n de contenido web din\u00e1mico, copias de seguridad activas.</p> </li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#2-s3-intelligent-tiering","title":"2. S3 Intelligent-Tiering","text":"<ul> <li> <p>Descripci\u00f3n: S3 Intelligent-Tiering es una clase de almacenamiento que optimiza autom\u00e1ticamente los costos. Los objetos se mueven autom\u00e1ticamente entre las capas de acceso frecuente y de acceso a largo plazo seg\u00fan su patr\u00f3n de acceso.</p> </li> <li> <p>Casos de Uso: Datos con patrones de acceso impredecibles, donde se desea una combinaci\u00f3n de rendimiento y ahorro de costos.</p> </li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#3-s3-standard-ia-infrequent-access","title":"3. S3 Standard-IA (Infrequent Access)","text":"<ul> <li> <p>Descripci\u00f3n: Esta clase de almacenamiento est\u00e1 dise\u00f1ada para datos a los que se accede con menos frecuencia. Ofrece un costo menor que S3 Standard a cambio de una tarifa de recuperaci\u00f3n de datos si se accede antes de 30 d\u00edas.</p> </li> <li> <p>Casos de Uso: Almacenamiento de copias de seguridad a largo plazo, archivos de registro, datos de archivo inactivos.</p> </li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#4-s3-one-zone-ia","title":"4. S3 One Zone-IA","text":"<ul> <li> <p>Descripci\u00f3n: Similar a S3 Standard-IA, pero los datos se almacenan en una sola zona de disponibilidad en lugar de replicarse en m\u00faltiples zonas. Esto reduce costos, pero los datos no est\u00e1n tan disponibles como en otras clases.</p> </li> <li> <p>Casos de Uso: Datos que pueden recrearse si se pierden, como copias de seguridad de bajo costo.</p> </li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#5-s3-glacier","title":"5. S3 Glacier","text":"<ul> <li> <p>Descripci\u00f3n: S3 Glacier es una clase de almacenamiento de archivo que ofrece un costo extremadamente bajo a cambio de tiempos de acceso m\u00e1s lentos. Los datos se almacenan en formato de archivo y deben recuperarse antes de acceder a ellos.</p> </li> <li> <p>Casos de Uso: Archivado a largo plazo, cumplimiento normativo, datos de archivo que no se necesitan con frecuencia.</p> </li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#6-s3-glacier-deep-archive","title":"6. S3 Glacier Deep Archive","text":"<ul> <li> <p>Descripci\u00f3n: Esta es la opci\u00f3n m\u00e1s econ\u00f3mica de Amazon S3, pero tambi\u00e9n la m\u00e1s lenta en t\u00e9rminos de acceso. Se utiliza para datos de archivo que se mantienen a largo plazo y rara vez se necesitan.</p> </li> <li> <p>Casos de Uso: Cumplimiento normativo a largo plazo, retenci\u00f3n de registros hist\u00f3ricos, datos de archivo de bajo costo.</p> </li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#7-s3-outposts","title":"7. S3 Outposts","text":"<ul> <li> <p>Descripci\u00f3n: S3 Outposts permite almacenar datos en ubicaciones locales en los data centers de AWS Outposts. Los datos almacenados en S3 Outposts est\u00e1n sincronizados con la regi\u00f3n de AWS m\u00e1s cercana.</p> </li> <li> <p>Casos de Uso: Escenarios donde es necesario almacenar datos en ubicaciones locales en conjunto con infraestructura local.</p> </li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#8-s3-object-lock","title":"8. S3 Object Lock","text":"<ul> <li> <p>Descripci\u00f3n: Esta caracter\u00edstica permite configurar pol\u00edticas de bloqueo de objetos para evitar la eliminaci\u00f3n o modificaci\u00f3n de objetos durante un per\u00edodo de retenci\u00f3n especificado.</p> </li> <li> <p>Casos de Uso: Cumplimiento normativo, protecci\u00f3n de datos cr\u00edticos contra borrado accidental.</p> </li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#seleccion-de-la-clase-de-almacenamiento","title":"Selecci\u00f3n de la Clase de Almacenamiento","text":"<p>La elecci\u00f3n de la clase de almacenamiento adecuada en Amazon S3 depende de los requisitos espec\u00edficos de tu aplicaci\u00f3n y de tu presupuesto. Aqu\u00ed hay algunas pautas generales para ayudarte a seleccionar la clase correcta:</p> <ul> <li>Si necesitas un alto rendimiento y disponibilidad inmediata, opta por S3 Standard.</li> <li>Si tienes datos con patrones de acceso impredecibles y deseas ahorrar costos, considera S3 Intelligent-Tiering.</li> <li>Para datos a los que se accede con menos frecuencia, S3 Standard-IA es una buena opci\u00f3n.</li> <li>Si puedes recrear datos perdidos y deseas un bajo costo, elige S3 One Zone-IA.</li> <li>Para archivar datos a largo plazo con costo m\u00ednimo, utiliza S3 Glacier o S3 Glacier Deep Archive.</li> <li>Si necesitas almacenar datos en ubicaciones locales con AWS Outposts, considera S3 Outposts.</li> </ul> <p>Recuerda que puedes cambiar la clase de almacenamiento de tus objetos en cualquier momento seg\u00fan cambien tus necesidades</p> <p>.</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#conclusion_1","title":"Conclusi\u00f3n","text":"<p>Amazon S3 ofrece una amplia gama de opciones de clase de almacenamiento para adaptarse a diversas necesidades. Al comprender las caracter\u00edsticas y los casos de uso de cada clase de almacenamiento, puedes tomar decisiones informadas sobre c\u00f3mo almacenar y gestionar tus datos en Amazon S3 de manera eficiente y rentable. Experimenta con las diferentes clases de almacenamiento para optimizar tus costos y satisfacer los requisitos de tu aplicaci\u00f3n.</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#documentacion-ampliada-amazon-simple-storage-service-amazon-s3_1","title":"Documentaci\u00f3n Ampliada: Amazon Simple Storage Service (Amazon S3)","text":""},{"location":"SBD/Tema01/AlmacenamientoAWS/#introduccion-a-amazon-s3_1","title":"Introducci\u00f3n a Amazon S3","text":"<p>Amazon Simple Storage Service (Amazon S3) es un servicio de almacenamiento en la nube escalable y duradero que permite almacenar y recuperar datos en cualquier momento desde cualquier ubicaci\u00f3n en la web. Amazon S3 es ampliamente utilizado para el almacenamiento de objetos, copias de seguridad, distribuci\u00f3n de contenido y como componente central de muchas aplicaciones en la nube.</p> <p>Documentaci\u00f3n Oficial de Amazon S3: Amazon S3 - Documentaci\u00f3n de AWS</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#conceptos-clave-de-amazon-s3_1","title":"Conceptos Clave de Amazon S3","text":""},{"location":"SBD/Tema01/AlmacenamientoAWS/#1-buckets_1","title":"1. Buckets","text":"<ul> <li>Buckets: Un bucket de Amazon S3 es un contenedor para almacenar objetos. Los objetos se almacenan en buckets y deben tener un nombre \u00fanico en todo Amazon S3.</li> </ul> <p>Documentaci\u00f3n Relacionada: Creaci\u00f3n de un Bucket - AWS</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#2-objetos_1","title":"2. Objetos","text":"<ul> <li>Objetos: Los objetos son unidades de datos que se almacenan en buckets. Un objeto puede ser cualquier tipo de archivo, como documentos, im\u00e1genes, videos y m\u00e1s.</li> </ul> <p>Documentaci\u00f3n Relacionada: Carga de Objetos en S3 - AWS</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#3-regiones_1","title":"3. Regiones","text":"<ul> <li>Regiones: Amazon S3 est\u00e1 disponible en m\u00faltiples regiones de todo el mundo. Puedes seleccionar la regi\u00f3n que mejor se adapte a tus necesidades de rendimiento y cumplimiento.</li> </ul> <p>Documentaci\u00f3n Relacionada: Seleccionar una Regi\u00f3n para tu Bucket - AWS</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#4-clases-de-almacenamiento_1","title":"4. Clases de Almacenamiento","text":"<ul> <li>Clases de Almacenamiento: Amazon S3 ofrece diversas clases de almacenamiento, como S3 Standard, S3 Intelligent-Tiering, S3 Glacier, S3 Glacier Deep Archive, entre otras, cada una dise\u00f1ada para diferentes casos de uso y costos.</li> </ul> <p>Documentaci\u00f3n Relacionada: Clases de Almacenamiento de S3 - AWS</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#5-control-de-acceso_1","title":"5. Control de Acceso","text":"<ul> <li>Control de Acceso: Puedes definir pol\u00edticas de control de acceso para determinar qui\u00e9n puede acceder y qu\u00e9 acciones pueden realizar en tus buckets y objetos.</li> </ul> <p>Documentaci\u00f3n Relacionada: Control de Acceso en S3 - AWS</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#configuracion-y-uso-basico-de-amazon-s3_1","title":"Configuraci\u00f3n y Uso B\u00e1sico de Amazon S3","text":""},{"location":"SBD/Tema01/AlmacenamientoAWS/#1-creacion-de-un-bucket_1","title":"1. Creaci\u00f3n de un Bucket","text":"<ul> <li>Desde la Consola de AWS, puedes crear un nuevo bucket y seleccionar su regi\u00f3n.</li> </ul> <p>Documentaci\u00f3n Relacionada: Creaci\u00f3n de un Bucket - AWS</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#2-carga-de-objetos_1","title":"2. Carga de Objetos","text":"<ul> <li>Puedes cargar objetos en tu bucket utilizando la Consola de AWS, la AWS CLI o SDKs.</li> </ul> <p>Documentaci\u00f3n Relacionada: Carga de Objetos en S3 - AWS</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#3-acceso-a-objetos_1","title":"3. Acceso a Objetos","text":"<ul> <li>Puedes configurar permisos de acceso a objetos para definir qui\u00e9n puede ver y descargar los objetos.</li> </ul> <p>Documentaci\u00f3n Relacionada: Configuraci\u00f3n de Permisos de Objetos - AWS</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#4-versiones-de-objetos_1","title":"4. Versiones de Objetos","text":"<ul> <li>Amazon S3 admite versiones de objetos, lo que permite mantener un historial de cambios en los objetos.</li> </ul> <p>Documentaci\u00f3n Relacionada: Versionado de Objetos en S3 - AWS</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#5-almacenamiento-de-datos_1","title":"5. Almacenamiento de Datos","text":"<ul> <li>Utiliza las clases de almacenamiento adecuadas seg\u00fan tus requisitos de rendimiento y costo.</li> </ul> <p>Documentaci\u00f3n Relacionada: Clases de Almacenamiento de S3 - AWS</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#casos-de-uso-comunes-de-amazon-s3_1","title":"Casos de Uso Comunes de Amazon S3","text":""},{"location":"SBD/Tema01/AlmacenamientoAWS/#1-almacenamiento-de-copias-de-seguridad_1","title":"1. Almacenamiento de Copias de Seguridad","text":"<ul> <li>Utiliza Amazon S3 para almacenar copias de seguridad de tus datos y sistemas cr\u00edticos.</li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#2-distribucion-de-contenido-estatico_1","title":"2. Distribuci\u00f3n de Contenido Est\u00e1tico","text":"<ul> <li>Almacena y distribuye contenido web est\u00e1tico, como im\u00e1genes y archivos CSS, para mejorar la velocidad de carga de tu sitio web.</li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#3-almacenamiento-de-datos-para-aplicaciones_1","title":"3. Almacenamiento de Datos para Aplicaciones","text":"<ul> <li>Almacena datos de aplicaciones y archivos generados por usuarios en Amazon S3.</li> </ul>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#4-almacenamiento-de-datos-para-analisis_1","title":"4. Almacenamiento de Datos para An\u00e1lisis","text":"<ul> <li>Almacena datos para an\u00e1lisis y procesamiento posterior utilizando servicios de AWS como Amazon Redshift y Amazon Athena.</li> </ul> <p>Documentaci\u00f3n Relacionada: Casos de Uso de Amazon S3 - AWS</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#recomendaciones-de-seguridad_1","title":"Recomendaciones de Seguridad","text":"<ul> <li>Configuraci\u00f3n de Control de Acceso: Limita el acceso a tus buckets y objetos utilizando pol\u00edticas y ACLs.</li> </ul> <p>Documentaci\u00f3n Relacionada: Control de Acceso en S3 - AWS</p> <ul> <li>Cifrado de Datos: Habilita el cifrado de datos en reposo y en tr\u00e1nsito para proteger la confidencialidad de tus datos.</li> </ul> <p>Documentaci\u00f3n Relacionada: Cifrado de Datos en S3 - AWS</p> <ul> <li>Versionado de Objetos: Utiliza la funci\u00f3n de versionado de objetos para mantener un historial de cambios.</li> </ul> <p>Documentaci\u00f3n Relacionada: Versionado de Objetos en S3 - AWS</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#ayuda-y-documentacion_1","title":"Ayuda y Documentaci\u00f3n","text":"<p>Amazon S3 ofrece documentaci\u00f3n completa en la p\u00e1gina oficial de AWS. Tambi\u00e9n puedes utilizar la AWS CLI y SDKs para interactuar program\u00e1ticamente con Amazon S3.</p>"},{"location":"SBD/Tema01/AlmacenamientoAWS/#conclusion_2","title":"Conclusi\u00f3n","text":"<p>Amazon Simple Storage Service (Amazon S3) es un servicio de almacenamiento vers\u00e1til y altamente escalable que se utiliza en una amplia variedad de aplicaciones y casos de uso. Al comprender las caracter\u00edsticas y los casos de uso de cada clase de almacenamiento, puedes tomar</p>"},{"location":"SBD/Tema01/BigDataConAWS/","title":"1. Big Data con AWS","text":""},{"location":"SBD/Tema01/BigDataConAWS/#11-introduccion-a-aws","title":"1.1. Introducci\u00f3n a AWS","text":"<p>Amazon Web Services (AWS) es una plataforma de servicios en la nube l\u00edder en el mundo que ofrece una amplia gama de servicios de infraestructura, almacenamiento, c\u00f3mputo, bases de datos, an\u00e1lisis y m\u00e1s. </p> <p>AWS proporciona la infraestructura necesaria para ejecutar aplicaciones, almacenar datos y procesar informaci\u00f3n a escala global. En el contexto de Big Data, AWS ofrece herramientas y servicios para administrar, procesar y analizar grandes vol\u00famenes de datos de manera eficiente y escalable.</p> <p>AWS ofrece una amplia gama de servicios y herramientas para facilitar proyectos de Big Data. Puedes aprovechar la escalabilidad, la variedad de servicios y las medidas de seguridad de AWS para administrar y analizar grandes vol\u00famenes de datos de manera efectiva.</p>"},{"location":"SBD/Tema01/BigDataConAWS/#12-por-que-aws-para-big-data","title":"1.2. Por Qu\u00e9 AWS para Big Data","text":""},{"location":"SBD/Tema01/BigDataConAWS/#121-escalabilidad","title":"1.2.1. Escalabilidad","text":"<p>AWS ofrece la capacidad de escalar recursos de manera flexible seg\u00fan las necesidades de tu proyecto de Big Data. Puedes aumentar o reducir la capacidad de c\u00f3mputo y almacenamiento seg\u00fan la demanda, lo que te permite manejar grandes vol\u00famenes de datos sin preocuparte por la infraestructura subyacente.</p>"},{"location":"SBD/Tema01/BigDataConAWS/#122-variedad-de-servicios","title":"1.2.2. Variedad de Servicios","text":"<p>AWS proporciona una amplia gama de servicios espec\u00edficamente dise\u00f1ados para el procesamiento y an\u00e1lisis de Big Data. Estos servicios incluyen Amazon EMR (Elastic MapReduce), Amazon Redshift, AWS Glue, Amazon Kinesis y m\u00e1s.</p>"},{"location":"SBD/Tema01/BigDataConAWS/#123-seguridad-y-cumplimiento","title":"1.2.3. Seguridad y Cumplimiento","text":"<p>AWS implementa medidas de seguridad y cumplimiento rigurosas para proteger tus datos. Puedes administrar el acceso a tus recursos y datos, cifrar la informaci\u00f3n en tr\u00e1nsito y en reposo, y cumplir con regulaciones espec\u00edficas de la industria.</p>"},{"location":"SBD/Tema01/BigDataConAWS/#13-principales-servicios-de-aws-para-big-data","title":"1.3. Principales Servicios de AWS para Big Data","text":""},{"location":"SBD/Tema01/BigDataConAWS/#131-amazon-s3-amazon-simple-storage-service","title":"1.3.1. Amazon S3 (Amazon Simple Storage Service)","text":"<p>Amazon S3 es un servicio de almacenamiento en la nube altamente escalable, seguro y confiable que permite a las empresas almacenar, gestionar y acceder a datos de manera eficiente en la plataforma de AWS.</p>"},{"location":"SBD/Tema01/BigDataConAWS/#132-amazon-emr-elastic-mapreduce","title":"1.3.2. Amazon EMR (Elastic MapReduce)","text":"<p>Amazon EMR es un servicio de administraci\u00f3n de cl\u00fasteres de c\u00f3digo abierto que te permite procesar grandes vol\u00famenes de datos de manera distribuida utilizando frameworks como Apache Hadoop, Apache Spark y Apache Hive. Puedes crear y administrar cl\u00fasteres de EMR de manera sencilla y escalarlos seg\u00fan sea necesario.</p>"},{"location":"SBD/Tema01/BigDataConAWS/#133-amazon-redshift","title":"1.3.3. Amazon Redshift","text":"<p>Amazon Redshift es un servicio de data warehousing completamente administrado que te permite analizar grandes conjuntos de datos de manera eficiente. Es ideal para almacenar y consultar datos anal\u00edticos, y admite integraci\u00f3n con herramientas de visualizaci\u00f3n populares.</p>"},{"location":"SBD/Tema01/BigDataConAWS/#134-aws-glue","title":"1.3.4. AWS Glue","text":"<p>AWS Glue es un servicio de ETL (Extract, Transform, Load) completamente administrado que facilita la preparaci\u00f3n y transformaci\u00f3n de datos para an\u00e1lisis. Puedes definir trabajos de ETL utilizando un entorno visual o escribir scripts en lenguaje Python.</p>"},{"location":"SBD/Tema01/BigDataConAWS/#135-amazon-kinesis","title":"1.3.5. Amazon Kinesis","text":"<p>Amazon Kinesis es una plataforma para el streaming de datos en tiempo real. Te permite capturar, procesar y analizar flujos de datos en tiempo real, lo que es fundamental para aplicaciones de an\u00e1lisis en tiempo real y procesamiento de eventos.</p>"},{"location":"SBD/Tema01/BigDataConAWS/#14-ejemplo-de-uso-procesamiento-de-logs-con-aws","title":"1.4. Ejemplo de Uso: Procesamiento de Logs con AWS","text":"<p>Supongamos que deseas analizar registros (logs) de aplicaciones web para obtener informaci\u00f3n valiosa. Puedes utilizar AWS para este escenario:</p> <ol> <li> <p>Almacenamiento de Logs: Utiliza Amazon S3 para almacenar los registros de aplicaciones web de manera escalable y duradera.</p> </li> <li> <p>Procesamiento con Amazon EMR: Crea un cl\u00faster de Amazon EMR para procesar los registros utilizando Apache Spark. Puedes aplicar transformaciones y consultas para extraer informaci\u00f3n relevante.</p> </li> <li> <p>Almacenamiento de Resultados: Guarda los resultados del procesamiento en Amazon Redshift para consultas posteriores y an\u00e1lisis enriquecidos.</p> </li> <li> <p>Visualizaci\u00f3n con Amazon QuickSight: Utiliza Amazon QuickSight para crear paneles de control y visualizaciones interactivas que muestren informaci\u00f3n clave extra\u00edda de los logs.</p> </li> </ol>"},{"location":"SBD/Tema01/BigDataConAWS/#15-aws-academy","title":"1.5 AWS Academy","text":"<p>AWS Academy es un programa global de formaci\u00f3n t\u00e9cnica respaldado por Amazon Web Services. Ofrece a estudiantes y educadores la capacitaci\u00f3n y los recursos necesarios para desarrollar habilidades en la nube. </p> <p>La misi\u00f3n de AWS Academy es proporcionar educaci\u00f3n de alta calidad y actualizada en AWS, ayudando a los estudiantes a prepararse para carreras en la nube.</p>"},{"location":"SBD/Tema01/BigDataConAWS/#151-beneficios","title":"1.5.1 Beneficios","text":"<ul> <li>Los estudiantes tienen la oportunidad de adquirir habilidades en la nube altamente demandadas por empleadores en una variedad de industrias. Ejemplo: Un estudio de empleo reciente mostr\u00f3 que el 80% de las empresas buscan profesionales de AWS.</li> <li>AWS Academy se enfoca en la pr\u00e1ctica y proyectos reales, lo que permite a los estudiantes aplicar sus conocimientos en situaciones del mundo real.</li> </ul>"},{"location":"SBD/Tema01/BigDataConAWS/#152-cursos-ofrecidos","title":"1.5.2. Cursos Ofrecidos","text":"<ul> <li>AWS Academy ofrece una amplia gama de cursos que cubren una variedad de temas, desde fundamentos de la nube hasta cursos especializados en \u00e1reas c\u00f3mo desarrollo de aplicaciones y an\u00e1lisis de datos. Ejemplo: \"Fundamentos de AWS Cloud\" es un curso de nivel inicial que introduce los conceptos b\u00e1sicos de la nube.</li> <li>Muchos de los cursos de AWS Academy est\u00e1n dise\u00f1ados para preparar a los estudiantes para las certificaciones de AWS, lo que les brinda una ventaja competitiva en el mercado laboral.</li> </ul>"},{"location":"SBD/Tema01/BigDataConAWS/#153-learner-labs","title":"1.5.3. Learner Labs","text":"<ul> <li>Los Learner Labs son espacios en l\u00ednea donde los estudiantes pueden realizar pr\u00e1cticas relacionadas con AWS. Estos entornos ya est\u00e1n preconfigurados con recursos de AWS, c\u00f3mo servidores virtuales y bases de datos, lo que permite a los estudiantes centrarse en aprender sin preocuparse por la infraestructura subyacente.</li> </ul>"},{"location":"SBD/Tema01/BigDataConAWS/#154-certificaciones","title":"1.5.4. Certificaciones","text":"<p>Las certificaciones de AWS son reconocidas globalmente y validan las habilidades t\u00e9cnicas en la nube. Ejemplo: La certificaci\u00f3n \"AWS Certified Solutions Architect\" demuestra la capacidad de dise\u00f1ar sistemas escalables en AWS.</p>"},{"location":"SBD/Tema01/DataLakeAWS/","title":"Documentaci\u00f3n: Creaci\u00f3n y Gesti\u00f3n de un Data Lake en AWS","text":""},{"location":"SBD/Tema01/DataLakeAWS/#introduccion-a-data-lake-en-aws","title":"Introducci\u00f3n a Data Lake en AWS","text":"<p>Un data lake es un repositorio centralizado que permite almacenar grandes vol\u00famenes de datos en su formato original. AWS ofrece una serie de servicios que te permiten crear y gestionar un data lake escalable, seguro y altamente disponible.</p> <p>Documentaci\u00f3n Oficial de AWS sobre Data Lake: AWS Data Lake - Documentaci\u00f3n de AWS</p>"},{"location":"SBD/Tema01/DataLakeAWS/#componentes-clave-de-un-data-lake-en-aws","title":"Componentes Clave de un Data Lake en AWS","text":""},{"location":"SBD/Tema01/DataLakeAWS/#1-amazon-s3","title":"1. Amazon S3","text":"<ul> <li>Amazon S3: Utiliza Amazon Simple Storage Service (Amazon S3) como el almacenamiento principal para tu data lake. Amazon S3 es altamente escalable, duradero y ofrece capacidades de cifrado para garantizar la seguridad de tus datos.</li> </ul> <p>Documentaci\u00f3n Relacionada: Amazon S3 - Documentaci\u00f3n de AWS</p>"},{"location":"SBD/Tema01/DataLakeAWS/#2-aws-glue","title":"2. AWS Glue","text":"<ul> <li>AWS Glue: Utiliza AWS Glue para catalogar, limpiar y transformar datos antes de cargarlos en el data lake. AWS Glue es un servicio de ETL totalmente administrado.</li> </ul> <p>Documentaci\u00f3n Relacionada: AWS Glue - Documentaci\u00f3n de AWS</p>"},{"location":"SBD/Tema01/DataLakeAWS/#3-aws-lake-formation","title":"3. AWS Lake Formation","text":"<ul> <li>AWS Lake Formation: AWS Lake Formation te ayuda a configurar y gestionar un data lake de manera m\u00e1s sencilla, incluida la configuraci\u00f3n de permisos y la gesti\u00f3n de metadatos.</li> </ul> <p>Documentaci\u00f3n Relacionada: AWS Lake Formation - Documentaci\u00f3n de AWS</p>"},{"location":"SBD/Tema01/DataLakeAWS/#4-herramientas-de-analisis","title":"4. Herramientas de An\u00e1lisis","text":"<ul> <li>Herramientas de An\u00e1lisis: Utiliza herramientas de an\u00e1lisis como Amazon Athena, Amazon Redshift, AWS EMR y AWS Quicksight para consultar, analizar y visualizar datos en tu data lake.</li> </ul> <p>Documentaci\u00f3n Relacionada: Herramientas de An\u00e1lisis en AWS - Documentaci\u00f3n de AWS</p>"},{"location":"SBD/Tema01/DataLakeAWS/#configuracion-y-uso-basico-de-un-data-lake-en-aws","title":"Configuraci\u00f3n y Uso B\u00e1sico de un Data Lake en AWS","text":""},{"location":"SBD/Tema01/DataLakeAWS/#1-creacion-de-un-data-lake","title":"1. Creaci\u00f3n de un Data Lake","text":"<ul> <li>Configura un bucket de Amazon S3 como el almacenamiento central de tu data lake.</li> </ul> <p>Documentaci\u00f3n Relacionada: Creaci\u00f3n de un Data Lake en Amazon S3 - AWS</p>"},{"location":"SBD/Tema01/DataLakeAWS/#2-catalogacion-de-datos","title":"2. Catalogaci\u00f3n de Datos","text":"<ul> <li>Utiliza AWS Glue para catalogar datos en tu data lake, lo que facilita la b\u00fasqueda y el acceso a los datos.</li> </ul> <p>Documentaci\u00f3n Relacionada: Catalogaci\u00f3n de Datos con AWS Glue - AWS</p>"},{"location":"SBD/Tema01/DataLakeAWS/#3-gobernanza-y-seguridad","title":"3. Gobernanza y Seguridad","text":"<ul> <li>Configura pol\u00edticas de control de acceso, cifrado de datos y auditor\u00eda para garantizar la seguridad y la conformidad de tus datos en el data lake.</li> </ul> <p>Documentaci\u00f3n Relacionada: Gobernanza y Seguridad en Data Lake - AWS</p>"},{"location":"SBD/Tema01/DataLakeAWS/#4-procesamiento-de-datos","title":"4. Procesamiento de Datos","text":"<ul> <li>Utiliza servicios de procesamiento de datos como AWS EMR o AWS Lambda para realizar transformaciones y an\u00e1lisis en tus datos del data lake.</li> </ul> <p>Documentaci\u00f3n Relacionada: Procesamiento de Datos en AWS - Documentaci\u00f3n de AWS</p>"},{"location":"SBD/Tema01/DataLakeAWS/#casos-de-uso-comunes-de-un-data-lake-en-aws","title":"Casos de Uso Comunes de un Data Lake en AWS","text":""},{"location":"SBD/Tema01/DataLakeAWS/#1-analisis-de-datos","title":"1. An\u00e1lisis de Datos","text":"<ul> <li>Almacena y analiza datos de m\u00faltiples fuentes para obtener informaci\u00f3n valiosa sobre tu negocio.</li> </ul>"},{"location":"SBD/Tema01/DataLakeAWS/#2-data-warehousing","title":"2. Data Warehousing","text":"<ul> <li>Utiliza un data lake junto con servicios como Amazon Redshift para implementar soluciones de data warehousing.</li> </ul>"},{"location":"SBD/Tema01/DataLakeAWS/#3-ciencia-de-datos","title":"3. Ciencia de Datos","text":"<ul> <li>Facilita proyectos de ciencia de datos al proporcionar un repositorio centralizado para datos brutos y procesados.</li> </ul> <p>Documentaci\u00f3n Relacionada: Casos de Uso de Data Lake - AWS</p>"},{"location":"SBD/Tema01/DataLakeAWS/#recomendaciones-de-mejores-practicas","title":"Recomendaciones de Mejores Pr\u00e1cticas","text":"<ul> <li> <p>Planificaci\u00f3n de Datos: Dise\u00f1a una estrategia de planificaci\u00f3n de datos que defina c\u00f3mo se organizar\u00e1n y catalogar\u00e1n los datos en el data lake.</p> </li> <li> <p>Automatizaci\u00f3n de Procesos: Utiliza servicios de automatizaci\u00f3n como AWS Step Functions para orquestar flujos de trabajo de procesamiento de datos.</p> </li> </ul> <p>Documentaci\u00f3n Relacionada: Orquestaci\u00f3n de Flujos de Trabajo con AWS Step Functions - AWS</p> <ul> <li>Monitorizaci\u00f3n y Auditor\u00eda: Implementa una soluci\u00f3n de monitorizaci\u00f3n y auditor\u00eda para supervisar el uso y el rendimiento de tu data lake.</li> </ul> <p>Documentaci\u00f3n Relacionada: Monitorizaci\u00f3n en AWS - AWS</p>"},{"location":"SBD/Tema01/DataLakeAWS/#ayuda-y-documentacion-adicional","title":"Ayuda y Documentaci\u00f3n Adicional","text":"<p>Para obtener m\u00e1s detalles sobre la creaci\u00f3n y gesti\u00f3n de un data lake en AWS, te recomiendo consultar la documentaci\u00f3n oficial de AWS sobre Data Lake. Adem\u00e1s, AWS ofrece tutoriales detallados y ejemplos de implementaci\u00f3n para ayudarte en tu proyecto de data lake.</p>"},{"location":"SBD/Tema01/DataLakeAWS/#conclusion","title":"Conclusi\u00f3n","text":"<p>La creaci\u00f3n y gesti\u00f3n de un data lake en AWS es fundamental para aprovechar al m\u00e1ximo tus datos y habilitar an\u00e1lisis avanzados. Siguiendo las mejores pr\u00e1cticas de seguridad y gobernanza, puedes construir un data lake robusto y escalable que satisfaga las necesidades de tu organizaci\u00f3n.</p> <p>Puedes crear y gestionar un data lake en AWS sin utilizar AWS Lake Formation siguiendo una serie de pasos y utilizando los servicios principales de AWS. A continuaci\u00f3n, te proporciono una gu\u00eda paso a paso sobre c\u00f3mo hacerlo:</p>"},{"location":"SBD/Tema01/DataLakeAWS/#creacion-y-gestion-de-un-data-lake-en-aws-sin-aws-lake-formation","title":"Creaci\u00f3n y Gesti\u00f3n de un Data Lake en AWS sin AWS Lake Formation","text":""},{"location":"SBD/Tema01/DataLakeAWS/#paso-1-configurar-amazon-s3","title":"Paso 1: Configurar Amazon S3","text":"<ol> <li> <p>Crear un Bucket de Amazon S3: Inicia sesi\u00f3n en la consola de AWS y crea un bucket de S3 que servir\u00e1 como el almacenamiento central para tu data lake. Organiza tus datos en carpetas dentro del bucket seg\u00fan las categor\u00edas o fuentes de datos.</p> </li> <li> <p>Configurar Pol\u00edticas de Acceso: Configura las pol\u00edticas de acceso y permisos en tu bucket de S3 para garantizar que solo los usuarios autorizados puedan acceder a los datos. Utiliza las pol\u00edticas de S3 y IAM para administrar el acceso.</p> </li> <li> <p>Habilitar Cifrado: Habilita el cifrado de datos en reposo y en tr\u00e1nsito en tu bucket de S3 para garantizar la seguridad de los datos almacenados.</p> </li> </ol>"},{"location":"SBD/Tema01/DataLakeAWS/#paso-2-catalogar-datos-con-aws-glue","title":"Paso 2: Catalogar Datos con AWS Glue","text":"<ol> <li> <p>Utilizar AWS Glue Data Catalog: Utiliza AWS Glue Data Catalog para catalogar y gestionar los metadatos de tus datos en el data lake. AWS Glue permite crear tablas y definir esquemas para tus datos.</p> </li> <li> <p>Ejecutar Crawler: Configura y ejecuta crawlers de AWS Glue para descubrir y catalogar autom\u00e1ticamente los datos en tu bucket de S3. Esto facilita la b\u00fasqueda y el acceso a los datos.</p> </li> </ol>"},{"location":"SBD/Tema01/DataLakeAWS/#paso-3-procesamiento-y-transformacion-de-datos","title":"Paso 3: Procesamiento y Transformaci\u00f3n de Datos","text":"<ol> <li>Utilizar Servicios de Procesamiento: Utiliza servicios de procesamiento como AWS Lambda, AWS Glue ETL o AWS Step Functions para realizar transformaciones y limpieza de datos seg\u00fan sea necesario antes de cargarlos en el data lake.</li> </ol>"},{"location":"SBD/Tema01/DataLakeAWS/#paso-4-herramientas-de-analisis","title":"Paso 4: Herramientas de An\u00e1lisis","text":"<ol> <li>Configurar Herramientas de An\u00e1lisis: Configura herramientas de an\u00e1lisis como Amazon Athena, Amazon Redshift o AWS QuickSight para consultar, analizar y visualizar datos en tu data lake.</li> </ol>"},{"location":"SBD/Tema01/DataLakeAWS/#paso-5-gobernanza-y-seguridad","title":"Paso 5: Gobernanza y Seguridad","text":"<ol> <li> <p>Implementar Gobernanza: Define pol\u00edticas y procedimientos de gobernanza de datos para garantizar la calidad y la conformidad de los datos en el data lake.</p> </li> <li> <p>Auditor\u00eda y Monitoreo: Implementa una soluci\u00f3n de auditor\u00eda y monitoreo para supervisar el acceso y el rendimiento de tu data lake. Puedes utilizar AWS CloudTrail y Amazon CloudWatch para esto.</p> </li> </ol>"},{"location":"SBD/Tema01/DataLakeAWS/#paso-6-automatizacion-de-procesos","title":"Paso 6: Automatizaci\u00f3n de Procesos","text":"<ol> <li>Automatizaci\u00f3n de Flujos de Trabajo: Utiliza servicios como AWS Step Functions para orquestar flujos de trabajo de procesamiento y an\u00e1lisis de datos de manera automatizada.</li> </ol>"},{"location":"SBD/Tema01/DataLakeAWS/#paso-7-casos-de-uso-comunes","title":"Paso 7: Casos de Uso Comunes","text":"<ol> <li>Implementar Casos de Uso: Utiliza tu data lake para una variedad de casos de uso, como an\u00e1lisis de datos, ciencia de datos, an\u00e1lisis de registros y m\u00e1s, aprovechando las herramientas y servicios adecuados.</li> </ol>"},{"location":"SBD/Tema01/DataLakeAWS/#recomendaciones-de-mejores-practicas_1","title":"Recomendaciones de Mejores Pr\u00e1cticas","text":"<ul> <li> <p>Planifica y dise\u00f1a cuidadosamente la estructura de tus datos en el data lake para facilitar la b\u00fasqueda y el acceso.</p> </li> <li> <p>Implementa pol\u00edticas de retenci\u00f3n de datos para administrar el ciclo de vida de los datos en el data lake.</p> </li> <li> <p>Capacita a los usuarios y equipos de TI en el uso de las herramientas y servicios de AWS para el data lake.</p> </li> <li> <p>Establece una estrategia de escalabilidad para que el data lake pueda crecer con la cantidad de datos y el uso.</p> </li> </ul>"},{"location":"SBD/Tema01/DataLakeAWS/#ayuda-y-documentacion-adicional_1","title":"Ayuda y Documentaci\u00f3n Adicional","text":"<p>Para obtener m\u00e1s detalles sobre la creaci\u00f3n y gesti\u00f3n de un data lake en AWS sin utilizar AWS Lake Formation, puedes consultar la documentaci\u00f3n oficial de AWS sobre Data Lake. Adem\u00e1s, AWS ofrece tutoriales y ejemplos de implementaci\u00f3n para ayudarte en tu proyecto de data lake.</p>"},{"location":"SBD/Tema01/DataLakeAWS/#conclusion_1","title":"Conclusi\u00f3n","text":"<p>Crear y gestionar un data lake en AWS sin AWS Lake Formation es factible utilizando los servicios principales de AWS y siguiendo las mejores pr\u00e1cticas de gobernanza y seguridad. Esto te permitir\u00e1 almacenar, procesar y analizar datos de manera eficiente y escalable en tu organizaci\u00f3n.</p> <p>Claro, aqu\u00ed tienes un ejemplo simplificado de c\u00f3mo crear y gestionar un data lake en AWS utilizando Amazon S3, AWS Glue y Amazon Athena. En este ejemplo, supondremos que estamos trabajando con datos de registros de servidores web almacenados en archivos JSON en un bucket de Amazon S3.</p>"},{"location":"SBD/Tema01/DataLakeAWS/#paso-1-configurar-amazon-s3_1","title":"Paso 1: Configurar Amazon S3","text":"<ol> <li> <p>Crear un Bucket de Amazon S3: Inicia sesi\u00f3n en la consola de AWS y crea un bucket de S3 llamado \"mi-data-lake\".</p> </li> <li> <p>Configurar Pol\u00edticas de Acceso: Configura pol\u00edticas de acceso en el bucket de S3 para definir qui\u00e9n puede acceder a los datos y en qu\u00e9 condiciones.</p> </li> <li> <p>Habilitar Cifrado: Habilita el cifrado de datos en reposo y en tr\u00e1nsito en el bucket de S3 para garantizar la seguridad de los datos almacenados.</p> </li> </ol>"},{"location":"SBD/Tema01/DataLakeAWS/#paso-2-catalogar-datos-con-aws-glue_1","title":"Paso 2: Catalogar Datos con AWS Glue","text":"<ol> <li> <p>Utilizar AWS Glue Data Catalog: Configura AWS Glue Data Catalog para catalogar y gestionar los metadatos de tus datos en el data lake.</p> </li> <li> <p>Ejecutar Crawler: Crea un Crawler de AWS Glue y config\u00faralo para que descubra y catalogue autom\u00e1ticamente los datos en el bucket de S3. El Crawler puede reconocer el formato JSON de los archivos y crear tablas en el cat\u00e1logo.</p> </li> </ol>"},{"location":"SBD/Tema01/DataLakeAWS/#paso-3-procesamiento-y-transformacion-de-datos_1","title":"Paso 3: Procesamiento y Transformaci\u00f3n de Datos","text":"<ol> <li>Utilizar AWS Glue ETL: Utiliza AWS Glue ETL para definir trabajos de transformaci\u00f3n de datos. Por ejemplo, puedes limpiar y estructurar los datos JSON en tablas relacionales.</li> </ol>"},{"location":"SBD/Tema01/DataLakeAWS/#paso-4-herramientas-de-analisis_1","title":"Paso 4: Herramientas de An\u00e1lisis","text":"<ol> <li>Configurar Amazon Athena: Configura Amazon Athena para que consulte los datos catalogados en el cat\u00e1logo de AWS Glue. Puedes utilizar SQL est\u00e1ndar para consultar los datos.</li> </ol>"},{"location":"SBD/Tema01/DataLakeAWS/#paso-5-gobernanza-y-seguridad_1","title":"Paso 5: Gobernanza y Seguridad","text":"<ol> <li> <p>Pol\u00edticas de Acceso: Configura pol\u00edticas de acceso y permisos en Amazon Athena para controlar qui\u00e9n puede ejecutar consultas en el data lake.</p> </li> <li> <p>Auditor\u00eda y Monitoreo: Implementa la auditor\u00eda y el monitoreo de las consultas en Amazon Athena utilizando AWS CloudTrail y Amazon CloudWatch.</p> </li> </ol>"},{"location":"SBD/Tema01/DataLakeAWS/#paso-6-automatizacion-de-procesos_1","title":"Paso 6: Automatizaci\u00f3n de Procesos","text":"<ol> <li>Automatizaci\u00f3n de Flujos de Trabajo: Utiliza AWS Step Functions para crear flujos de trabajo automatizados que incluyan la ejecuci\u00f3n de trabajos de AWS Glue ETL y consultas en Amazon Athena.</li> </ol>"},{"location":"SBD/Tema01/DataLakeAWS/#paso-7-casos-de-uso-comunes_1","title":"Paso 7: Casos de Uso Comunes","text":"<ol> <li>Casos de Uso: Utiliza el data lake para casos de uso comunes, como an\u00e1lisis de registros de servidores web, generaci\u00f3n de informes de tr\u00e1fico, seguimiento de usuarios y m\u00e1s.</li> </ol> <p>Este es un ejemplo b\u00e1sico, y en un entorno real, la configuraci\u00f3n y gesti\u00f3n del data lake ser\u00edan m\u00e1s complejas y estar\u00edan dise\u00f1adas para satisfacer las necesidades espec\u00edficas de tu organizaci\u00f3n. Adem\u00e1s, AWS ofrece servicios adicionales para la gesti\u00f3n de datos, como AWS Glue DataBrew y AWS Lake Formation, que pueden facilitar a\u00fan m\u00e1s la creaci\u00f3n y gesti\u00f3n de un data lake.</p> <p>Crear un Data Lake en AWS sin utilizar AWS Lake Formation implica configurar manualmente los servicios de AWS para almacenar, catalogar y gestionar tus datos. A continuaci\u00f3n, te proporciono un ejemplo pr\u00e1ctico de c\u00f3mo puedes crear un Data Lake utilizando servicios como Amazon S3, AWS Glue y Amazon Athena:</p> <p>Paso 1: Configuraci\u00f3n de Almacenamiento en Amazon S3:</p> <ol> <li>Crea un bucket de Amazon S3 para actuar como el almacenamiento central de tu Data Lake.</li> <li>Organiza tus datos en carpetas l\u00f3gicas dentro del bucket de S3. Por ejemplo, puedes tener carpetas como <code>datos_brutos</code>, <code>datos_transformados</code>, <code>archivos_csv</code>, etc.</li> </ol> <p>Paso 2: Carga de Datos:</p> <ol> <li>Carga tus datos en formato bruto en la carpeta <code>datos_brutos</code> en tu bucket de S3. Puedes utilizar la consola de AWS, la CLI de AWS o herramientas de transferencia como AWS DataSync.</li> </ol> <p>Paso 3: Catalogaci\u00f3n de Datos con AWS Glue:</p> <ol> <li>Configura AWS Glue Crawler para descubrir y catalogar autom\u00e1ticamente los datos en tu bucket de S3. Debes definir una base de datos de AWS Glue y tablas para cada conjunto de datos que quieras catalogar.</li> <li>Ejecuta el Crawler para que escanee tus datos y genere metadatos en el Cat\u00e1logo de AWS Glue.</li> </ol> <p>Paso 4: Transformaci\u00f3n de Datos con AWS Glue:</p> <ol> <li>Crea trabajos de AWS Glue para realizar transformaciones en tus datos catalogados si es necesario. Esto podr\u00eda incluir la limpieza de datos, la agregaci\u00f3n o la conversi\u00f3n de formatos.</li> <li>Almacena los resultados de las transformaciones en una ubicaci\u00f3n espec\u00edfica en tu bucket de S3, por ejemplo, en la carpeta <code>datos_transformados</code>.</li> </ol> <p>Paso 5: Consulta de Datos con Amazon Athena:</p> <ol> <li>Utiliza Amazon Athena para consultar los datos en tu Data Lake. Puedes escribir consultas SQL est\u00e1ndar para acceder a los datos catalogados en el Cat\u00e1logo de AWS Glue.</li> <li>Amazon Athena utiliza Presto como motor de consulta, lo que te permite realizar an\u00e1lisis ad hoc sobre tus datos.</li> </ol> <p>Paso 6: Gesti\u00f3n de Seguridad y Acceso:</p> <ol> <li>Configura pol\u00edticas de acceso en Amazon S3 para controlar qui\u00e9n puede acceder y modificar tus datos.</li> <li>Utiliza AWS Identity and Access Management (IAM) para gestionar permisos de acceso a los servicios de AWS y recursos.</li> </ol> <p>Paso 7: Monitoreo y Optimizaci\u00f3n:</p> <ol> <li>Configura la monitorizaci\u00f3n y los registros de AWS CloudWatch para supervisar el rendimiento de tu Data Lake.</li> <li>Optimiza la configuraci\u00f3n de almacenamiento y las consultas para mejorar la eficiencia y reducir costos.</li> </ol> <p>Este es un ejemplo b\u00e1sico de c\u00f3mo puedes crear un Data Lake en AWS sin utilizar AWS Lake Formation. Ten en cuenta que esta configuraci\u00f3n manual puede ser m\u00e1s laboriosa que utilizar Lake Formation, que simplifica muchas de estas tareas. Adem\u00e1s, puedes personalizar y expandir esta arquitectura seg\u00fan las necesidades espec\u00edficas de tu proyecto y tu organizaci\u00f3n.</p> <p>Claro, aqu\u00ed tienes un conjunto de datos de ejemplo en formato CSV que puedes utilizar para tu Data Lake en AWS. Estos datos representan informaci\u00f3n ficticia de ventas:</p> <p>datos_brutos/ejemplo_ventas.csv:</p> <pre><code>fecha,producto,cantidad,precio\n2023-01-01,Producto A,10,100.00\n2023-01-01,Producto B,5,75.00\n2023-01-02,Producto A,8,100.00\n2023-01-02,Producto C,3,50.00\n2023-01-03,Producto B,12,75.00\n2023-01-03,Producto A,15,100.00\n2023-01-04,Producto C,6,50.00\n2023-01-04,Producto A,20,100.00\n2023-01-05,Producto B,7,75.00\n2023-01-05,Producto C,4,50.00\n</code></pre> <p>Puedes cargar estos datos en tu bucket de Amazon S3 en la carpeta <code>datos_brutos</code> para comenzar a construir tu Data Lake. Luego, puedes seguir los pasos mencionados anteriormente para catalogar, transformar y consultar estos datos utilizando servicios como AWS Glue y Amazon Athena.</p> <p>Recuerda que estos datos son solo un ejemplo y puedes adaptarlos seg\u00fan tus necesidades y el escenario de tu proyecto de Data Lake en AWS.</p>"},{"location":"SBD/Tema01/Ec2AWS/","title":"Documentaci\u00f3n: Amazon EC2 (Elastic Compute Cloud)","text":""},{"location":"SBD/Tema01/Ec2AWS/#introduccion-a-amazon-ec2","title":"Introducci\u00f3n a Amazon EC2","text":"<p>Amazon Elastic Compute Cloud (Amazon EC2) es un servicio de c\u00f3mputo en la nube escalable que permite a las empresas y desarrolladores lanzar y administrar m\u00e1quinas virtuales (instancias) en la infraestructura de AWS. EC2 proporciona la capacidad de c\u00f3mputo necesaria para ejecutar aplicaciones y cargas de trabajo de manera flexible y rentable.</p>"},{"location":"SBD/Tema01/Ec2AWS/#principales-caracteristicas-de-ec2","title":"Principales Caracter\u00edsticas de EC2","text":""},{"location":"SBD/Tema01/Ec2AWS/#1-elasticidad","title":"1. Elasticidad","text":"<ul> <li>Escalabilidad Vertical y Horizontal: Puedes escalar tus instancias hacia arriba (aumentar la capacidad) o hacia abajo (disminuir la capacidad) seg\u00fan la demanda. Esto garantiza que siempre tengas suficiente capacidad de c\u00f3mputo.</li> </ul>"},{"location":"SBD/Tema01/Ec2AWS/#2-variedad-de-tipos-de-instancia","title":"2. Variedad de Tipos de Instancia","text":"<ul> <li>Diversidad de Recursos: EC2 ofrece una amplia variedad de tipos de instancia optimizados para diferentes cargas de trabajo, desde instancias de prop\u00f3sito general hasta instancias optimizadas para GPU y c\u00f3mputo intensivo.</li> </ul>"},{"location":"SBD/Tema01/Ec2AWS/#3-seleccion-de-sistema-operativo","title":"3. Selecci\u00f3n de Sistema Operativo","text":"<ul> <li>Flexibilidad de SO: Puedes seleccionar el sistema operativo que mejor se adapte a tus necesidades, incluidos Linux, Windows, macOS y sistemas basados en contenedores como Amazon ECS.</li> </ul>"},{"location":"SBD/Tema01/Ec2AWS/#4-almacenamiento-y-snapshot","title":"4. Almacenamiento y Snapshot","text":"<ul> <li>Almacenamiento EBS: EC2 ofrece almacenamiento Elastic Block Store (EBS) para adjuntar vol\u00famenes de datos persistentes a tus instancias. Puedes crear instant\u00e1neas (snapshots) de estos vol\u00famenes para copias de seguridad y replicaci\u00f3n.</li> </ul>"},{"location":"SBD/Tema01/Ec2AWS/#5-redes-y-seguridad","title":"5. Redes y Seguridad","text":"<ul> <li>Amazon VPC: Puedes crear tu propia red virtual (Amazon VPC) para aislar tus instancias y controlar el acceso a ellas. Tambi\u00e9n puedes configurar grupos de seguridad y asignar direcciones IP el\u00e1sticas.</li> </ul>"},{"location":"SBD/Tema01/Ec2AWS/#6-alta-disponibilidad","title":"6. Alta Disponibilidad","text":"<ul> <li>Auto Scaling y Load Balancing: EC2 es compatible con Auto Scaling para ajustar autom\u00e1ticamente el n\u00famero de instancias en funci\u00f3n de la demanda. Tambi\u00e9n puedes usar Amazon Elastic Load Balancing (ELB) para distribuir el tr\u00e1fico entre m\u00faltiples instancias.</li> </ul>"},{"location":"SBD/Tema01/Ec2AWS/#creacion-de-instancias-ec2","title":"Creaci\u00f3n de Instancias EC2","text":"<p>El proceso de creaci\u00f3n de instancias EC2 generalmente implica los siguientes pasos:</p>"},{"location":"SBD/Tema01/Ec2AWS/#1-acceso-a-la-consola-de-aws","title":"1. Acceso a la Consola de AWS","text":"<p>Inicia sesi\u00f3n en la Consola de AWS con tus credenciales de cuenta.</p>"},{"location":"SBD/Tema01/Ec2AWS/#2-navegacion-a-ec2","title":"2. Navegaci\u00f3n a EC2","text":"<p>Desde el panel de AWS, navega a la secci\u00f3n \"Servicios\" y selecciona \"EC2\" bajo la categor\u00eda \"Compute\".</p>"},{"location":"SBD/Tema01/Ec2AWS/#3-lanzamiento-de-una-instancia","title":"3. Lanzamiento de una Instancia","text":"<ul> <li>Haz clic en \"Launch Instance\" para comenzar el proceso de lanzamiento de una instancia.</li> <li>Selecciona una AMI (Amazon Machine Image) que sirva como plantilla para tu instancia.</li> <li>Elije el tipo de instancia, la cantidad de instancias y configura la red y el almacenamiento seg\u00fan tus necesidades.</li> <li>Configura las reglas de seguridad y las claves de acceso.</li> <li>Revisa y confirma la configuraci\u00f3n antes de lanzar la instancia.</li> </ul>"},{"location":"SBD/Tema01/Ec2AWS/#4-conexion-a-la-instancia","title":"4. Conexi\u00f3n a la Instancia","text":"<p>Despu\u00e9s de lanzar la instancia, puedes conectarte a ella utilizando SSH (para sistemas basados en Unix) o RDP (para sistemas Windows).</p>"},{"location":"SBD/Tema01/Ec2AWS/#casos-de-uso-de-ec2","title":"Casos de Uso de EC2","text":"<p>Amazon EC2 es vers\u00e1til y se utiliza en una amplia variedad de casos de uso, que incluyen:</p> <ul> <li>Hospedaje de Sitios Web: Ejecutar servidores web y alojar sitios y aplicaciones web.</li> <li>Desarrollo y Pruebas: Crear entornos de desarrollo y pruebas bajo demanda.</li> <li>Procesamiento de Datos: Ejecutar tareas de procesamiento de datos y an\u00e1lisis con clusters de EC2.</li> <li>Aplicaciones Empresariales: Ejecutar aplicaciones empresariales cr\u00edticas en instancias EC2.</li> <li>Ejecuci\u00f3n de Contenedores: Implementar y administrar contenedores con servicios como Amazon ECS y EKS.</li> </ul>"},{"location":"SBD/Tema01/Ec2AWS/#conclusion","title":"Conclusion","text":"<p>Amazon EC2 es un componente esencial de la plataforma AWS y proporciona una potente capacidad de c\u00f3mputo en la nube para empresas y desarrolladores. Con su escalabilidad, variedad de tipos de instancia y opciones de configuraci\u00f3n, EC2 es adecuado para una amplia gama de casos de uso y aplicaciones en la nube. Explora EC2 y aprovecha sus capacidades para tus proyectos y cargas de trabajo.</p> <p>Por supuesto, aqu\u00ed tienes una documentaci\u00f3n sobre el servicio Amazon Elastic Compute Cloud (EC2) de AWS, que incluye enlaces a la documentaci\u00f3n oficial de AWS para obtener informaci\u00f3n detallada sobre cada aspecto del servicio:</p>"},{"location":"SBD/Tema01/Ec2AWS/#amazon-elastic-compute-cloud-ec2","title":"Amazon Elastic Compute Cloud (EC2)","text":"<p>Amazon Elastic Compute Cloud (EC2) es un servicio de c\u00f3mputo en la nube escalable y altamente configurable que permite a los usuarios ejecutar m\u00e1quinas virtuales (instancias) en la infraestructura de AWS. Las instancias de EC2 proporcionan capacidad de c\u00f3mputo bajo demanda y se utilizan para una amplia variedad de casos de uso, desde hospedar sitios web hasta ejecutar aplicaciones empresariales y realizar tareas de procesamiento de datos.</p>"},{"location":"SBD/Tema01/Ec2AWS/#contenido","title":"Contenido","text":"<ol> <li>Conceptos Fundamentales</li> <li>Creaci\u00f3n de Instancias EC2</li> <li>Almacenamiento y Snapshots</li> <li>Redes y Seguridad</li> <li>Escalabilidad y Balanceo de Carga</li> <li>Gesti\u00f3n y Monitoreo</li> <li>Documentaci\u00f3n Adicional de AWS</li> </ol>"},{"location":"SBD/Tema01/Ec2AWS/#1-conceptos-fundamentales","title":"1. Conceptos Fundamentales","text":"<ul> <li>\u00bfQu\u00e9 es Amazon EC2?: Introducci\u00f3n al servicio EC2 y sus caracter\u00edsticas clave.</li> <li>Tipos de Instancias: Informaci\u00f3n sobre los diferentes tipos de instancias EC2 y c\u00f3mo elegir la m\u00e1s adecuada para tus necesidades.</li> <li>Regiones y Zonas de Disponibilidad: Explicaci\u00f3n de la infraestructura global de AWS y c\u00f3mo seleccionar la regi\u00f3n adecuada.</li> </ul>"},{"location":"SBD/Tema01/Ec2AWS/#2-creacion-de-instancias-ec2","title":"2. Creaci\u00f3n de Instancias EC2","text":"<ul> <li>Iniciar una Instancia EC2: Gu\u00eda paso a paso para crear tu primera instancia EC2.</li> <li>Plantillas de Lanzamiento (AMIs): Informaci\u00f3n sobre las Amazon Machine Images (AMIs) y c\u00f3mo personalizarlas.</li> <li>Acceso a Instancias: C\u00f3mo conectarse a tus instancias EC2 de diferentes formas, incluyendo SSH y SSM.</li> </ul>"},{"location":"SBD/Tema01/Ec2AWS/#3-almacenamiento-y-snapshots","title":"3. Almacenamiento y Snapshots","text":"<ul> <li>Almacenamiento en EBS: Detalles sobre Elastic Block Store (EBS) y c\u00f3mo adjuntar y gestionar vol\u00famenes de EBS.</li> <li>Snapshots de EBS: C\u00f3mo crear y administrar snapshots de tus vol\u00famenes de EBS.</li> <li>Almacenamiento en Instancias: Informaci\u00f3n sobre el almacenamiento local en instancias EC2.</li> </ul>"},{"location":"SBD/Tema01/Ec2AWS/#4-redes-y-seguridad","title":"4. Redes y Seguridad","text":"<ul> <li>Grupos de Seguridad de EC2: C\u00f3mo configurar grupos de seguridad para controlar el tr\u00e1fico de red hacia y desde tus instancias.</li> <li>Asignaci\u00f3n de IPs El\u00e1sticas: C\u00f3mo asignar IPs el\u00e1sticas a tus instancias EC2 para mantener direcciones IP persistentes.</li> <li>VPC (Virtual Private Cloud): Informaci\u00f3n sobre la creaci\u00f3n y configuraci\u00f3n de redes virtuales privadas en AWS.</li> </ul>"},{"location":"SBD/Tema01/Ec2AWS/#5-escalabilidad-y-balanceo-de-carga","title":"5. Escalabilidad y Balanceo de Carga","text":"<ul> <li>Auto Scaling: C\u00f3mo utilizar Auto Scaling para ajustar autom\u00e1ticamente la capacidad de tus instancias.</li> <li>Balanceo de Carga (ELB): Informaci\u00f3n sobre Elastic Load Balancing (ELB) y c\u00f3mo distribuir el tr\u00e1fico de red entre tus instancias.</li> </ul>"},{"location":"SBD/Tema01/Ec2AWS/#6-gestion-y-monitoreo","title":"6. Gesti\u00f3n y Monitoreo","text":"<ul> <li>Amazon CloudWatch: C\u00f3mo utilizar Amazon CloudWatch para monitorear tus instancias EC2 y recursos de AWS.</li> <li>Administraci\u00f3n de Instancias con SSM: Uso de AWS Systems Manager para administrar y automatizar tareas en tus instancias EC2.</li> </ul>"},{"location":"SBD/Tema01/Ec2AWS/#7-documentacion-adicional-de-aws","title":"7. Documentaci\u00f3n Adicional de AWS","text":"<ul> <li>Documentaci\u00f3n de Amazon EC2: P\u00e1gina principal de la documentaci\u00f3n oficial de Amazon EC2 con gu\u00edas detalladas y referencias.</li> <li>Preguntas Frecuentes (FAQ): Preguntas frecuentes sobre EC2.</li> </ul> <p>Esta documentaci\u00f3n proporciona una base s\u00f3lida para comenzar a utilizar Amazon EC2 y aprovechar todas sus capacidades. Te recomiendo explorar los enlaces y recursos mencionados para obtener informaci\u00f3n detallada sobre cada aspecto del servicio EC2 de AWS.</p>"},{"location":"SBD/Tema01/EtlAWS/","title":"Documentaci\u00f3n Ampliada: AWS Glue","text":""},{"location":"SBD/Tema01/EtlAWS/#introduccion-a-aws-glue","title":"Introducci\u00f3n a AWS Glue","text":"<p>AWS Glue es un servicio de extracci\u00f3n, transformaci\u00f3n y carga (ETL) completamente administrado que facilita la preparaci\u00f3n y carga de datos para el an\u00e1lisis en plataformas de big data. Con AWS Glue, puedes definir, ejecutar y monitorear trabajos ETL de manera sencilla, lo que te permite mover y transformar datos desde diversas fuentes hacia almacenes de datos como Amazon Redshift, Amazon S3 y m\u00e1s.</p> <p>Documentaci\u00f3n Oficial de AWS Glue: AWS Glue - Documentaci\u00f3n de AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#componentes-clave-de-aws-glue","title":"Componentes Clave de AWS Glue","text":""},{"location":"SBD/Tema01/EtlAWS/#1-crawlers","title":"1. Crawlers","text":"<ul> <li>Crawlers: Los crawlers son componentes de AWS Glue que exploran tus fuentes de datos, identifican la estructura y el esquema de tus datos y generan metadatos que se utilizan en los trabajos ETL.</li> </ul> <p>Documentaci\u00f3n Relacionada: Crawlers en AWS Glue - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#2-catalogo-de-datos","title":"2. Cat\u00e1logo de Datos","text":"<ul> <li>Cat\u00e1logo de Datos: El cat\u00e1logo de datos de AWS Glue es un repositorio centralizado donde se almacenan los metadatos de tus fuentes de datos, tablas y esquemas. Facilita la gesti\u00f3n y organizaci\u00f3n de tus datos.</li> </ul> <p>Documentaci\u00f3n Relacionada: Cat\u00e1logo de Datos en AWS Glue - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#3-trabajos-etl","title":"3. Trabajos ETL","text":"<ul> <li>Trabajos ETL: Los trabajos ETL son scripts o flujos de trabajo que transforman y mueven datos de una fuente a un destino. AWS Glue permite crear, programar y monitorear trabajos ETL de manera sencilla.</li> </ul> <p>Documentaci\u00f3n Relacionada: Trabajos ETL en AWS Glue - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#4-desarrollo-de-scripts","title":"4. Desarrollo de Scripts","text":"<ul> <li>Desarrollo de Scripts: AWS Glue permite desarrollar trabajos ETL utilizando lenguajes como Python o Scala. Puedes personalizar la l\u00f3gica de transformaci\u00f3n de datos seg\u00fan tus necesidades.</li> </ul> <p>Documentaci\u00f3n Relacionada: Desarrollo de Trabajos con Scripts en AWS Glue - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#configuracion-y-uso-basico-de-aws-glue","title":"Configuraci\u00f3n y Uso B\u00e1sico de AWS Glue","text":""},{"location":"SBD/Tema01/EtlAWS/#1-creacion-de-un-crawler","title":"1. Creaci\u00f3n de un Crawler","text":"<ul> <li>Utiliza un crawler para explorar tus fuentes de datos y generar metadatos en el cat\u00e1logo de datos de AWS Glue.</li> </ul> <p>Documentaci\u00f3n Relacionada: Creaci\u00f3n de un Crawler en AWS Glue - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#2-creacion-de-un-catalogo-de-datos","title":"2. Creaci\u00f3n de un Cat\u00e1logo de Datos","text":"<ul> <li>Configura un cat\u00e1logo de datos en AWS Glue para almacenar metadatos de fuentes de datos, tablas y esquemas.</li> </ul> <p>Documentaci\u00f3n Relacionada: Configuraci\u00f3n del Cat\u00e1logo de Datos en AWS Glue - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#3-desarrollo-de-un-trabajo-etl","title":"3. Desarrollo de un Trabajo ETL","text":"<ul> <li>Desarrolla un trabajo ETL en AWS Glue utilizando lenguajes de script como Python o Scala para transformar y mover datos.</li> </ul> <p>Documentaci\u00f3n Relacionada: Desarrollo de Trabajos ETL en AWS Glue - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#4-ejecucion-de-un-trabajo-etl","title":"4. Ejecuci\u00f3n de un Trabajo ETL","text":"<ul> <li>Programa y ejecuta trabajos ETL en AWS Glue para mover y transformar datos de acuerdo a tus necesidades.</li> </ul> <p>Documentaci\u00f3n Relacionada: Programaci\u00f3n y Ejecuci\u00f3n de Trabajos en AWS Glue - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#casos-de-uso-comunes-de-aws-glue","title":"Casos de Uso Comunes de AWS Glue","text":""},{"location":"SBD/Tema01/EtlAWS/#1-preparacion-de-datos-para-analisis","title":"1. Preparaci\u00f3n de Datos para An\u00e1lisis","text":"<ul> <li>Utiliza AWS Glue para preparar datos antes de cargarlos en almacenes de datos como Amazon Redshift o Amazon S3.</li> </ul>"},{"location":"SBD/Tema01/EtlAWS/#2-integracion-de-datos-de-multiples-fuentes","title":"2. Integraci\u00f3n de Datos de M\u00faltiples Fuentes","text":"<ul> <li>Combina y transforma datos de diversas fuentes para obtener una vista unificada de tus datos.</li> </ul>"},{"location":"SBD/Tema01/EtlAWS/#3-limpieza-y-transformacion-de-datos","title":"3. Limpieza y Transformaci\u00f3n de Datos","text":"<ul> <li>Aplica reglas de limpie</li> </ul> <p>za y transformaci\u00f3n a tus datos para asegurar la calidad y consistencia.</p> <p>Documentaci\u00f3n Relacionada: Casos de Uso de AWS Glue - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#recomendaciones-de-seguridad","title":"Recomendaciones de Seguridad","text":"<ul> <li>Control de Acceso: Configura pol\u00edticas de control de acceso para garantizar que solo usuarios autorizados puedan acceder y modificar tus recursos de AWS Glue.</li> </ul> <p>Documentaci\u00f3n Relacionada: Control de Acceso en AWS Glue - AWS</p> <ul> <li>Seguridad de Datos: Utiliza cifrado de datos en reposo y en tr\u00e1nsito para proteger la confidencialidad de tus datos en AWS Glue.</li> </ul> <p>Documentaci\u00f3n Relacionada: Seguridad en AWS Glue - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#ayuda-y-documentacion","title":"Ayuda y Documentaci\u00f3n","text":"<p>Para obtener m\u00e1s detalles sobre AWS Glue y c\u00f3mo utilizarlo en tus proyectos, puedes consultar la documentaci\u00f3n oficial de AWS Glue. Adem\u00e1s, AWS ofrece ejemplos de c\u00f3digo y tutoriales para facilitar la implementaci\u00f3n.</p>"},{"location":"SBD/Tema01/EtlAWS/#conclusion","title":"Conclusi\u00f3n","text":"<p>AWS Glue es una herramienta poderosa para la preparaci\u00f3n y carga de datos en entornos de big data. Al comprender los componentes clave y las mejores pr\u00e1cticas de seguridad, puedes aprovechar al m\u00e1ximo este servicio para facilitar tus flujos de trabajo ETL y an\u00e1lisis de datos.</p> <p>Por supuesto, aqu\u00ed tienes una documentaci\u00f3n ampliada sobre Amazon Athena, un servicio de AWS que permite consultar datos almacenados en Amazon S3 utilizando SQL est\u00e1ndar.</p>"},{"location":"SBD/Tema01/EtlAWS/#documentacion-ampliada-amazon-athena","title":"Documentaci\u00f3n Ampliada: Amazon Athena","text":""},{"location":"SBD/Tema01/EtlAWS/#introduccion-a-amazon-athena","title":"Introducci\u00f3n a Amazon Athena","text":"<p>Amazon Athena es un servicio de consulta interactiva y an\u00e1lisis de datos que permite analizar grandes vol\u00famenes de datos almacenados en Amazon S3 utilizando SQL est\u00e1ndar. Con Athena, puedes ejecutar consultas SQL en datos no estructurados o semiestructurados sin necesidad de configurar ni administrar la infraestructura de base de datos.</p> <p>Documentaci\u00f3n Oficial de Amazon Athena: Amazon Athena - Documentaci\u00f3n de AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#componentes-clave-de-amazon-athena","title":"Componentes Clave de Amazon Athena","text":""},{"location":"SBD/Tema01/EtlAWS/#1-tablas","title":"1. Tablas","text":"<ul> <li>Tablas: En Amazon Athena, las tablas son definiciones de esquema que se utilizan para organizar y consultar datos en Amazon S3. Puedes crear tablas externas que hacen referencia a datos en S3 sin mover ni copiar los datos.</li> </ul> <p>Documentaci\u00f3n Relacionada: Creaci\u00f3n de Tablas en Athena - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#2-consultas","title":"2. Consultas","text":"<ul> <li>Consultas: Utiliza consultas SQL est\u00e1ndar para analizar y extraer informaci\u00f3n de tus datos en Amazon S3.</li> </ul> <p>Documentaci\u00f3n Relacionada: Ejecuci\u00f3n de Consultas en Athena - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#3-resultados","title":"3. Resultados","text":"<ul> <li>Resultados: Las consultas en Athena generan resultados que pueden descargarse, visualizarse o almacenarse en otros servicios de AWS.</li> </ul> <p>Documentaci\u00f3n Relacionada: Gesti\u00f3n de Resultados de Consultas en Athena - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#configuracion-y-uso-basico-de-amazon-athena","title":"Configuraci\u00f3n y Uso B\u00e1sico de Amazon Athena","text":""},{"location":"SBD/Tema01/EtlAWS/#1-creacion-de-tablas","title":"1. Creaci\u00f3n de Tablas","text":"<ul> <li>Define tablas en Athena para organizar y consultar tus datos en Amazon S3. Puedes crear tablas externas que hacen referencia a datos en S3 sin mover los datos.</li> </ul> <p>Documentaci\u00f3n Relacionada: Creaci\u00f3n de Tablas en Athena - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#2-ejecucion-de-consultas","title":"2. Ejecuci\u00f3n de Consultas","text":"<ul> <li>Utiliza la consola de Athena o herramientas como AWS Glue DataBrew para ejecutar consultas SQL en tus datos en Amazon S3.</li> </ul> <p>Documentaci\u00f3n Relacionada: Ejecuci\u00f3n de Consultas en Athena - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#3-gestion-de-resultados","title":"3. Gesti\u00f3n de Resultados","text":"<ul> <li>Administra y descarga los resultados de tus consultas en Athena para su an\u00e1lisis o almacenamiento en otros servicios de AWS.</li> </ul> <p>Documentaci\u00f3n Relacionada: Gesti\u00f3n de Resultados de Consultas en Athena - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#casos-de-uso-comunes-de-amazon-athena","title":"Casos de Uso Comunes de Amazon Athena","text":""},{"location":"SBD/Tema01/EtlAWS/#1-analisis-de-datos-en-amazon-s3","title":"1. An\u00e1lisis de Datos en Amazon S3","text":"<ul> <li>Utiliza Athena para analizar y consultar datos en Amazon S3 sin necesidad de moverlos a una base de datos.</li> </ul>"},{"location":"SBD/Tema01/EtlAWS/#2-analisis-de-logs-y-registros","title":"2. An\u00e1lisis de Logs y Registros","text":"<ul> <li>Examina registros y datos de aplicaci\u00f3n almacenados en Amazon S3 para obtener informaci\u00f3n sobre el rendimiento y el comportamiento de las aplicaciones.</li> </ul>"},{"location":"SBD/Tema01/EtlAWS/#3-exploracion-de-datos","title":"3. Exploraci\u00f3n de Datos","text":"<ul> <li>Realiza an\u00e1lisis exploratorios de datos utilizando consultas SQL en datos semiestructurados o no estructurados.</li> </ul> <p>Documentaci\u00f3n Relacionada: Casos de Uso de Athena - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#recomendaciones-de-seguridad_1","title":"Recomendaciones de Seguridad","text":"<ul> <li>Control de Acceso: Configura pol\u00edticas de control de acceso para garantizar que solo usuarios autorizados puedan ejecutar consultas en Athena.</li> </ul> <p>Documentaci\u00f3n Relacionada: Control de Acceso en Athena - AWS</p> <ul> <li>Cifrado de Datos: Habilita el cifrado de datos en reposo y en tr\u00e1nsito para proteger la confidencialidad de tus datos en Athena.</li> </ul> <p>Documentaci\u00f3n Relacionada: Cifrado de Datos en Athena - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#ayuda-y-documentacion_1","title":"Ayuda y Documentaci\u00f3n","text":"<p>Para obtener m\u00e1s detalles sobre Amazon Athena y c\u00f3mo utilizarlo en tus proyectos, puedes consultar la documentaci\u00f3n oficial de Amazon Athena. Adem\u00e1s, AWS ofrece ejemplos de c\u00f3digo y tutoriales para facilitar la implementaci\u00f3n.</p>"},{"location":"SBD/Tema01/EtlAWS/#conclusion_1","title":"Conclusi\u00f3n","text":"<p>Amazon Athena es una herramienta poderosa para analizar datos en Amazon S3 mediante consultas SQL est\u00e1ndar. Al comprender los conceptos clave y las mejores pr\u00e1cticas de seguridad, puedes aprovechar al m\u00e1ximo este servicio para realizar an\u00e1lisis de datos eficaces en tu organizaci\u00f3n.</p> <p>Claro, aqu\u00ed tienes una documentaci\u00f3n detallada sobre c\u00f3mo realizar una ETL (Extract, Transform, Load) utilizando los servicios de AWS, junto con enlaces a la documentaci\u00f3n oficial de AWS para obtener m\u00e1s informaci\u00f3n detallada.</p>"},{"location":"SBD/Tema01/EtlAWS/#documentacion-realizacion-de-una-etl-con-servicios-de-aws","title":"Documentaci\u00f3n: Realizaci\u00f3n de una ETL con Servicios de AWS","text":""},{"location":"SBD/Tema01/EtlAWS/#introduccion-a-etl-en-aws","title":"Introducci\u00f3n a ETL en AWS","text":"<p>El proceso ETL (Extract, Transform, Load) es fundamental para la gesti\u00f3n y preparaci\u00f3n de datos en la nube de AWS. AWS ofrece una variedad de servicios que facilitan la ejecuci\u00f3n de ETL en la nube.</p> <p>Documentaci\u00f3n Oficial de AWS sobre ETL: ETL en AWS - Documentaci\u00f3n de AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#componentes-clave-de-una-etl-en-aws","title":"Componentes Clave de una ETL en AWS","text":""},{"location":"SBD/Tema01/EtlAWS/#1-amazon-s3","title":"1. Amazon S3","text":"<ul> <li>Amazon S3: Utiliza Amazon Simple Storage Service (Amazon S3) como el almacenamiento central para tus datos brutos y transformados. Amazon S3 es escalable y altamente duradero.</li> </ul> <p>Documentaci\u00f3n Relacionada: Amazon S3 - Documentaci\u00f3n de AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#2-aws-glue","title":"2. AWS Glue","text":"<ul> <li>AWS Glue: Utiliza AWS Glue para automatizar tareas de extracci\u00f3n, transformaci\u00f3n y carga de datos. AWS Glue es un servicio de ETL totalmente administrado.</li> </ul> <p>Documentaci\u00f3n Relacionada: AWS Glue - Documentaci\u00f3n de AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#3-aws-step-functions","title":"3. AWS Step Functions","text":"<ul> <li>AWS Step Functions: Utiliza AWS Step Functions para orquestar flujos de trabajo de ETL que involucran m\u00faltiples pasos y servicios de AWS.</li> </ul> <p>Documentaci\u00f3n Relacionada: AWS Step Functions - Documentaci\u00f3n de AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#4-herramientas-de-analisis","title":"4. Herramientas de An\u00e1lisis","text":"<ul> <li>Herramientas de An\u00e1lisis: Utiliza herramientas de an\u00e1lisis como Amazon Athena, Amazon Redshift o AWS QuickSight para analizar los datos transformados despu\u00e9s de la carga.</li> </ul> <p>Documentaci\u00f3n Relacionada: Herramientas de An\u00e1lisis en AWS - Documentaci\u00f3n de AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#configuracion-y-uso-basico-de-una-etl-en-aws","title":"Configuraci\u00f3n y Uso B\u00e1sico de una ETL en AWS","text":""},{"location":"SBD/Tema01/EtlAWS/#1-configuracion-de-almacenamiento","title":"1. Configuraci\u00f3n de Almacenamiento","text":"<ul> <li>Configura un Bucket de Amazon S3: Crea un bucket de Amazon S3 para almacenar tus datos brutos y transformados.</li> </ul> <p>Documentaci\u00f3n Relacionada: Crear un Bucket de S3 - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#2-extraccion-de-datos","title":"2. Extracci\u00f3n de Datos","text":"<ul> <li>Utiliza AWS Glue Crawlers: Configura y ejecuta AWS Glue Crawlers para descubrir y catalogar autom\u00e1ticamente los datos en tus fuentes, como bases de datos, archivos y servicios.</li> </ul> <p>Documentaci\u00f3n Relacionada: Configurar AWS Glue Crawlers - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#3-transformacion-de-datos","title":"3. Transformaci\u00f3n de Datos","text":"<ul> <li>Utiliza AWS Glue ETL Jobs: Crea trabajos de ETL en AWS Glue para transformar datos seg\u00fan tus necesidades. Puedes utilizar el editor de scripts de Python de AWS Glue para personalizar las transformaciones.</li> </ul> <p>Documentaci\u00f3n Relacionada: Crear un Job de AWS Glue ETL - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#4-carga-de-datos","title":"4. Carga de Datos","text":"<ul> <li>Carga de Datos en Amazon S3: Despu\u00e9s de la transformaci\u00f3n, carga los datos en un bucket de Amazon S3 para su posterior an\u00e1lisis.</li> </ul> <p>Documentaci\u00f3n Relacionada: Cargar Datos en Amazon S3 - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#5-orquestacion-de-flujos-de-trabajo","title":"5. Orquestaci\u00f3n de Flujos de Trabajo","text":"<ul> <li>Orquesta Flujos de Trabajo con AWS Step Functions: Utiliza AWS Step Functions para orquestar flujos de trabajo de ETL que involucran extracci\u00f3n, transformaci\u00f3n y carga de datos.</li> </ul> <p>Documentaci\u00f3n Relacionada: Orquestaci\u00f3n de Flujos de Trabajo con AWS Step Functions - AWS</p>"},{"location":"SBD/Tema01/EtlAWS/#recomendaciones-de-mejores-practicas","title":"Recomendaciones de Mejores Pr\u00e1cticas","text":"<ul> <li> <p>Seguridad y Gobernanza: Implementa pol\u00edticas de seguridad y gobernanza para proteger tus datos y garantizar la conformidad con las regulaciones.</p> </li> <li> <p>Monitorizaci\u00f3n y Auditor\u00eda: Establece una soluci\u00f3n de monitorizaci\u00f3n y auditor\u00eda para supervisar el rendimiento y la calidad de los flujos de trabajo de ETL.</p> </li> <li> <p>Automatizaci\u00f3n: Utiliza la automatizaci\u00f3n para ejecutar flujos de trabajo de ETL de manera regular y confiable.</p> </li> </ul>"},{"location":"SBD/Tema01/EtlAWS/#ejemplo-practico-de-etl-con-aws","title":"Ejemplo Pr\u00e1ctico de ETL con AWS","text":"<p>A continuaci\u00f3n, se muestra un ejemplo simplificado de un flujo de trabajo de ETL utilizando AWS Glue y Amazon S3:</p> <ol> <li> <p>Configura un bucket de Amazon S3 para almacenar los datos brutos.</p> </li> <li> <p>Crea un Crawler de AWS Glue para catalogar autom\u00e1ticamente los datos en el bucket.</p> </li> <li> <p>Crea un Job de AWS Glue ETL para transformar los datos catalogados.</p> </li> <li> <p>Ejecuta el Job de AWS Glue para realizar la transformaci\u00f3n y carga los datos transformados en otro bucket de Amazon S3.</p> </li> <li> <p>Utiliza Amazon Athena para consultar y analizar los datos transformados.</p> </li> </ol> <p>Este es solo un ejemplo b\u00e1sico, y los flujos de trabajo de ETL pueden ser mucho m\u00e1s complejos seg\u00fan las necesidades de tu organizaci\u00f3n.</p>"},{"location":"SBD/Tema01/EtlAWS/#ayuda-y-documentacion-adicional","title":"Ayuda y Documentaci\u00f3n Adicional","text":"<p>Para obtener m\u00e1s detalles sobre c\u00f3mo realizar una ETL con servicios de AWS, consulta la documentaci\u00f3n oficial de AWS sobre ETL. Adem\u00e1s, AWS ofrece tutoriales y ejemplos detallados para ayudarte en tu proyecto de ETL en la nube de AWS.</p>"},{"location":"SBD/Tema01/EtlAWS/#conclusion_2","title":"Conclusi\u00f3n","text":"<p>Realizar una ETL en AWS es esencial para la gesti\u00f3n de datos y an\u00e1lisis en la nube. Utilizando los servicios de AWS como AWS Glue, Amazon S3 y AWS Step Functions, puedes automatizar y simplificar el proceso de extracci\u00f3n, transformaci\u00f3n y carga de datos para satisfacer las necesidades de tu organizaci\u00f3n.</p> <p>Puedes encontrar ejemplos pr\u00e1cticos de c\u00f3mo realizar una ETL (Extract, Transform, Load) en AWS en varios lugares, incluyendo la documentaci\u00f3n oficial de AWS, tutoriales en l\u00ednea y recursos de la comunidad. Aqu\u00ed te proporciono un ejemplo sencillo de una ETL utilizando AWS Glue y Amazon S3 como punto de partida:</p> <p>Escenario: Supongamos que tienes datos brutos almacenados en Amazon S3 en formato CSV y deseas realizar una ETL para transformar estos datos y cargarlos en otra ubicaci\u00f3n en Amazon S3 en formato Parquet.</p> <p>Pasos para un Ejemplo Pr\u00e1ctico de ETL en AWS:</p> <ol> <li>Configuraci\u00f3n de Almacenamiento en Amazon S3:</li> <li>Crea un bucket de Amazon S3 para almacenar tus datos brutos y transformados.</li> <li> <p>Por ejemplo, crea dos carpetas en tu bucket: <code>datos_brutos</code> y <code>datos_transformados</code>.</p> </li> <li> <p>Utiliza AWS Glue Crawlers para Descubrir los Datos:</p> </li> <li> <p>Configura un Crawler de AWS Glue para descubrir y catalogar autom\u00e1ticamente los datos en la carpeta <code>datos_brutos</code> de tu bucket de S3.</p> </li> <li> <p>Crea un Job de AWS Glue ETL:</p> </li> <li>Crea un trabajo de ETL en AWS Glue utilizando el editor de scripts de Python de Glue.</li> <li> <p>Define la transformaci\u00f3n que deseas aplicar a tus datos. Por ejemplo, puedes seleccionar columnas, filtrar filas o realizar agregaciones.</p> </li> <li> <p>Ejecuta el Job de AWS Glue:</p> </li> <li>Ejecuta el trabajo de ETL de AWS Glue para realizar la transformaci\u00f3n de datos.</li> <li> <p>Durante el proceso de transformaci\u00f3n, guarda los datos transformados en la carpeta <code>datos_transformados</code> en tu bucket de S3 en formato Parquet.</p> </li> <li> <p>An\u00e1lisis de los Datos Transformados:</p> </li> <li>Utiliza herramientas de an\u00e1lisis como Amazon Athena o Amazon Redshift para consultar y analizar los datos transformados en formato Parquet.</li> </ol> <p>Este es un ejemplo simplificado de una ETL en AWS utilizando AWS Glue y Amazon S3. Puedes adaptar este escenario b\u00e1sico seg\u00fan tus necesidades espec\u00edficas. Para ejemplos m\u00e1s detallados y casos de uso m\u00e1s complejos, te recomiendo consultar la documentaci\u00f3n oficial de AWS, tutoriales en l\u00ednea y recursos de la comunidad, donde encontrar\u00e1s ejemplos pr\u00e1cticos adicionales y proyectos de ETL m\u00e1s avanzados. Tambi\u00e9n puedes explorar ejemplos de c\u00f3digo y scripts de ETL en repositorios de GitHub y foros de discusi\u00f3n relacionados con AWS.</p> <p>Por supuesto, aqu\u00ed te proporciono un conjunto de datos de ejemplo en formato CSV que podr\u00edas utilizar en el escenario de ETL en AWS que describ\u00ed anteriormente. Estos datos representan informaci\u00f3n ficticia de ventas:</p> <p>datos_brutos/ejemplo_ventas.csv:</p> <pre><code>fecha,producto,cantidad,precio\n2023-01-01,Producto A,10,100.00\n2023-01-01,Producto B,5,75.00\n2023-01-02,Producto A,8,100.00\n2023-01-02,Producto C,3,50.00\n2023-01-03,Producto B,12,75.00\n2023-01-03,Producto A,15,100.00\n2023-01-04,Producto C,6,50.00\n2023-01-04,Producto A,20,100.00\n2023-01-05,Producto B,7,75.00\n2023-01-05,Producto C,4,50.00\n</code></pre> <p>Estos datos representan ventas diarias de tres productos diferentes durante cinco d\u00edas. Cada fila contiene la fecha de la venta, el producto vendido, la cantidad y el precio unitario.</p> <p>Puedes cargar estos datos en tu bucket de Amazon S3 en la carpeta <code>datos_brutos</code> y utilizar AWS Glue para realizar la transformaci\u00f3n y guardar los datos transformados en la carpeta <code>datos_transformados</code> en formato Parquet.</p> <p>Recuerda que estos datos son solo un ejemplo y puedes adaptarlos seg\u00fan tus necesidades y el escenario de tu proyecto de ETL en AWS.</p>"},{"location":"SBD/Tema01/IamAWS/","title":"Documentaci\u00f3n: AWS Identity and Access Management (IAM)","text":""},{"location":"SBD/Tema01/IamAWS/#introduccion-a-iam","title":"Introducci\u00f3n a IAM","text":"<p>AWS Identity and Access Management (IAM) es un servicio que te permite controlar el acceso a los recursos y servicios de AWS de forma segura. IAM permite administrar identidades, como usuarios y roles, y asignar permisos para que los usuarios y aplicaciones puedan acceder y realizar acciones en los recursos de AWS. IAM es fundamental para garantizar la seguridad y el cumplimiento en tu entorno de AWS.</p>"},{"location":"SBD/Tema01/IamAWS/#conceptos-clave-de-iam","title":"Conceptos Clave de IAM","text":""},{"location":"SBD/Tema01/IamAWS/#1-usuarios","title":"1. Usuarios","text":"<ul> <li>Usuarios IAM: Representan individuos o aplicaciones que necesitan acceso a recursos de AWS. Cada usuario tiene credenciales \u00fanicas y se le asignan permisos espec\u00edficos.</li> </ul>"},{"location":"SBD/Tema01/IamAWS/#2-grupos","title":"2. Grupos","text":"<ul> <li>Grupos IAM: Los grupos son conjuntos l\u00f3gicos de usuarios. Puedes asignar permisos a un grupo en lugar de asignarlos individualmente a cada usuario. Esto facilita la gesti\u00f3n de permisos.</li> </ul>"},{"location":"SBD/Tema01/IamAWS/#3-roles","title":"3. Roles","text":"<ul> <li>Roles IAM: Los roles son identidades temporales que pueden asumir usuarios, servicios o recursos en AWS. Los roles son \u00fatiles para aplicaciones que se ejecutan en instancias EC2, servicios de Lambda y m\u00e1s.</li> </ul>"},{"location":"SBD/Tema01/IamAWS/#4-politicas","title":"4. Pol\u00edticas","text":"<ul> <li>Pol\u00edticas IAM: Las pol\u00edticas son documentos que definen los permisos y las acciones permitidas o denegadas en recursos de AWS. Puedes adjuntar pol\u00edticas a usuarios, grupos y roles.</li> </ul>"},{"location":"SBD/Tema01/IamAWS/#configuracion-inicial-de-iam","title":"Configuraci\u00f3n Inicial de IAM","text":""},{"location":"SBD/Tema01/IamAWS/#1-acceso-a-la-consola-de-iam","title":"1. Acceso a la Consola de IAM","text":"<p>Para configurar IAM, inicia sesi\u00f3n en la Consola de AWS y navega a la secci\u00f3n \"Security, Identity, &amp; Compliance\" y selecciona \"IAM\".</p>"},{"location":"SBD/Tema01/IamAWS/#2-creacion-de-usuarios","title":"2. Creaci\u00f3n de Usuarios","text":"<ul> <li>Crea usuarios IAM para las personas o aplicaciones que necesitan acceso a AWS.</li> <li>Asigna nombres de usuario y opciones de autenticaci\u00f3n, como contrase\u00f1as o claves de acceso.</li> </ul>"},{"location":"SBD/Tema01/IamAWS/#3-creacion-de-grupos","title":"3. Creaci\u00f3n de Grupos","text":"<ul> <li>Crea grupos IAM l\u00f3gicos y asigna permisos a esos grupos.</li> <li>Agrega usuarios a grupos para asignarles permisos de manera eficiente.</li> </ul>"},{"location":"SBD/Tema01/IamAWS/#4-creacion-de-roles","title":"4. Creaci\u00f3n de Roles","text":"<ul> <li>Crea roles IAM para aplicaciones, servicios o recursos espec\u00edficos que necesitan acceder a otros servicios de AWS.</li> <li>Define las pol\u00edticas de permisos que se asocian con el rol.</li> </ul>"},{"location":"SBD/Tema01/IamAWS/#5-creacion-de-politicas","title":"5. Creaci\u00f3n de Pol\u00edticas","text":"<ul> <li>Define pol\u00edticas IAM que establezcan las acciones permitidas o denegadas en recursos de AWS.</li> <li>Adjunta pol\u00edticas a usuarios, grupos o roles seg\u00fan sea necesario.</li> </ul>"},{"location":"SBD/Tema01/IamAWS/#ejemplos-de-uso-de-iam","title":"Ejemplos de Uso de IAM","text":""},{"location":"SBD/Tema01/IamAWS/#1-control-de-acceso-a-s3","title":"1. Control de Acceso a S3","text":"<ul> <li>Utiliza IAM para definir pol\u00edticas que limiten el acceso de usuarios a un bucket de Amazon S3 espec\u00edfico.</li> </ul>"},{"location":"SBD/Tema01/IamAWS/#2-autenticacion-multifactor-mfa","title":"2. Autenticaci\u00f3n Multifactor (MFA)","text":"<ul> <li>Habilita la autenticaci\u00f3n multifactor (MFA) para usuarios IAM para una capa adicional de seguridad.</li> </ul>"},{"location":"SBD/Tema01/IamAWS/#3-uso-de-roles-para-aplicaciones","title":"3. Uso de Roles para Aplicaciones","text":"<ul> <li>Crea un rol IAM que una aplicaci\u00f3n EC2 puede asumir para acceder a otros servicios de AWS, como bases de datos.</li> </ul>"},{"location":"SBD/Tema01/IamAWS/#recomendaciones-de-seguridad","title":"Recomendaciones de Seguridad","text":"<ul> <li>Principio de Menor Privilegio: Asigna permisos m\u00ednimos necesarios para realizar una tarea.</li> <li>Rotaci\u00f3n de Credenciales: Cambia las contrase\u00f1as y las claves de acceso peri\u00f3dicamente.</li> <li>Auditor\u00eda: Habilita el registro de auditor\u00eda en IAM para supervisar eventos y actividades.</li> </ul>"},{"location":"SBD/Tema01/IamAWS/#ayuda-y-documentacion","title":"Ayuda y Documentaci\u00f3n","text":"<p>La Consola de IAM proporciona una amplia documentaci\u00f3n y ayuda en l\u00ednea. Adem\u00e1s, puedes consultar la documentaci\u00f3n oficial de AWS IAM para obtener informaci\u00f3n detallada sobre pol\u00edticas, roles y pr\u00e1cticas recomendadas.</p>"},{"location":"SBD/Tema01/IamAWS/#conclusion","title":"Conclusion","text":"<p>IAM es un servicio esencial para garantizar la seguridad y el control de acceso en tu entorno de AWS. Al comprender los conceptos clave de IAM y seguir las mejores pr\u00e1cticas de seguridad, puedes proteger tus recursos y datos de manera efectiva en la nube de AWS.</p>"},{"location":"SBD/Tema01/InfraestructuraAWS/","title":"Documentaci\u00f3n: Infraestructura de AWS","text":""},{"location":"SBD/Tema01/InfraestructuraAWS/#introduccion-a-la-infraestructura-de-aws","title":"Introducci\u00f3n a la Infraestructura de AWS","text":"<p>AWS es una plataforma de servicios en la nube que ofrece una amplia gama de servicios de c\u00f3mputo, almacenamiento, bases de datos, redes, an\u00e1lisis y m\u00e1s. La infraestructura de AWS est\u00e1 dise\u00f1ada para proporcionar una base s\u00f3lida para la implementaci\u00f3n de aplicaciones y servicios en la nube.</p>"},{"location":"SBD/Tema01/InfraestructuraAWS/#regiones-y-zonas-de-disponibilidad","title":"Regiones y Zonas de Disponibilidad","text":""},{"location":"SBD/Tema01/InfraestructuraAWS/#regiones","title":"Regiones","text":"<p>AWS opera en m\u00faltiples regiones en todo el mundo. Cada regi\u00f3n es una ubicaci\u00f3n geogr\u00e1fica que contiene uno o m\u00e1s centros de datos llamados \"Zonas de Disponibilidad\". Ejemplos de regiones incluyen \"us-east-1\" (Norte de Virginia), \"eu-west-1\" (Irlanda), entre otras.</p>"},{"location":"SBD/Tema01/InfraestructuraAWS/#zonas-de-disponibilidad","title":"Zonas de Disponibilidad","text":"<p>Cada regi\u00f3n de AWS consta de al menos dos Zonas de Disponibilidad (AZ). Las Zonas de Disponibilidad son centros de datos separados f\u00edsicamente dentro de una regi\u00f3n y est\u00e1n dise\u00f1adas para ser independientes entre s\u00ed en t\u00e9rminos de energ\u00eda y conectividad. Esto proporciona alta disponibilidad y redundancia.</p>"},{"location":"SBD/Tema01/InfraestructuraAWS/#servicios-clave-de-aws","title":"Servicios Clave de AWS","text":""},{"location":"SBD/Tema01/InfraestructuraAWS/#amazon-ec2-elastic-compute-cloud","title":"Amazon EC2 (Elastic Compute Cloud)","text":"<p>Amazon EC2 es un servicio de c\u00f3mputo escalable que te permite lanzar y administrar m\u00e1quinas virtuales (instancias) en la nube. Puedes seleccionar el tipo de instancia, el sistema operativo y la capacidad de c\u00f3mputo que necesitas.</p>"},{"location":"SBD/Tema01/InfraestructuraAWS/#amazon-s3-simple-storage-service","title":"Amazon S3 (Simple Storage Service)","text":"<p>Amazon S3 es un servicio de almacenamiento en la nube que permite almacenar y recuperar datos en forma de objetos. Es altamente escalable, duradero y se utiliza para almacenar una variedad de tipos de datos, incluidas im\u00e1genes, videos, archivos de registro y m\u00e1s.</p>"},{"location":"SBD/Tema01/InfraestructuraAWS/#amazon-rds-relational-database-service","title":"Amazon RDS (Relational Database Service)","text":"<p>Amazon RDS es un servicio de base de datos relacional gestionado que facilita la configuraci\u00f3n, administraci\u00f3n y escalabilidad de bases de datos. Admite motores de bases de datos como MySQL, PostgreSQL, SQL Server y otros.</p>"},{"location":"SBD/Tema01/InfraestructuraAWS/#aws-lambda","title":"AWS Lambda","text":"<p>AWS Lambda es un servicio de c\u00f3mputo sin servidor que te permite ejecutar c\u00f3digo en respuesta a eventos sin necesidad de administrar servidores. Es ideal para la ejecuci\u00f3n de funciones peque\u00f1as y ef\u00edmeras.</p>"},{"location":"SBD/Tema01/InfraestructuraAWS/#amazon-vpc-virtual-private-cloud","title":"Amazon VPC (Virtual Private Cloud)","text":"<p>Amazon VPC te permite crear una red virtual aislada en la nube donde puedes lanzar recursos de AWS de manera segura. Puedes definir tu propia topolog\u00eda de red, subredes, tablas de rutas y reglas de seguridad.</p>"},{"location":"SBD/Tema01/InfraestructuraAWS/#aws-iam-identity-and-access-management","title":"AWS IAM (Identity and Access Management)","text":"<p>AWS IAM es un servicio de administraci\u00f3n de identidades que te permite controlar el acceso a tus recursos de AWS. Puedes crear usuarios, grupos y roles, y definir pol\u00edticas de acceso.</p>"},{"location":"SBD/Tema01/InfraestructuraAWS/#arquitecturas-en-aws","title":"Arquitecturas en AWS","text":""},{"location":"SBD/Tema01/InfraestructuraAWS/#arquitectura-de-alta-disponibilidad-ha","title":"Arquitectura de Alta Disponibilidad (HA)","text":"<p>Las arquitecturas de alta disponibilidad en AWS se basan en la redundancia y la distribuci\u00f3n geogr\u00e1fica de recursos para garantizar que las aplicaciones est\u00e9n siempre disponibles incluso en caso de fallos.</p>"},{"location":"SBD/Tema01/InfraestructuraAWS/#escalabilidad-automatica","title":"Escalabilidad Autom\u00e1tica","text":"<p>La escalabilidad autom\u00e1tica permite que los recursos de AWS se ajusten autom\u00e1ticamente en funci\u00f3n de la demanda. Puedes escalar horizontalmente agregando instancias o verticalmente mejorando las instancias existentes.</p>"},{"location":"SBD/Tema01/InfraestructuraAWS/#arquitectura-sin-servidor-serverless","title":"Arquitectura sin Servidor (Serverless)","text":"<p>Las arquitecturas sin servidor aprovechan servicios como AWS Lambda y Amazon API Gateway para eliminar la administraci\u00f3n de servidores y ejecutar c\u00f3digo de manera ef\u00edmera en respuesta a eventos.</p>"},{"location":"SBD/Tema01/InfraestructuraAWS/#conclusion","title":"Conclusion","text":"<p>La infraestructura de AWS proporciona una base s\u00f3lida y escalable para implementar aplicaciones y servicios en la nube. Con una amplia gama de servicios y opciones de arquitectura, AWS es una plataforma vers\u00e1til para satisfacer diversas necesidades empresariales. Explora los servicios de AWS y considera c\u00f3mo pueden adaptarse a tus proyectos y aplicaciones espec\u00edficos.</p>"},{"location":"SBD/Tema01/PracticasAWS/","title":"12. Practicas AWS","text":""},{"location":"SBD/Tema01/PracticasAWS/#121-practica-etl-en-aws","title":"12.1. Practica ETL en AWS","text":"<ul> <li>En un bucket S3 (datos_etl) crear dos directorios</li> <li>ciclistas</li> <li>ciclistas_procesados</li> <li>Con AWS Glue</li> <li>Utilizando el Data Catalog, crear:<ul> <li>Base de datos (datos_ciclistas)</li> <li>Tabla (ciclistas)</li> </ul> </li> <li>Crear un job (Lab Role) S3 Originales --&gt; Eliminar Duplicados --&gt; Filtrar filas --&gt; S3 Procesados</li> <li>RedShift (Lab Role)</li> <li>Crear cluster admin Qwe_1234</li> <li>Crear Base de Datos</li> <li>Crear Tabla y Definir campos</li> <li>Cargar datos en la Tabla (IGNOREHEADER AS 1)</li> <li>Crear una funci\u00f3n Lamda que haga de trigger (lanzador) del Job</li> </ul>"},{"location":"SBD/Tema01/PracticasAWS/#122-etl-con-jobs-en-aws-glue","title":"12.2. ETL con jobs en AWS Glue","text":"<p>Servicios AWS: - AWS S3 - AWS Glue Studio - AWS Glue Data Catalog - AWS Glue Job - AWS RedShift - AWS Lambda</p> <p>La pr\u00e1ctica consiste en hacer una ETL con AWS Glue utilizando un Job Visual. Los datos de partida son un fichero en formato csv con datos de ciclistas. Cargaremos los datos en un bucket S3 que tendr\u00e1 dos carpetas, una para los datos de entrada y otra para los datos procesados. Crearemos un job que elimine los duplicados y haga un filtro y seleccione solo las filas con el campo Severity = \"Grave\". Luego cargaremos los datos en un cluster RedShift y finalmente prepararemos una funci\u00f3n lambda que lance el job. Revisaremos c\u00f3mo se monitorizan los jobs y las funciones lambda.</p> <p>Los campos del fichero son:</p> <p>Pasos a seguir: - En un bucket S3 (datos_etl) crear dos directorios   - ciclistas   - ciclistas_procesados - Con AWS Glue   - Utilizando el Data Catalog, crear:     - Base de datos (datos_ciclistas)     - Tabla (ciclistas)   - Crear un job (Lab Role) S3 Originales --&gt; Eliminar Duplicados --&gt; Filtrar filas --&gt; S3 Procesados - RedShift (Lab Role)   - Crear cluster admin Qwe_1234   - Crear Base de Datos   - Crear Tabla y Definir campos   - Cargar datos en la Tabla (IGNOREHEADER AS 1) - Crear una funci\u00f3n Lambda que haga de trigger (lanzador) del Job</p>"},{"location":"SBD/Tema01/PracticasAWS/#123-etl-con-pyspark","title":"12.3. ETL con PySpark","text":"<p>Servicios AWS: - AWS S3 - AWS Glue Studio - AWS Glue Data Catalog - AWS Glue Job - AWS Glue Crawler - AWS Glue Workflow</p> <p>La pr\u00e1ctica consiste en hacer una ETL con AWS Glue utilizando c\u00f3digo PySpark. Los datos de partida son dos ficheros en formato csv con datos de clientes y ventas. Cargaremos los datos en un bucket S3 que tambi\u00e9n tendr\u00e1 una carpeta para el script de PySpark y otra para los resultados de salida. Con estos datos queremos un fichero con las ventas totales por cliente en formato JSON.</p> <p>Los campos de los ficheros son: - customers: {CUSTOMERID, CUSTOMERNAME, EMAIL, CITY, COUNTRY, TERRITORY, CONTACTFIRSTNAME, CONTACTLASTNAME} - sales: {ORDERNUMBER, QUANTITYORDERED, PRICEEACH, ORDERLINENUMBER, SALES, ORDERDATE, STATUS, QTR_ID, MONTH_ID, YEAR_ID, PRODUCTLINE, MSRP, PRODUCTCODE, DEALSIZE, CUSTOMERID}</p> <p>Pasos a seguir: - Crear un bucket (lago de datos) S3 con la siguiente estructura de carpetas y ficheros:   - datos     - clientes       - customers.csv     - ventas       - sales.csv   - Crear un crawler con AWS Glue que rastree la carpeta datos y los introduzca en una BD (ventas)   - Crear un job con Glue de Spark Script Editor     - El script debe:       - Guardarse en la carpeta scripts del bucket       - Debe seleccionar las ventas totales por cliente y guardarlas en un fichero en formato JSON en la carpeta de salida   - Crear un flujo de trabajo que lo haga todo.     - A\u00f1adir Workflow     - A\u00f1adir Trigger       - Inicio Rastreador     - A\u00f1adir Nodo       - Seleccionar Crawler     - A\u00f1adir Trigger       - Type: Evento       - Start after ANY watched event     - A\u00f1adir Job/Crawler       - Seleccionar Crawler       - Evento: \u00c9xito</p> <p>Ejemplo de c\u00f3digo:</p> <pre><code>import sys\nfrom datetime import datetime\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\nspark = SparkSession\\\n    .builder\\\n    .appName(\"SparkETL\")\\\n    .getOrCreate()\n\nspark.catalog.setCurrentDatabase(\"ventas\")\ndf = spark.sql(\"select * from clientes\")\n# df.show()\n\ndf = df.select(\"customername\", \"email\")\n# df.show()\n\ndf.write.format(\"json\").mode(\"overwrite\").save(\"s3://lagodatos/salida/\")\n</code></pre>"},{"location":"SBD/Tema01/PracticasAWS/#124-streaming-con-kinesis-data-streams-y-kinesis-data-firehose","title":"12.4. Streaming con Kinesis Data Streams y Kinesis Data Firehose","text":"<p>Servicios AWS: - AWS S3 - AWS Glue Data Catalog - AWS Cloud9 - AWS Kinesis Data Streams - AWS Kinesis Data Firehose</p> <p>La pr\u00e1ctica consiste en crear un escenario en el que Amazon Kinesis Delivery Stream convierte los datos de origen con formato JSON en datos de destino con formato Apache Parquet mediante el esquema de tabla de cat\u00e1logo de Glue y los almacena en un S3.</p> <p>Pasos a seguir: - Crear un bucket S3 - Crear una base de datos y una tabla (Glue Data Catalog) con el siguiente esquema:   - firstname: string   - lastname: string   - age: int - Configurar el data stream (Kinesis Data Streams) - Crear el delivery stream (Kinesis Firehose) - Configurar el entorno de desarrollo - Crear el c\u00f3digo Python que hace la funci\u00f3n de productor</p> <pre><code>import boto3\nimport random\nimport time\n\nclient = boto3.client('kinesis')\n\npartitionkey = random.randint(10, 100);\n\nfor a in range(1, 10, 1):\n    edad = random.randint(1, 90);\n    mydata = '{ \"firstname\": \"John\", \"lastname\": \"Smith\", \"age\": ' + str(edad) + ' }'\n    print(mydata)\n\n    response = client.put_record(StreamName=\"flujo_luis\", Data=mydata, PartitionKey=str(partitionkey))\n    print(response)\n    time.sleep(\n\n5)\n</code></pre>"},{"location":"SBD/Tema01/PracticasAWS/#125-streaming-con-kinesis-data-firehose-y-kinesis-data-analytics","title":"12.5. Streaming con Kinesis Data Firehose y Kinesis Data Analytics","text":"<p>Servicios AWS: - AWS S3 - AWS Kinesis Data Firehose - AWS Kinesis Data Analytics</p> <p>La pr\u00e1ctica consiste en crear un flujo de datos de bolsa con Amazon Kinesis Delivery Stream y con Kinesis Data Analytics guardarlos en un Bucket S3 tanto en crudo tal cual llegan, como ligeramente procesados. Para ello, utilizaremos 4 carpetas en el bucket, dos para los datos limpios (\u00e9xito y errores) y dos para los datos procesados (\u00e9xito y errores): - datos_brutos - errores_datos_brutos - datos_limpios - errores_datos_limpios</p> <p>Vamos a necesitar tambi\u00e9n dos flujos de Kinesis Firehose.</p> <p>Pasos a seguir:</p> <ul> <li>Crear el primer flujo (Firehose) con una entrada directa de datos (DIRECT PUT). Le asignamos las carpetas para el destino que se crear\u00e1n autom\u00e1ticamente.</li> <li>Datos del flujo: 1 Mb o 60 seg</li> <li>Rol: LabRole</li> <li>Realizar el test del Data Stream Delivery</li> <li>Comprobar que se crean los datos en bruto en la carpeta correspondiente</li> <li>Limpieza con Kinesis Data Analytics (con aplicaciones SQL)</li> <li>Crear aplicaci\u00f3n</li> <li>Descubrir el esquema. Confirmar que se est\u00e1n generando los datos de prueba</li> <li>Configurar la plantilla. Funci\u00f3n de agregaci\u00f3n en una ventana deslizante de tiempo</li> <li>Probar y ejecutar</li> <li>Crear el segundo flujo de destino</li> <li>Ejecutar</li> </ul> <p>Ayuda: Enlace a video de ayuda</p> <p>Ejemplos de An\u00e1lisis de Flujo Continuo:</p> <pre><code>-- Continuous Filter\n-- Performs a continuous filter based on a WHERE condition.\n-- .----------.   .----------.   .----------.\n-- |  SOURCE  |   |  INSERT  |   |  DESTIN. |\n-- Source--&gt;|  STREAM  |--&gt;| &amp; SELECT |--&gt;|  STREAM |--&gt;Destination\n-- |          |   |  (PUMP)  |   |          |\n-- '----------'   '----------'   '----------'\n-- STREAM (in-application): a continuously updated entity that you can SELECT from and INSERT into like a TABLE\n-- PUMP: an entity used to continuously 'SELECT ... FROM' a source STREAM, and INSERT SQL results into an output STREAM\n-- Create output stream, which can be used to send to a destination\nCREATE OR REPLACE STREAM \"DESTINATION_SQL_STREAM\" (ticker_symbol VARCHAR(4), sector VARCHAR(12), change REAL, price REAL);\n-- Create pump to insert into output\nCREATE OR REPLACE PUMP \"STREAM_PUMP\" AS INSERT INTO \"DESTINATION_SQL_STREAM\"\n-- Select all columns from source stream\nSELECT STREAM ticker_symbol, sector, change, price\nFROM \"SOURCE_SQL_STREAM_001\"\n-- LIKE compares a string to a string pattern (_ matches all char, % matches substring)\n-- SIMILAR TO compares string to a regex, may use ESCAPE\nWHERE sector SIMILAR TO '%TECH%';\n</code></pre> <pre><code>CREATE OR REPLACE STREAM \"DESTINATION_SQL_STREAM\" (\"eventType\" VARCHAR(16), \"ses_timestamp\" timestamp, \"messageId\" VARCHAR(64), \"ses_to\" VARCHAR(64), \"ses_configuration_set\" VARCHAR(32));\n\nCREATE OR REPLACE PUMP \"STREAM_PUMP\" AS INSERT INTO \"DESTINATION_SQL_STREAM\"\n\nSELECT STREAM \"eventType\", \"ses_timestamp\", \"messageId\", \"ses_to\", \"ses_configuration_set\"\nFROM \"SOURCE_SQL_STREAM_001\"\nWHERE \"eventType\" = 'Send'\n</code></pre>"},{"location":"SBD/Tema01/PracticasAWS/#126-visualizacion-de-datos-con-quicksight","title":"12.6. Visualizaci\u00f3n de datos con QuickSight","text":"<p>Para todos los ejercicios pr\u00e1cticos, utilizaremos el archivo de datos SaaS-Sales.csv proporcionado aqu\u00ed.</p> <p>Este conjunto de datos representa datos de ventas de una empresa ficticia de SaaS (Software como Servicio) que vende software de ventas y marketing a otras empresas (B2B). Cada fila de datos es una transacci\u00f3n/pedido.</p> <p>Servicios AWS: - AWS QuickSight</p> <p>Pasos a seguir: - Acceder a AWS QuickSight - Registrarse (gratis 30 d\u00edas) - Crear un nuevo an\u00e1lisis - Cargar un dataset - Empezar a a\u00f1adir visualizaciones - Publicar el informe</p> <p>Ayuda: Enlace al tutorial de ayuda</p>"},{"location":"SBD/Tema01/RedShiftAWS/","title":"Documentaci\u00f3n: AWS Redshift - Almacenamiento de Datos y An\u00e1lisis","text":"<p>Amazon Redshift es un servicio de almac\u00e9n de datos en la nube totalmente administrado y escalable. Permite a las organizaciones almacenar grandes vol\u00famenes de datos y realizar an\u00e1lisis complejos de manera eficiente. A continuaci\u00f3n, encontrar\u00e1s una gu\u00eda detallada sobre AWS Redshift, junto con enlaces a la documentaci\u00f3n oficial de AWS para obtener informaci\u00f3n adicional.</p>"},{"location":"SBD/Tema01/RedShiftAWS/#introduccion-a-amazon-redshift","title":"Introducci\u00f3n a Amazon Redshift","text":""},{"location":"SBD/Tema01/RedShiftAWS/#que-es-amazon-redshift","title":"\u00bfQu\u00e9 es Amazon Redshift?","text":"<p>Amazon Redshift es un servicio de almac\u00e9n de datos en la nube que permite a las organizaciones almacenar, consultar y analizar grandes conjuntos de datos de manera eficiente. Est\u00e1 dise\u00f1ado para admitir cargas de trabajo de an\u00e1lisis de datos y se basa en una arquitectura de almacenamiento columnar y distribuida.</p> <p>Documentaci\u00f3n Oficial de AWS sobre Amazon Redshift: Amazon Redshift - Documentaci\u00f3n de AWS</p>"},{"location":"SBD/Tema01/RedShiftAWS/#caracteristicas-clave-de-amazon-redshift","title":"Caracter\u00edsticas Clave de Amazon Redshift","text":""},{"location":"SBD/Tema01/RedShiftAWS/#1-almacenamiento-columnar","title":"1. Almacenamiento Columnar","text":"<ul> <li>Amazon Redshift almacena los datos en formato columnar, lo que permite un acceso m\u00e1s r\u00e1pido y eficiente a los datos relevantes para consultas.</li> </ul>"},{"location":"SBD/Tema01/RedShiftAWS/#2-escalabilidad","title":"2. Escalabilidad","text":"<ul> <li>Redshift es escalable, lo que significa que puedes aumentar o reducir la capacidad de almacenamiento y c\u00f3mputo seg\u00fan las necesidades de tu organizaci\u00f3n.</li> </ul>"},{"location":"SBD/Tema01/RedShiftAWS/#3-integracion-con-herramientas-de-analisis","title":"3. Integraci\u00f3n con Herramientas de An\u00e1lisis","text":"<ul> <li>Redshift es compatible con una variedad de herramientas de an\u00e1lisis y visualizaci\u00f3n, como Tableau, Power BI y Quicksight.</li> </ul>"},{"location":"SBD/Tema01/RedShiftAWS/#4-seguridad-y-cumplimiento","title":"4. Seguridad y Cumplimiento","text":"<ul> <li>Ofrece capacidades de seguridad avanzadas, incluyendo encriptaci\u00f3n, autenticaci\u00f3n basada en IAM y auditor\u00eda de consultas.</li> </ul>"},{"location":"SBD/Tema01/RedShiftAWS/#configuracion-y-uso-basico-de-amazon-redshift","title":"Configuraci\u00f3n y Uso B\u00e1sico de Amazon Redshift","text":""},{"location":"SBD/Tema01/RedShiftAWS/#1-creacion-de-un-cluster","title":"1. Creaci\u00f3n de un Cluster","text":"<ul> <li>Inicia sesi\u00f3n en la consola de AWS y crea un cluster de Amazon Redshift. Configura el tama\u00f1o del cluster y otros par\u00e1metros seg\u00fan tus necesidades.</li> </ul> <p>Documentaci\u00f3n Relacionada: Creaci\u00f3n de un Cluster de Amazon Redshift - AWS</p>"},{"location":"SBD/Tema01/RedShiftAWS/#2-configuracion-de-conexiones","title":"2. Configuraci\u00f3n de Conexiones","text":"<ul> <li>Configura las conexiones a tu cluster Redshift desde aplicaciones y herramientas de an\u00e1lisis. Obt\u00e9n la cadena de conexi\u00f3n y las credenciales necesarias.</li> </ul> <p>Documentaci\u00f3n Relacionada: Configuraci\u00f3n de Conexiones de Amazon Redshift - AWS</p>"},{"location":"SBD/Tema01/RedShiftAWS/#3-creacion-de-tablas-y-cargas-de-datos","title":"3. Creaci\u00f3n de Tablas y Cargas de Datos","text":"<ul> <li>Utiliza SQL para crear tablas en tu cluster Redshift y carga datos desde fuentes externas, como archivos CSV o desde otros sistemas de almacenamiento.</li> </ul> <p>Documentaci\u00f3n Relacionada: Creaci\u00f3n de Tablas y Carga de Datos en Redshift - AWS</p>"},{"location":"SBD/Tema01/RedShiftAWS/#4-consultas-y-analisis","title":"4. Consultas y An\u00e1lisis","text":"<ul> <li>Utiliza herramientas de an\u00e1lisis y consulta SQL para explorar y analizar tus datos almacenados en Redshift.</li> </ul> <p>Documentaci\u00f3n Relacionada: Consultas y An\u00e1lisis en Amazon Redshift - AWS</p>"},{"location":"SBD/Tema01/RedShiftAWS/#5-seguridad","title":"5. Seguridad","text":"<ul> <li>Configura la seguridad en Redshift mediante la gesti\u00f3n de grupos de seguridad, la encriptaci\u00f3n de datos y la autenticaci\u00f3n IAM.</li> </ul> <p>Documentaci\u00f3n Relacionada: Seguridad en Amazon Redshift - AWS</p>"},{"location":"SBD/Tema01/RedShiftAWS/#recomendaciones-de-mejores-practicas","title":"Recomendaciones de Mejores Pr\u00e1cticas","text":"<ul> <li> <p>Realiza copias de seguridad y mant\u00e9n una estrategia de recuperaci\u00f3n ante desastres.</p> </li> <li> <p>Monitorea el rendimiento del cluster y ajusta la capacidad de acuerdo a las cargas de trabajo.</p> </li> <li> <p>Utiliza anal\u00edtica y visualizaci\u00f3n de datos para obtener informaci\u00f3n valiosa de tus datos almacenados en Redshift.</p> </li> </ul>"},{"location":"SBD/Tema01/RedShiftAWS/#ejemplo-practico-de-uso-de-amazon-redshift","title":"Ejemplo Pr\u00e1ctico de Uso de Amazon Redshift","text":"<p>A continuaci\u00f3n, un ejemplo simplificado de c\u00f3mo utilizar Amazon Redshift:</p> <ol> <li> <p>Crear un cluster de Redshift en la consola de AWS.</p> </li> <li> <p>Utilizar una herramienta de administraci\u00f3n SQL o una aplicaci\u00f3n de an\u00e1lisis para conectarse al cluster y crear una base de datos.</p> </li> <li> <p>Crear tablas en la base de datos y cargar datos desde un archivo CSV.</p> </li> <li> <p>Ejecutar consultas SQL para analizar los datos y obtener informaci\u00f3n valiosa.</p> </li> </ol> <p>Este es un escenario b\u00e1sico, y Redshift es altamente configurable y escalable seg\u00fan las necesidades de tu organizaci\u00f3n.</p>"},{"location":"SBD/Tema01/RedShiftAWS/#ayuda-y-documentacion-adicional","title":"Ayuda y Documentaci\u00f3n Adicional","text":"<p>Para obtener m\u00e1s detalles sobre Amazon Redshift y c\u00f3mo utilizarlo, consulta la documentaci\u00f3n oficial de AWS sobre Amazon Redshift. Adem\u00e1s, AWS ofrece tutoriales y ejemplos de implementaci\u00f3n para ayudarte en tu proyecto de an\u00e1lisis de datos con Redshift.</p> <p>Puedes encontrar ejemplos pr\u00e1cticos de c\u00f3mo utilizar Amazon Redshift en la documentaci\u00f3n oficial de AWS, as\u00ed como en tutoriales y recursos en l\u00ednea. Aqu\u00ed te proporciono un ejemplo pr\u00e1ctico sencillo para que puedas comenzar:</p> <p>Escenario: Supongamos que tienes un conjunto de datos de ventas en formato CSV y deseas utilizar Amazon Redshift para crear un almac\u00e9n de datos y realizar an\u00e1lisis b\u00e1sicos.</p> <p>Pasos para un Ejemplo Pr\u00e1ctico:</p> <ol> <li>Crear un Cluster Redshift:</li> <li>Inicia sesi\u00f3n en la consola de AWS.</li> <li>Navega a Amazon Redshift y crea un nuevo cluster.</li> <li> <p>Configura el tama\u00f1o del cluster, las opciones de seguridad y otras configuraciones seg\u00fan tus necesidades.</p> </li> <li> <p>Conectar a tu Cluster:</p> </li> <li>Utiliza una herramienta de SQL como SQL Workbench/J o una aplicaci\u00f3n de an\u00e1lisis compatible con Redshift para conectarte a tu cluster.</li> <li> <p>Proporciona la cadena de conexi\u00f3n y las credenciales del cluster.</p> </li> <li> <p>Crear una Base de Datos:</p> </li> <li>Utiliza SQL para crear una base de datos en tu cluster.</li> <li> <p>Por ejemplo:    <pre><code>CREATE DATABASE ventas;\n</code></pre></p> </li> <li> <p>Crear Tablas y Cargar Datos:</p> </li> <li>Crea tablas en la base de datos para almacenar los datos de ventas.</li> <li>Carga los datos desde tus archivos CSV en las tablas.</li> <li> <p>Por ejemplo:    <pre><code>CREATE TABLE ventas (\nid INT,\nfecha DATE,\nproducto VARCHAR(255),\ncantidad INT,\nprecio DECIMAL(10, 2)\n);\n\nCOPY ventas FROM 's3://tu-bucket/tu-archivo.csv'\nCREDENTIALS 'aws_access_key_id=TU_ACCESS_KEY;aws_secret_access_key=TU_SECRET_KEY'\nDELIMITER ','\nCSV;\n</code></pre></p> </li> <li> <p>Consultas y An\u00e1lisis:</p> </li> <li>Ejecuta consultas SQL para analizar tus datos.</li> <li> <p>Por ejemplo, puedes calcular el total de ventas por producto:    <pre><code>SELECT producto, SUM(precio * cantidad) AS total_ventas\nFROM ventas\nGROUP BY producto;\n</code></pre></p> </li> <li> <p>Gesti\u00f3n de Seguridad:</p> </li> <li>Configura la seguridad de tu cluster Redshift para garantizar que solo usuarios autorizados puedan acceder a los datos.</li> </ol> <p>Este es un ejemplo muy simplificado, pero te da una idea de c\u00f3mo puedes utilizar Amazon Redshift para crear un almac\u00e9n de datos y realizar an\u00e1lisis b\u00e1sicos. Para escenarios m\u00e1s complejos y ejemplos detallados, te recomiendo consultar los recursos adicionales proporcionados en la documentaci\u00f3n de AWS y en tutoriales en l\u00ednea. Adem\u00e1s, puedes explorar ejemplos y proyectos de c\u00f3digo en plataformas de desarrollo como GitHub, donde encontrar\u00e1s casos de uso m\u00e1s avanzados de Amazon Redshift.</p>"},{"location":"SBD/Tema01/VpcAWS/","title":"Documentaci\u00f3n: Amazon Virtual Private Cloud (Amazon VPC)","text":""},{"location":"SBD/Tema01/VpcAWS/#introduccion-a-amazon-vpc","title":"Introducci\u00f3n a Amazon VPC","text":"<p>Amazon Virtual Private Cloud (Amazon VPC) es un servicio de red que te permite crear una red virtual aislada en la nube de AWS. Con Amazon VPC, puedes controlar la topolog\u00eda de red, asignar direcciones IP, definir tablas de rutas y aplicar reglas de seguridad para tus recursos en la nube. VPC te permite crear una extensi\u00f3n virtual de tu centro de datos en AWS.</p>"},{"location":"SBD/Tema01/VpcAWS/#conceptos-clave-de-amazon-vpc","title":"Conceptos Clave de Amazon VPC","text":""},{"location":"SBD/Tema01/VpcAWS/#1-red-virtual-vpc","title":"1. Red Virtual (VPC)","text":"<ul> <li>VPC: Una VPC es tu propia red virtual en la nube de AWS. Puedes personalizar su topolog\u00eda, direcciones IP y configuraci\u00f3n de red.</li> </ul>"},{"location":"SBD/Tema01/VpcAWS/#2-subredes","title":"2. Subredes","text":"<ul> <li>Subredes: Una VPC se divide en subredes. Puedes crear subredes p\u00fablicas y privadas seg\u00fan tus necesidades.</li> </ul>"},{"location":"SBD/Tema01/VpcAWS/#3-tablas-de-rutas","title":"3. Tablas de Rutas","text":"<ul> <li>Tablas de Rutas: Las tablas de rutas definen c\u00f3mo se enrutan los paquetes dentro y fuera de la VPC. Puedes personalizar las rutas y las reglas.</li> </ul>"},{"location":"SBD/Tema01/VpcAWS/#4-grupos-de-seguridad","title":"4. Grupos de Seguridad","text":"<ul> <li>Grupos de Seguridad: Los grupos de seguridad son cortafuegos virtuales que controlan el tr\u00e1fico de entrada y salida de las instancias EC2.</li> </ul>"},{"location":"SBD/Tema01/VpcAWS/#5-listas-de-control-de-acceso-a-la-red-nacl","title":"5. Listas de Control de Acceso a la Red (NACL)","text":"<ul> <li>NACL: Las NACL son conjuntos de reglas que controlan el tr\u00e1fico de subredes en la VPC.</li> </ul>"},{"location":"SBD/Tema01/VpcAWS/#6-vpn-y-direct-connect","title":"6. VPN y Direct Connect","text":"<ul> <li>VPN y Direct Connect: Amazon VPC te permite conectar tu red local a la VPC mediante una conexi\u00f3n VPN o Direct Connect para acceso privado.</li> </ul>"},{"location":"SBD/Tema01/VpcAWS/#configuracion-de-amazon-vpc","title":"Configuraci\u00f3n de Amazon VPC","text":""},{"location":"SBD/Tema01/VpcAWS/#1-creacion-de-una-vpc","title":"1. Creaci\u00f3n de una VPC","text":"<ul> <li>Define una VPC especificando su rango de direcciones IP y configuraci\u00f3n de red.</li> <li>Puedes seleccionar un rango de direcciones IP IPv4 CIDR para tu VPC.</li> </ul>"},{"location":"SBD/Tema01/VpcAWS/#2-creacion-de-subredes","title":"2. Creaci\u00f3n de Subredes","text":"<ul> <li>Divide tu VPC en subredes p\u00fablicas y privadas.</li> <li>Configura las tablas de rutas para permitir o denegar el tr\u00e1fico.</li> </ul>"},{"location":"SBD/Tema01/VpcAWS/#3-configuracion-de-grupos-de-seguridad","title":"3. Configuraci\u00f3n de Grupos de Seguridad","text":"<ul> <li>Define grupos de seguridad para controlar el tr\u00e1fico de red a instancias EC2.</li> <li>Asigna reglas de entrada y salida a los grupos de seguridad.</li> </ul>"},{"location":"SBD/Tema01/VpcAWS/#4-configuracion-de-nacl","title":"4. Configuraci\u00f3n de NACL","text":"<ul> <li>Configura listas de control de acceso a la red (NACL) para controlar el tr\u00e1fico de subredes.</li> <li>Define reglas de entrada y salida en las NACL.</li> </ul>"},{"location":"SBD/Tema01/VpcAWS/#escenarios-comunes-en-amazon-vpc","title":"Escenarios Comunes en Amazon VPC","text":""},{"location":"SBD/Tema01/VpcAWS/#1-vpc-con-subredes-publicas-y-privadas","title":"1. VPC con Subredes P\u00fablicas y Privadas","text":"<ul> <li>Crea una VPC con subredes p\u00fablicas que tienen acceso a Internet y subredes privadas que est\u00e1n aisladas.</li> </ul>"},{"location":"SBD/Tema01/VpcAWS/#2-conexion-a-una-red-local","title":"2. Conexi\u00f3n a una Red Local","text":"<ul> <li>Configura una VPN o Direct Connect para conectar tu red local a tu VPC en AWS.</li> </ul>"},{"location":"SBD/Tema01/VpcAWS/#3-despliegue-de-aplicaciones-en-vpc","title":"3. Despliegue de Aplicaciones en VPC","text":"<ul> <li>Implementa aplicaciones y servicios en tus subredes personalizadas dentro de la VPC.</li> </ul>"},{"location":"SBD/Tema01/VpcAWS/#recomendaciones-de-seguridad","title":"Recomendaciones de Seguridad","text":"<ul> <li>Seguridad de Grupos: Asigna grupos de seguridad para controlar el tr\u00e1fico de red a nivel de instancia.</li> <li>Aislamiento de Subredes: Dise\u00f1a subredes para aislar componentes cr\u00edticos.</li> <li>Monitoreo y Auditor\u00eda: Habilita el monitoreo y la auditor\u00eda de red para supervisar y proteger tu VPC.</li> </ul>"},{"location":"SBD/Tema01/VpcAWS/#ayuda-y-documentacion","title":"Ayuda y Documentaci\u00f3n","text":"<p>Amazon VPC ofrece documentaci\u00f3n detallada y ejemplos de configuraci\u00f3n en la documentaci\u00f3n oficial de AWS. Tambi\u00e9n puedes utilizar la Consola de AWS para configurar y administrar tu VPC de manera visual.</p>"},{"location":"SBD/Tema01/VpcAWS/#conclusion","title":"Conclusion","text":"<p>Amazon Virtual Private Cloud (Amazon VPC) es un servicio esencial para crear y administrar redes virtuales personalizadas en la nube de AWS. Con la capacidad de definir la topolog\u00eda de red, asignar direcciones IP y aplicar reglas de seguridad, VPC proporciona un control completo sobre la conectividad y la seguridad de tus recursos en AWS. Explora Amazon VPC para dise\u00f1ar y desplegar redes personalizadas en la nube.</p>"},{"location":"SOM/IndiceSOM/","title":"Sistemes Operatius Monolloc","text":""},{"location":"SOM/IndiceSOM/#temes","title":"Temes","text":"<ul> <li>T1 Sistemes Inform\u00e0tics </li> <li>T2 Sistemes Operatius </li> </ul>"},{"location":"SOM/Tema01/SistemesInformatics/","title":"1. Sistemes Inform\u00e0tics","text":""},{"location":"SOM/Tema01/SistemesInformatics/#11-que-es-un-ordinador","title":"1.1. \u00bfQu\u00e9 \u00e8s un ordinador?","text":"<p>En valenci\u00e0, la paraula ''ordinador'' t\u00e9 el seu origen en la paraula francesa ''ordinateur'' i fa refer\u00e8ncia a un dispositiu electr\u00f2nic, de prop\u00f2sit general, amb capacitat per a rebre informaci\u00f3, emmagatzemar-la durant un temps (almenys l'imprescindible per a dur a terme la seua tasca), processar-la per a rebre un resultat i oferir aqueix resultat a l'exterior.</p> <p>Per tant, podem dir que es tracta d'un dispositiu amb les seg\u00fcent funcions b\u00e0siques:</p> <ul> <li>Transmetre informaci\u00f3.</li> <li>Emmagatzemar informaci\u00f3.</li> <li>Processar informaci\u00f3.</li> </ul>"},{"location":"SOM/Tema01/SistemesInformatics/#12-definicions","title":"1.2. Definicions","text":"<ul> <li>Inform\u00e0tica: Ci\u00e8ncia que estudia el tractament autom\u00e0tic de les dades. Procedeix de la fusi\u00f3 de les paraules informaci\u00f3 i autom\u00e0tica.</li> <li>Ordinador: M\u00e0quina composta d'elements f\u00edsics (Maquinari) capa\u00e7 de realitzar una gran varietat de treballs a gran velocitat i amb gran precisi\u00f3.</li> <li>Maquinari (Hardware): Component f\u00edsics d'un ordinador.</li> <li>Programari (Software): Component no f\u00edsics de l'ordinador que posen en funcionament a aquests \u00faltims. S\u00f3n els programes que ens serveixen per a processar les dades.</li> <li>Microprogramari (Firmware): Part intangible (programari) dels components del maquinari. Programari amb el qual estan programades les mem\u00f2ries ROM</li> <li>Instrucci\u00f3: Ordre necess\u00e0ria perqu\u00e8 funcionen els components f\u00edsics.</li> <li>Programa: Conjunt d'instruccions.</li> <li>Aplicaci\u00f3: Conjunt de programes.</li> <li>Sistema Operatiu: Component Programari d'un Sistema Inform\u00e0tic capa\u00e7 de fer que els programes processen informaci\u00f3 (dades) sobre els components electr\u00f2nics d'un ordinador.</li> </ul>"},{"location":"SOM/Tema01/SistemesInformatics/#13-sistema-informatic","title":"1.3. Sistema Inform\u00e0tic","text":"<p>Sempre que parlem d'un sistema, ens referim a diferents elements que es relacionen entre si. En el cas d'un sistema inform\u00e0tic, aquests elements s\u00f3n tres:</p> <ul> <li>El maquinari: Que inclou qualsevol dispositiu electr\u00f2nic utilitzat en el proc\u00e9s de la informaci\u00f3.</li> <li>El programari: Que est\u00e0 format per qualsevol element l\u00f2gic involucrat en el proc\u00e9s.</li> <li>Els usuaris: S\u00f3n les persones que ho utilitzen. \u00c9s l'element en el qual menys es repara quan es parla d'un sistema inform\u00e0tic, per\u00f2 sense ell, la resta dels elements no tindrien sentit.</li> </ul> <p>Els elements f\u00edsics que formen part del sistema inform\u00e0tic reben el nom gen\u00e8ric de maquinari. Aquest concepte \u00e9s molt gen\u00e8ric i pot fer refer\u00e8ncia tant a components que es troben dins del moble de l'ordinador, com a uns altres que estan en l'exterior.</p> <p>La majoria continuen utilitzant, hui dia, el model d'arquitectura que va introduir el matem\u00e0tic hongar\u00e9s John Von Neumann en 1949. Segons aquest esquema, un ordinador pot representar-se d'una forma modular, amb aquests quatre elements:</p>"},{"location":"SOM/Tema01/SistemesInformatics/#14-components-fisics","title":"1.4 Components f\u00edsics","text":"<ul> <li>El processador, que dirigeix el funcionament de l'ordinador i processa les dades.</li> <li>La mem\u00f2ria principal, que emmagatzema les instruccions que executa el processador i les dades sobre els quals s'apliquen aquestes.</li> <li>Els dispositius d'entrada/eixida, que comuniquen a l'ordinador amb el seu entorn.</li> <li>Els busos, que actuen com a canal de comunicaci\u00f3 entre el processador, la mem\u00f2ria i els dispositius d'entrada/eixida.</li> </ul> <p>En definitiva, podr\u00edem representar l'esquema de funcionament de l'arquitectura Von Neumann amb aquesta imatge:</p> <p></p>"},{"location":"SOM/Tema01/SistemesInformatics/#15-cpu-processador","title":"1.5. CPU (Processador)","text":"<p>Ja hem dit abans que el processador \u00e9s una part fonamental del maquinari de l'ordinador, que s'encarrega de llegir de mem\u00f2ria les instruccions que ha d'executar, les interpreta i les executa.</p> <p>Encara que l'estructura interna de qualsevol processador actual \u00e9s extremadament complexa, a nivell l\u00f2gic podem dir que est\u00e0 formada pels seg\u00fcents elements:</p> <ul> <li>Una unitat aritm\u00e8tica i l\u00f2gica o ALU (de Arithmetic-Logic Unit)</li> <li>Una unitat de control o UC</li> <li>Una s\u00e8rie de registres de mem\u00f2ria</li> </ul> <p>Vegem aquests components amb detall.</p>"},{"location":"SOM/Tema01/SistemesInformatics/#151-alu","title":"1.5.1. ALU","text":"<p>La ALU (de Arithmetic-Logic Unit) s'encarrega de realitzar les operacions de c\u00e0lcul:</p> <ul> <li>Aritm\u00e8tiques (com les sumes)</li> <li>L\u00f2giques (com AND i OR)</li> <li>Comparatives (que permeten saber, per exemple si un valor \u00e9s major que un altre).</li> </ul>"},{"location":"SOM/Tema01/SistemesInformatics/#152-uc","title":"1.5.2. UC","text":"<p>La UC realitza el seg\u00fcent:</p> <ul> <li>Obt\u00e9 de la mem\u00f2ria la seg\u00fcent instrucci\u00f3 a executar.</li> <li>La interpreta.</li> <li>Torna a la mem\u00f2ria per a obtindre les dades implicades en l'operaci\u00f3, els situa en la ALU.</li> <li>Una vegada obtingut el resultat, el retorna a la posici\u00f3 adequada de la mem\u00f2ria.</li> </ul>"},{"location":"SOM/Tema01/SistemesInformatics/#153-registres","title":"1.5.3. Registres","text":"<p>Els registres de mem\u00f2ria emmagatzemen temporalment informaci\u00f3 relacionada amb el processament de dades que s'est\u00e0 realitzant. Els principals s\u00f3n:</p> <ul> <li>Comptador de programa: Que guarda la direcci\u00f3 de mem\u00f2ria de la instrucci\u00f3 que s'executar\u00e0 a continuaci\u00f3 de l'actual.</li> <li>Registre Acumulador: Guarda els resultats temporals d'una operaci\u00f3 c\u00edclica que es troba en curs en la ALU.</li> <li>Registre d'instrucci\u00f3: Cont\u00e9 el codi de la instrucci\u00f3 que s'est\u00e0 executant.</li> <li>Registre de direcci\u00f3 de memoria: Cont\u00e9 la direcci\u00f3 de mem\u00f2ria a la que s\u2019accedeix.</li> <li>Registre d'intercanvi de mem\u00f2ria: Cont\u00e9 informaci\u00f3 que s\u2019ha de escriure o llegir de la memoria.</li> </ul>"},{"location":"SOM/Tema01/SistemesInformatics/#154-joc-dinstrucions","title":"1.5.4. Joc d'instrucions","text":"<p>\u00c9s el conjunt d'instruccions que un processador en particular \u00e9s capa\u00e7 d'interpretar. En realitat, a m\u00e9s de les instruccions, tamb\u00e9 estan implicats els registres que inclou el disseny del processador, els tipus de dades que sap manejar, les interrupcions, etc.</p> <p>Algunes vegades, processadors amb un disseny totalment diferents tenen els mateixos jocs d'instruccions. Aix\u00f2 es fa perseguint la compatibilitat entre diferents models de processador del mateix fabricador o entre processadors de fabricants diferents.</p> <p>No obstant aix\u00f2, el normal \u00e9s que els processadors que s\u00f3n diferents tinguen jocs d'instruccions diferents.</p> <p>Interpretant el funcionament del processador d'una forma simplista, podem dir que l'execuci\u00f3 d'un programa es basa en un proc\u00e9s repetitiu en el qual es van llegint i executant una s\u00e8rie d'instruccions preestablides.</p> <p>Cicle d'instrucci\u00f3</p> <p>El Cicle b\u00e0sic d'instrucci\u00f3 i segueix el seg\u00fcent esquema:</p> <p></p> <p>En definitiva, el cicle d'instrucci\u00f3 \u00e9s el per\u00edode que tarda el processador a executar una instrucci\u00f3 del seu joc d'instruccions. Podem dividir-ho en dues etapes:</p> <ul> <li>Lectura</li> <li>Execuci\u00f3</li> </ul> <p>Etapa de Lectura</p> <p>Llegir la instrucci\u00f3 seg\u00fcent:</p> <ul> <li>Es passa el valor del comptador de programa al bus de direccions.</li> <li>Es carrega la instrucci\u00f3 des de la mem\u00f2ria principal al registre de dades.</li> <li>S'incrementa el valor del comptador de programa.</li> <li>Es carrega el valor del registre de dades en el registre d'instrucci\u00f3.</li> <li>El descodificador d'instrucci\u00f3 interpreta la instrucci\u00f3.</li> <li>Si hi ha dades implicades, es carreguen en els registres de dades.</li> </ul> <p>Etapa de Execuci\u00f3</p> <p>Executar la instrucci\u00f3:</p> <ul> <li>La unitat de control interpreta la instrucci\u00f3 com una s\u00e8rie de senyals de control i les envia a les unitats funcionals implicades perqu\u00e8 es realitze l'acci\u00f3.</li> <li>Segons la instrucci\u00f3, el resultat s'envia a la mem\u00f2ria principal o a algun dispositiu.</li> </ul> <p>Instruccions</p> <p>En general, les instruccions es classifiquen en les seg\u00fcents categories:</p> <ul> <li>Processador-mem\u00f2ria: S'envien dades des de la mem\u00f2ria al processador o al rev\u00e9s.</li> <li>Processador-E/S: S'envien dades des d'un dispositiu d'entrada/eixida al processador o al rev\u00e9s.</li> <li>Tractament de dades: realitza una operaci\u00f3 aritm\u00e8tica, una operaci\u00f3 l\u00f2gica o una comparaci\u00f3.</li> <li>Control: Modifica la seq\u00fc\u00e8ncia d'execuci\u00f3 del programa, \u00e9s a dir, col\u00b7loca un valor diferent en el registre comptador de programa.</li> </ul> <p>Interrupcions</p> <p>Una Interrupci\u00f3 consisteix en un senyal que rep el processador en unes certes situacions i que ocasiona la suspensi\u00f3 temporal del programa que s'est\u00e0 executant. L'objectiu \u00e9s atendre possibles incid\u00e8ncies que sorgisquen al llarg de l'execuci\u00f3 del programa.</p> <p>D'aquesta manera s'aconsegueix millorar l'efici\u00e8ncia del processament perqu\u00e8, per exemple, el processador pot dedicar-se a executar un proc\u00e9s mentre espera una operaci\u00f3 d'entrada/eixida d'un proc\u00e9s diferent, sent avisat quan l'operaci\u00f3 d'entrada/eixida concloga.</p> <p>Existeixen diferents tipus d'interrupcions:</p> <ul> <li>De programa: Es produeixen quan, en tractar d'executar una instrucci\u00f3, s'obt\u00e9 un error inesperat (una divisi\u00f3 per zero, l'\u00fas d'una posici\u00f3 de mem\u00f2ria no autoritzada, etc).</li> <li>Per fallada maquinari: Quan es produeix una fallada imprevista en l'\u00fas d'un dispositiu (com un error de paritat en una posici\u00f3 de mem\u00f2ria).</li> <li>De rellotge: Produ\u00efdes pel rellotge del sistema.</li> <li>D'entrada/eixida: quan es produeix una situaci\u00f3 d'error en una operaci\u00f3 d'entrada/eixida o quan aquesta ha acabat satisfact\u00f2riament.</li> </ul>"},{"location":"SOM/Tema01/SistemesInformatics/#16-memoria","title":"1.6. Mem\u00f2ria","text":"<p>Des d'un punt de vista gen\u00e8ric, la mem\u00f2ria \u00e9s la part de l'ordinador que s'encarrega d'emmagatzemar les dades que intervenen en el proc\u00e9s. No obstant aix\u00f2, dins del sistema inform\u00e0tic existeixen diferents tipus de mem\u00f2ria, que anirem desglossant:</p>"},{"location":"SOM/Tema01/SistemesInformatics/#161-registres","title":"1.6.1. Registres","text":"<p>Els registres s\u00f3n xicotetes porcions de mem\u00f2ria que es troben integrades en el processador i que, per tant, funcionen a la mateixa velocitat que aquest.</p>"},{"location":"SOM/Tema01/SistemesInformatics/#162-memoria-cau","title":"1.6.2. Mem\u00f2ria Cau","text":"<ul> <li>El seu funcionament \u00e9s similar al de la mem\u00f2ria principal, que comentarem a continuaci\u00f3, per\u00f2 la seua grand\u00e0ria \u00e9s molt menor i el seu acc\u00e9s molt m\u00e9s r\u00e0pid.</li> <li>La idea \u00e9s que, quan el processador necessita accedir a una dada, es copia a la mem\u00f2ria cau tot el bloc que el cont\u00e9. Aix\u00ed, si es produeixen accessos successius (per a llegir o escriure) a la mateixa dada o a les seues dades circumdants, el temps d'acc\u00e9s es redueix dr\u00e0sticament.</li> </ul>"},{"location":"SOM/Tema01/SistemesInformatics/#163-memoria-principal","title":"1.6.3. Mem\u00f2ria Principal","text":"<ul> <li>Tamb\u00e9 es diu Mem\u00f2ria d'acc\u00e9s aleatori o Mem\u00f2ria RAM (de l'angl\u00e9s, Random-Access Memory) perqu\u00e8 en els primers ordinadors era l'\u00fanica mem\u00f2ria que permetia accedir a les dades sense seguir un ordre previ.</li> <li>Aquest \u00e9s el lloc on han de trobar-se tant les instruccions com les dades perqu\u00e8 el processador puga utilitzar-los. El seu contingut s'organitza en posicions de mem\u00f2ria que estan identificades de manera individual per una direcci\u00f3 \u00fanica.</li> </ul>"},{"location":"SOM/Tema01/SistemesInformatics/#164-dispositius-demmagatzematge-extern","title":"1.6.4. Dispositius d'emmagatzematge extern","text":"<ul> <li>S\u00f3n dispositius que permeten emmagatzemar grans volums d'informaci\u00f3.</li> <li>El seu principal caracter\u00edstica \u00e9s que no \u00e9s vol\u00e0til, \u00e9s a dir, no necessiten un subministrament continu de corrent el\u00e8ctric per a mantindre la informaci\u00f3 que contenen.</li> <li>Existeixen dispositius constru\u00efts a partir de tecnologies molt diferents, com les unitats magn\u00e8tiques (HDD, de l'angl\u00e9s Hard Disk Drive), les \u00f2ptiques (CD/DVD, de l'angl\u00e8s Compact Disc/Digital Versatile Disc) o les flaix (SSD, de l'angl\u00e9s Solid State Drive).</li> </ul>"},{"location":"SOM/Tema01/SistemesInformatics/#165-jerraquia-de-memories","title":"1.6.5. Jerraquia de memories","text":"<p>En principi, existeixen tres dades fonamentals que hem de tindre en compte quan ens referim a la mem\u00f2ria: la seua quantitat, la seua velocitat i el cost per unitat d'emmagatzematge (per exemple, el cost per byte).</p> <p>Gr\u00e0ficament obtenim una jerarquia de la mem\u00f2ria amb forma piramidal</p> <p></p> <p>Si prenem com a punt de partida la imatge anterior, podem afirmar que, segons es descendeix en la jerarquia, es compleixen les seg\u00fcents caracter\u00edstiques:</p> <ul> <li>Disminueix el cost per byte.</li> <li>Augmenta la capacitat.</li> <li>Augmenta el temps d'acc\u00e9s.</li> <li>Disminueix la freq\u00fc\u00e8ncia amb la qual s'accedeix a la mem\u00f2ria.</li> </ul> <p>La mem\u00f2ria principal funciona a una velocitat molt inferior a la del processador. No obstant aix\u00f2, aquest ha d'accedir a la mem\u00f2ria per a obtindre cadascuna de les instruccions que ha d'executar. Resulta evident la c\u00e0rrega que suposa aquesta situaci\u00f3 per al rendiment del processador.</p> <p>Per a resoldre-ho, els dissenyadors recorren al principi de proximitat. La idea consisteix a col\u00b7locar, entre el processador i la mem\u00f2ria principal, una mem\u00f2ria de poca grand\u00e0ria i gran velocitat, a la qual anomenem mem\u00f2ria cau.</p> <p>La idea \u00e9s que, cada vegada que el processador sol\u00b7licite una dada de la mem\u00f2ria principal, es busque primer en la mem\u00f2ria cau. Si no es troba, es llig de la mem\u00f2ria principal el bloc complet que cont\u00e9 la dada sol\u00b7licitada i es guarda en la cau.</p>"},{"location":"SOM/Tema01/SistemesInformatics/#17-unitats-dentradaeixida","title":"1.7. Unitats d'Entrada/Eixida","text":"<p>Els dispositius d'entrada/eixida tenen un doble objectiu:</p> <ul> <li>Permeten que l'ordinador es comunique amb l'exterior. Aix\u00ed obt\u00e9 la informaci\u00f3 que ha de processar o ofereix els resultats dels seus c\u00e0lculs.</li> <li>Codifiquen la informaci\u00f3 d'entrada en un format que comprenga l'ordinador i, la informaci\u00f3 d'eixida, en un format que s'entenga en l'exterior.</li> </ul> <p>El concepte general de les unitats d'entrada/eixida es va desenvolupar des de les primeres generacions d'ordinadors, en els quals les unitats d'entrada eren bastant senzilles.</p> <p>Hui dia, aquest concepte abasta una gran varietat de dispositius, molts d'ells totalment diferents i que s'han desenvolupat per a satisfer les necessitats de les diferents aplicacions.</p> <p>Existeix una gran varietat de dispositius d'entrada/eixida, per la qual cosa comen\u00e7arem per classificar-los en tres categories:</p> <ul> <li>Dispositius d'entrada: S\u00f3n els que s'encarreguen de subministrar informaci\u00f3 a l'ordinador. Entre els m\u00e9s coneguts trobem aquests: teclat, ratol\u00ed, webcam, micr\u00f2fon, esc\u00e0ner, etc</li> <li>Dispositius d'eixida: S\u00f3n els que s'encarreguen d'oferir informaci\u00f3 a l'exterior. Entre els m\u00e9s coneguts trobem aquests: monitor, impressora, altaveus, auriculars, etc.</li> <li>Dispositius d'entrada i eixida: S\u00f3n els que poden realitzar les dues funcions anteriors de manera simult\u00e0nia. Entre els m\u00e9s coneguts trobem els seg\u00fcents: pantalla t\u00e0ctil, dispositius d'emmagatzematge extern (discos durs, mem\u00f2ries USB, CDs o Dvds, \u2026), targetes de xarxa, \u2026</li> </ul>"},{"location":"SOM/Tema01/SistemesInformatics/#18-busos","title":"1.8. Busos","text":"<p>Podem definir els busos com a canals que serveixen per a transferir dades entre els diferents components d'un ordinador. Permeten interconnectar des de les diferents parts d'un circuit integrat fins als dispositius perif\u00e8rics units a l'ordinador. Segons la seua naturalesa, podem trobar dos tipus de busos:</p> <ul> <li>L\u00ednies de dades: Tots els bits d'un mateix byte s\u00f3n enviats alhora per les diferents l\u00ednies de dades del bus.</li> <li>L\u00ednies d'adre\u00e7a: Per elles circulen els bits que representen la posici\u00f3 de mem\u00f2ria o el dispositiu de dest\u00ed de la informaci\u00f3 que s'est\u00e0 transmetent.</li> <li>L\u00ednies de control: S'encarreguen d'enviar senyals de control entre els dispositius. Poden contindre informaci\u00f3 sobre l'estat de la comunicaci\u00f3, interrupcions o DMA.</li> </ul> <p></p>"},{"location":"SOM/Tema02/SistemesOperatius/","title":"2. Sistemes Operatius","text":""},{"location":"SOM/Tema02/SistemesOperatius/#21-sistema-operatiu-i-aplicacions","title":"2.1. Sistema Operatiu i aplicacions","text":"<p>La RAE defineix el programari com el conjunt de programes, instruccions i regles inform\u00e0tiques per a executar certes tasques en un ordinador\". Per tant, el programari \u00e9s l'encarregat de dirigir l'ordinador en la tasca d'obtenir resultats particulars.</p> <p>En l\u00ednies generals, podem dir que existeixen dos tipus de programari:</p> <ul> <li>El programari de sistema.</li> <li>El programari d'aplicaci\u00f3.</li> </ul> <p>Parlem d'ambd\u00f3s tipus de forma m\u00e9s detallada.</p>"},{"location":"SOM/Tema02/SistemesOperatius/#211-programari-de-sistema","title":"2.1.1. Programari de sistema","text":"<p>El programari de sistema, tamb\u00e9 conegut com a programari de base, \u00e9s el conjunt de programari que es fa c\u00e0rrec de gestionar els recursos hardware del sistema inform\u00e0tic, separant tant els usuaris finals com els desenvolupadors de programari de les seues caracter\u00edstiques espec\u00edfiques.</p> <p>Per a aconseguir-ho, incorpora una interf\u00edcie adequada per a l'usuari final i un conjunt de funcions i procediments que poden ser invocats pels programes d'aplicaci\u00f3 i que rep el nom d'API (de l'angl\u00e9s Application Programming Interface).</p> <p></p> <p>Encara que l'element fonamental del programari de sistema siga el sistema operatiu, tamb\u00e9 s'inclouen en aquest nivell els controladors de dispositiu, les eines de diagn\u00f2stic i altres utilitats. Entre els sistemes operatius m\u00e9s utilitzats es troben:</p> <ul> <li>Microsoft Windows.</li> <li>GNU/Linux.</li> <li>Apple macOS.</li> </ul>"},{"location":"SOM/Tema02/SistemesOperatius/#212-programari-daplicacio","title":"2.1.2. Programari d'aplicaci\u00f3","text":"<p>Est\u00e0 format per els programes que permeten als usuaris realitzar tasques concretes, que poden ser generals (processadors de text, fulls de c\u00e0lcul, navegadors d'internet, etc.) o espec\u00edfics per a activitats particulars que puguen recolzar-se en un sistema inform\u00e0tic (comptabilitat, disseny assistit per ordinador, videojocs, etc.).</p> <p>\u00c9s molt freq\u00fcent que just despr\u00e9s d'instal\u00b7lar un sistema operatiu trobem instal\u00b7lats alguns programes (calculadora, navegador, editor de textos, etc.). Encara que aquests programes formin part del programari d'aplicaci\u00f3, \u00e9s molt freq\u00fcent que els usuaris finals pensen que formen part del sistema operatiu. No obstant aix\u00f2, nom\u00e9s es tracta d'un complement que incorporen els prove\u00efdors del sistema operatiu per a facilitar certes tasques comunes.</p>"},{"location":"SOM/Tema02/SistemesOperatius/#22-sistemes-operatius-actuals","title":"2.2. Sistemes Operatius Actuals","text":"<p>Ja hem esmentat alguns dels sistemes operatius m\u00e9s utilitzats en ordinadors: Microsoft Windows, GNU/Linux i Apple macOS. No obstant aix\u00f2, nom\u00e9s ens hav\u00edem referit a les solucions d'escriptori, \u00e9s a dir, als sistemes operatius dissenyats per a l'\u00fas diari per part d'un usuari o un grup redu\u00eft d'ells. No obstant aix\u00f2, existeixen sistemes operatius per a ordinadors la tasca dels quals consisteix en oferir serveis a trav\u00e9s d'una xarxa a altres ordinadors (i als seus usuaris). En aquests casos, els sistemes d'escriptori actuen com a clients dels sistemes servidors.</p> <p>A m\u00e9s, existeixen altres sistemes operatius que, encara que no estan dissenyats per a ordinadors, tamb\u00e9 s\u00f3n molt comuns en l'actualitat, s\u00f3n els destinats a tel\u00e8fons intel\u00b7ligents (smartphones). Entre ells, podem mencionar els seg\u00fcents: Android, iOS, Windows Phone i, en menor mesura, Ubuntu Phone, Firefox OS, BlackBerry OS i Symbian.</p> <p>Tamb\u00e9 hi ha altres exemples de sistemes operatius que, en el moment d'escriure aquestes l\u00ednies, mostren certa rellev\u00e0ncia. Ens referim als seg\u00fcents:</p> <ul> <li>Chrome OS: Un sistema operatiu basat en GNU/Linux, desenvolupat per Google i orientat a Internet, on la principal eina \u00e9s el navegador.</li> <li>webOS: Un sistema operatiu basat en GNU/Linux, desenvolupat per LG per a incloure'l en els seus televisors.</li> <li>Tizen: Un sistema operatiu basat en GNU/Linux, recolzat per Linux Mobile Foundation (LiMo), Linux Foundation i Samsung, per a ser instal\u00b7lat en tel\u00e8fons intel\u00b7ligents, tauletes, etc.</li> </ul>"},{"location":"SOM/Tema02/SistemesOperatius/#23-interficie-dels-so","title":"2.3. Interficie dels S.O.","text":"<p>La interf\u00edcie dels sistemes operatius \u00e9s la que ens permet la comunicaci\u00f3 entre els usuaris i el maquinari de la m\u00e0quina.</p> <p></p> <p>Els sistemes operatius tenen dos tipus d'interf\u00edcie:</p> <ul> <li>Interf\u00edcie de tipus text: Totes les ordres que l'usuari introdu\u00efsca i les respostes que el SO dona se introduiran o visualitzaran mitjan\u00e7ant cadenes de car\u00e0cters.</li> <li>Interf\u00edcie de tipus gr\u00e0fic: El mitj\u00e0 de comunicaci\u00f3 entre l'usuari i l'ordinador \u00e9s gr\u00e0fic, i \u00e9s necessari l'\u00fas del ratol\u00ed.</li> </ul>"},{"location":"SOM/Tema02/SistemesOperatius/#24-so-lliures-i-propietaris","title":"2.4. S.O. Lliures i propietaris","text":"<p>Davant de l'\u00e0mplia gamma de sistemes operatius que trobem en l'actualitat, ens veiem obligats a acotar l'\u00e0mbit al qual ens dedicarem en aquest curs, amb l'objectiu de ser el m\u00e9s concrets possible. En aquest sentit, ens decantem per les versions d'escriptori de dos entorns principals:</p> <ul> <li>El sistema operatiu Microsoft Windows, com a paradigma dels sistemes amb llic\u00e8ncia de codi tancat.</li> <li>El sistema operatiu Ubuntu, que potser \u00e9s el que t\u00e9 una major repercussi\u00f3 entre els que ofereixen llic\u00e8ncies de codi obert.</li> </ul>"},{"location":"SOM/Tema02/SistemesOperatius/#25-programari-de-base-dun-so","title":"2.5. Programari de base d'un S.O.","text":"<p>Si no existira el programari de sistema tamb\u00e9 anomenat programari de base, cada programador que, per exemple, estiguera escrivint un programa que oferira dades impresses, hauria d'escriure les instruccions necess\u00e0ries per a controlar de forma precisa la impressora.</p> <p>Si, a m\u00e9s, l'objectiu f\u00f3ra que el programa funcionara en ordinadors amb diferents models d'impressora, hauria de repetir la feina per a cada model concret. Aix\u00f2 faria que la feina a la qual s'enfrontaria f\u00f3ra immensa.</p> <p>El programari de sistema fa que els programes d'aplicacions puguen manejar les impressores d'una forma gen\u00e8rica, amb ordres b\u00e0siques i senzilles, i el que \u00e9s m\u00e9s important, generals per a qualsevol model d'impressora.</p> <p>A m\u00e9s, aquesta idea s'aplica tamb\u00e9 a la resta dels dispositius: discs durs, dispositius d'emmagatzematge USB, monitors, ratolins, etc. Totes les ordres d'aquest tipus s\u00f3n les que formen l'API. Encara que l'element fonamental del programari de sistema siga el sistema operatiu, tamb\u00e9 s'inclouen en aquest nivell els controladors de dispositiu, les eines de diagn\u00f2stic i altres utilitats.</p>"},{"location":"SOM/Tema02/SistemesOperatius/#26-elements-i-estructura-del-so","title":"2.6. Elements i estructura del S.O.","text":"<p>Podr\u00edem definir el concepte de Sistema Operatiu com un programa, o un conjunt de programes que col\u00b7laboren entre ells per a administrar els elements f\u00edsics d'un sistema inform\u00e0tic, optimitzant el seu \u00fas i oferint determinats serveis als programes d'aplicaci\u00f3.</p> <p>Un sistema operatiu es far\u00e0 c\u00e0rrec d'aspectes com:</p> <ul> <li>L'\u00fas, compartit i ordenat, dels recursos entre diferents usuaris.</li> <li>La protecci\u00f3 de recursos, per evitar que un usuari accedisca a recursos als quals no est\u00e0 autoritzat.</li> </ul> <p>Perqu\u00e8 aquesta protecci\u00f3 siga possible, el sistema inform\u00e0tic ha de ser capa\u00e7 d'executar instruccions en dos nivells diferents:</p> <ul> <li>En mode usuari: \u00c9s el mode menys privilegiat de funcionament del sistema. En aquest mode no es permet l'acc\u00e9s directe al maquinari.</li> <li>En mode nucli (tamb\u00e9 anomenat mode kernel) o mode supervisor: En ell, les instruccions s'executen en un mode privilegiat, tenint acc\u00e9s directe a tota la mem\u00f2ria (inclosos els espais d'adreces de tots els processos que estiguen executant-se). En aquest mode nom\u00e9s s'executen algunes parts del sistema operatiu.</li> </ul>"},{"location":"SOM/Tema02/SistemesOperatius/#27-classificacio-dels-so","title":"2.7. Classificaci\u00f3 dels S.O.","text":"<p>Per nombre d'usuaris:</p> <ul> <li>Monousuari: Nom\u00e9s un usuari en l'ordinador en un moment donat.</li> <li>Multiusuari: Varios usuaris en l'ordinador.</li> </ul> <p>Per nombre de processos:</p> <ul> <li>Monotasca: Nom\u00e9s processa una tasca en un instant donat.</li> <li>Multitasca: Varies tasques simult\u00e0niament</li> </ul>"},{"location":"SOM/Tema02/SistemesOperatius/#28-elements-dun-so","title":"2.8. Elements d'un S.O.","text":"<p>Com podem imaginar, un sistema operatiu \u00e9s un programa molt complex que ha d'estar molt ben organitzat i estructurat internament per a dur a terme la seua feina d'una forma molt eficient. En aquest sentit, els sistemes operatius es subdividixen en diferents components que estan especialitzats en aspectes molt concrets del mateix.</p> <p>Els elements que constitueixen la majoria dels sistemes operatius s\u00f3n els seg\u00fcents:</p> <ul> <li>Gestor de processos.</li> <li>Gestor de mem\u00f2ria virtual.</li> <li>Gestor d'emmagatzematge secundari.</li> <li>Gestor d'entrada i eixida.</li> <li>Sistema d'arxius.</li> <li>Sistemes de protecci\u00f3.</li> <li>Sistema de comunicacions.</li> <li>Programes de sistema.</li> <li>Gestor de recursos.</li> </ul> <p></p>"},{"location":"SOM/Tema02/SistemesOperatius/#29-estructura-dun-so","title":"2.9 Estructura d'un  S.O.","text":"<p>Podem plantejar-nos la manera en qu\u00e8 aquests elements s'organitzen dins del sistema operatiu per a dur a terme el seu objectiu. Tamb\u00e9 ser\u00e0 important per al disseny del sistema establir quins components del mateix s'executen en mode nucli i quins en mode usuari.</p> <p>En aquest sentit, els plantejaments que s'apliquen en els sistemes operatius m\u00e9s coneguts s\u00f3n els seg\u00fcents:</p> <ul> <li>Monol\u00edtic.</li> <li>Micronucli.</li> <li>Nucli h\u00edbrid</li> </ul>"},{"location":"SOM/Tema02/SistemesOperatius/#291-monolitic","title":"2.9.1. Monol\u00edtic","text":""},{"location":"SOM/Tema02/SistemesOperatius/#292-micronucli","title":"2.9.2. Micronucli","text":""},{"location":"SOM/Tema02/SistemesOperatius/#293-nucli-hibrid","title":"2.9.3. Nucli h\u00edbrid","text":""},{"location":"SOM/Tema02/SistemesOperatius/#210-funcions-del-so","title":"2.10 Funcions del S.O.","text":"<p>La estructura d'un sistema operatiu es divideix en diferents m\u00f2duls que solen estar especialitzats en funcions concretes. En alguns casos existeix una relaci\u00f3 directa entre un determinat m\u00f2dul i una funci\u00f3 concreta del sistema operatiu. No obstant aix\u00f2, en altres casos, s\u00f3n diversos els m\u00f2duls que cooperen d'alguna manera per a dur a terme una funci\u00f3 espec\u00edfica.</p> <p>En qualsevol cas, aquestes s\u00f3n les principals funcions que duu a terme qualsevol sistema operatiu:</p> <ul> <li>Gesti\u00f3 de processos.</li> <li>Gesti\u00f3 de mem\u00f2ria.</li> <li>Gesti\u00f3 d'arxius.</li> <li>Gesti\u00f3 d'Entrada/Eixida (E/S).</li> </ul>"},{"location":"SOM/Tema02/SistemesOperatius/#bibliografia","title":"BIBLIOGRAFIA","text":"<p>Enlla\u00e7 a la font original (Cap\u00edtols 3 i 4)</p>"}]}